## Operating System

- 1 **什么是操作系统?**
    - Operating System
        - Opeating system: 内核 shell 文件系统 应用程序.
        - 操作系统(Operating System, 简称 OS)是管理计算机硬件与软件资源的程序, 是计算机的基石. 
        - 操作系统本质上是一个运行在计算机上的软件程序, 主要用于管理计算机硬件和软件资源. 举例: 运行在你电脑上的所有应用程序都通过操作系统来调用系统内存以及磁盘等等硬件. 
        - 操作系统存在屏蔽了硬件层的复杂性. 操作系统就像是硬件使用的负责人, 统筹着各种相关事项. 
    - Kernel
        - Kernel: 进程调度, 内存管理, 虚拟文件系统, 网络接口, 进程间通信.
        - 操作系统的内核(Kernel)是操作系统的核心部分, 它负责系统的内存管理, 硬件设备的管理, 文件系统的管理以及应用程序的管理. 内核是连接应用程序和硬件的桥梁, 决定着系统的性能和稳定性.
        - 操作系统的内核(Kernel)与中央处理器(CPU, Central Processing Unit)的区别:
            - 操作系统的内核(Kernel)属于操作系统层面, 而 CPU 属于硬件. 
            - CPU主要提供运算, 处理各种指令的能力. 内核(Kernel)主要负责系统管理比如内存管理, 它屏蔽了对硬件的操作.

    ![Untitled](operating_system/Kernel_Layout.png)

- 2 操作系统主要有哪些功能?
    - 操作系统有 6 大功能: 
        - 进程和线程的管理: 进程的创建, 撤销, 阻塞, 唤醒, 进程间的通信等. 
        - 存储管理: 内存的分配和管理, 外存(磁盘等)的分配和管理等. 
        - 文件管理: 文件的读, 写, 创建及删除等. 
        - 设备管理: 完成设备(输入输出设备和外部存储设备等)的请求或释放, 以及设备启动等功能. 
        - 网络管理: 操作系统负责管理计算机网络的使用. 网络是计算机系统中连接不同计算机的方式, 操作系统需要管理计算机网络的配置, 连接, 通信和安全等, 以提供高效可靠的网络服务. 
        - 安全管理: 用户的身份认证, 访问控制, 文件加密等, 以防止非法用户对系统资源的访问和操作. 

- 3 常见的操作系统有哪些?
    - Windows
        目前最流行的个人桌面操作系统 , 不做多的介绍, 大家都清楚. 界面简单易操作, 软件生态非常好. 
        玩玩电脑游戏还是必须要有 Windows 的, 所以我现在是一台 Windows 用于玩游戏, 一台 Mac 用于平时日常开发和学习使用. 

    - Unix
        最早的多用户, 多任务操作系统. 后面崛起的 Linux 在很多方面都参考了 Unix.
        目前这款操作系统已经逐渐逐渐退出操作系统的舞台. 

    - Linux
        Linux 是一套免费使用, 开源的类 Unix 操作系统. Linux 存在着许多不同的发行版本, 但它们都使用了 Linux 内核.
        严格来讲, Linux 这个词本身只表示 Linux 内核, 在 GNU/Linux 系统中, Linux 实际就是 Linux 内核, 而该系统的其余部分主要是由 GNU 工程编写和提供的程序组成. 单独的 Linux 内核并不能成为一个可以正常工作的操作系统.
        很多人更倾向使用 "GNU/Linux" 一词来表达人们通常所说的 "Linux". 

    - Mac OS
        苹果自家的操作系统, 编程体验和 Linux 相当, 但是界面, 软件生态以及用户体验各方面都要比 Linux 操作系统更好. 

- 4 什么是用户态和内核态?
    根据进程访问资源的特点, 我们可以把进程在系统上的运行分为两个级别:
    - 用户态(User Mode): 用户态运行的进程可以直接读取用户程序的数据, 拥有较低的权限. 当应用程序需要执行某些需要特殊权限的操作, 例如读写磁盘, 网络通信等, 就需要向操作系统发起系统调用请求, 进入内核态.
    - 内核态(Kernel Mode): 内核态运行的进程几乎可以访问计算机的任何资源包括系统的内存空间, 设备, 驱动程序等, 不受限制, 拥有非常高的权限. 当操作系统接收到进程的系统调用请求时, 就会从用户态切换到内核态, 执行相应的系统调用, 并将结果返回给进程, 最后再从内核态切换回用户态.
    - 内核态相比用户态拥有更高的特权级别, 因此能够执行更底层, 更敏感的操作. 不过, 由于进入内核态需要付出较高的开销(需要进行一系列的上下文切换和权限检查), 应该尽量减少进入内核态的次数, 以提高系统的性能和稳定性.
    - 当一个进程在执行用户自己的代码时处于用户运行态(用户态), 此时特权级最低, 为3级, 是普通的用户进程运行的特权级, 大部分用户直接面对的程序都是运行在用户态. Ring3状态不能访问Ring0的地址空间, 包括代码和数据; 当一个进程因为系统调用陷入内核代码中执行时处于内核运行态(内核态), 此时特权级最高, 为0级. 执行的内核代码会使用当前进程的内核栈, 每个进程都有自己的内核栈.

- 6 用户态和内核态是如何切换的?
    - 系统调用(Trap): 用户态进程 主动 要求切换到内核态的一种方式, 主要是为了使用内核态才能做的事情比如读取磁盘资源. 系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现. 
    - 中断(Interrupt): 当外围设备完成用户请求的操作后, 会向 CPU 发出相应的中断信号, 这时 CPU 会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序, 如果先前执行的指令是用户态下的程序, 那么这个转换的过程自然也就发生了由用户态到内核态的切换. 比如硬盘读写操作完成, 系统会切换到硬盘读写的中断处理程序中执行后续操作等. 
    - 异常(Exception): 当 CPU 在执行运行在用户态下的程序时, 发生了某些事先不可知的异常, 这时会触发由当前运行进程切换到处理此异常的内核相关程序中, 也就转到了内核态, 比如缺页异常. 
    - 在系统的处理上, 中断和异常类似, 都是通过中断向量表来找到相应的处理程序进行处理. 区别在于, 中断来自处理器外部, 不是由任何一条专门的指令造成, 而异常是执行当前指令的结果.

- 7 什么是系统调用?
    - 系统调用
        - 如果调用操作系统提供的内核态级别的子功能时, 就需要系统调用.
        - 在我们运行的用户程序中, 凡是与系统态级别的资源有关的操作(如文件管理, 进程控制, 内存管理等), 都必须通过系统调用方式向操作系统提出服务请求, 并由操作系统代为完成.

        - 系统调用按功能大致可分为如下几类:
            - 设备管理: 完成设备(输入输出设备和外部存储设备等)的请求或释放, 以及设备启动等功能. 
            - 文件管理: 完成文件的读, 写, 创建及删除等功能. 
            - 进程管理: 进程的创建, 撤销, 阻塞, 唤醒, 进程间的通信等功能. 
            - 内存管理: 完成内存的分配, 回收以及获取作业占用内存区大小及地址等功能. 
            - 系统调用和普通库函数调用非常相似, 只是系统调用由操作系统内核提供, 运行于内核态, 而普通的库函数调用由函数库或用户自己提供, 运行于用户态.

    - 系统调用与普通函数调用的区别
        - 系统调用
            1 使用`INT`和`IRET`指令, 内核和应用程序使用的是不同的堆栈, 因此存在堆栈的切换, 从用户态切换到内核态, 从而可以使用特权指令操控设备.
            2 依赖于内核, 不保证移植性.
            3 在用户空间和内核上下文环境间切换, 开销较大.
            4 是操作系统的一个入口点.

        - 普通函数调用
            1 使用`CALL`和`RET`指令, 调用时没有堆栈切换.
            2 平台移植性好.
            3 属于过程调用, 调用开销较小.
            4 一个普通功能函数的调用.

    - 系统调用是应用程序与操作系统之间进行交互的一种方式, 通过系统调用, 应用程序可以访问操作系统底层资源例如文件, 设备, 网络等. 

- 8 系统调用的过程
    - 用户态的程序发起系统调用, 因为系统调用中涉及一些特权指令(只能由操作系统内核态执行的指令), 用户态程序权限不足, 因此会中断执行, 也就是 Trap(Trap 是一种中断).
    - 发生中断后, 当前 CPU 执行的程序会中断, 跳转到中断处理程序. 内核程序开始执行, 也就是开始处理系统调用.
    - 内核处理完成后, 主动触发 Trap, 这样会再次发生中断, 切换回用户态工作.

- Bootloader
    - Bootloader
        - Bootloader全名为启动引导程序, 是第一段代码, 主要用来初始化处理器及外设, 然后调用Linux内核.
        - Linux内核在完成系统的初始化之后, 需要挂载一个文件系统作为根文件系统(RootFilesystem), 然后加载必要的内核模块, 启动应用程序. (一个嵌入式Linux系统从软件角度看可以分为四个部分: 引导加载程序(Bootloader), Linux内核, 文件系统, 应用程序).
        - 启动顺序: bootloader->linux kernel->rootfile->app

    - Bootloader启动顺序
        - Stage1: assembly
            - (1) 基本的硬件初始化(关闭看门狗和中断 MMU(带操作系统) CACHE 配置系统工作时钟).
            - (2) 为加载stage2准备RAM空间.
            - (3) 拷贝内核映像和文件系统映像到RAM中.
            - (4) 设置堆栈指针sp.
            - (5) 跳到stage2的入口点.

        - Stage2: c
            - (1) 初始化本阶段要使用到的硬件设备(led uart等).
            - (2) 检测系统的内存映射.
            - (3) 加载内核映像和文件系统映像.
            - (4) 设置内核的启动参数.

    - 嵌入式系统中广泛采用的非易失性存储器通常是Flash, 而Bootloader就位于该存储器的最前端, 所以系统上电或复位后执行的第一段程序便是Bootloader.

- 9 什么是进程和线程?
    - 进程(Process) 是指计算机中正在运行的一个程序实例. 举例: 打开的微信就是一个进程. 
    - 线程(Thread) 也被称为轻量级进程, 更加轻量. 多个线程可以在同一个进程中同时执行, 并且共享进程的资源比如内存空间, 文件句柄, 网络连接等. 举例: 打开的微信里就有一个线程专门用来拉取别人发你的最新的消息. 

- 10 进程和线程的区别是什么?
    ![Untitled](operating_system/process_thread.png)
    ![Untitled](operating_system/local_memory.png)

    - 一个进程中可以有多个线程, 多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源, 但是每个线程有自己的程序计数器, 虚拟机栈 和 本地方法栈.
    - 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程. 
    - 线程和进程最大的不同在于基本上各进程是独立的, 而各线程则不一定, 因为同一进程中的线程极有可能会相互影响. 
    - 线程执行开销小, 但不利于资源的管理和保护, 而进程正相反. 

- 11 有了进程为什么还需要线程?
    - 进程切换是一个开销很大的操作, 线程切换的成本较低. 
    - 线程更轻量, 一个进程可以创建多个线程. 
    - 多个线程可以并发处理不同的任务, 更有效地利用了多处理器和多核计算机. 而进程只能在一个时间干一件事, 如果在执行过程中遇到阻塞问题比如 IO 阻塞就会挂起直到结果返回. 
    - 同一进程内的线程共享内存和文件, 因此它们之间相互通信无须调用内核. 

- 12 为什么要使用多线程?
    - 总体: 
        - 从计算机底层来说: 线程可以比作是轻量级的进程, 是程序执行的最小单位, 线程间的切换和调度的成本远远小于进程. 另外, 多核 CPU 时代意味着多个线程可以同时运行, 这减少了线程上下文切换的开销.
        - 从当代互联网发展趋势来说: 现在的系统动不动就要求百万级甚至千万级的并发量, 而多线程并发编程正是开发高并发系统的基础, 利用好多线程机制可以大大提高系统整体的并发能力以及性能.
    
    - 计算机底层:
        - 单核时代: 在单核时代多线程主要是为了提高单进程利用 CPU 和 IO 系统的效率.  假设只运行了一个 Java 进程的情况, 当我们请求 IO 的时候, 如果 Java 进程中只有一个线程, 此线程被 IO 阻塞则整个进程被阻塞. CPU 和 IO 设备只有一个在运行, 那么可以简单地说系统整体效率只有 50%. 当使用多线程的时候, 一个线程被 IO 阻塞, 其他线程还可以继续使用 CPU. 从而提高了 Java 进程利用系统资源的整体效率. 
        - 多核时代: 多核时代多线程主要是为了提高进程利用多核 CPU 的能力. 举个例子: 假如我们要计算一个复杂的任务, 我们只用一个线程的话, 不论系统有几个 CPU 核心, 都只会有一个 CPU 核心被利用到. 而创建多个线程, 这些线程可以被映射到底层多个 CPU 上执行, 在任务中的多个线程没有资源竞争的情况下, 任务执行的效率会有显著性的提高, 约等于(单核时执行时间/CPU 核心数).

- 13 线程间的同步的方式有哪些?
    - 线程同步是两个或多个共享关键资源的线程的并发执行. 应该同步线程以避免关键的资源使用冲突. 常见的线程同步方式:
        - 互斥锁(Mutex): 采用互斥对象机制, 只有拥有互斥对象的线程才有访问公共资源的权限. 因为互斥对象只有一个, 所以可以保证公共资源不会被多个线程同时访问. 比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制.
        - 读写锁(Read-Write Lock): 允许多个线程同时读取共享资源, 但只有一个线程可以对共享资源进行写操作.
        - 信号量(Semaphore): 它允许同一时刻多个线程访问同一资源, 但是需要控制同一时刻访问此资源的最大线程数量.
        - 屏障(Barrier): 屏障是一种同步原语, 用于等待多个线程到达某个点再一起继续执行. 当一个线程到达屏障时, 它会停止执行并等待其他线程到达屏障, 直到所有线程都到达屏障后, 它们才会一起继续执行. 比如 Java 中的 CyclicBarrier 是这种机制.
        - 事件(Event): Wait/Notify, 通过通知操作的方式来保持多线程同步, 还可以方便的实现多线程优先级的比较操作.

- 14 PCB 是什么?包含哪些信息?
    - PCB(Process Control Block) 即进程控制块, 是操作系统中用来管理和跟踪进程的数据结构, 每个进程都对应着一个独立的 PCB. 你可以将 PCB 视为进程的大脑. 
    当操作系统创建一个新进程时, 会为该进程分配一个唯一的进程 ID, 并且为该进程创建一个对应的进程控制块. 当进程执行时, PCB 中的信息会不断变化, 操作系统会根据这些信息来管理和调度进程. 

    - PCB 主要包含下面几部分的内容: 
        - 进程的描述信息, 包括进程的名称, 标识符等等;
        - 进程的调度信息, 包括进程阻塞原因, 进程状态(就绪, 运行, 阻塞等), 进程优先级(标识进程的重要程度)等等;
        - 进程对资源的需求情况, 包括 CPU 时间, 内存空间, I/O 设备等等. 
        - 进程打开的文件信息, 包括文件描述符, 文件类型, 打开模式等等. 
        - 处理机的状态信息(由处理机的各种寄存器中的内容组成的), 包括通用寄存器, 指令计数器, 程序状态字 PSW, 用户栈指针. 

- 15 进程有哪几种状态?
    - 创建状态(new): 进程正在被创建, 尚未到就绪状态. 
    - 就绪状态(ready): 进程已处于准备运行状态, 即进程获得了除了处理器之外的一切所需资源, 一旦得到处理器资源(处理器分配的时间片)即可运行. 
    - 运行状态(running): 进程正在处理器上运行(单核 CPU 下任意时刻只有一个进程处于运行状态). 
    - 阻塞状态(waiting): 又称为等待状态, 进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成. 即使处理器空闲, 该进程也不能运行. 
    - 结束状态(terminated): 进程正在从系统中消失. 可能是进程正常结束或其他原因中断退出运行. 
    ![Untitled](operating_system/process_change.png)

- 16 **进程间的通信方式有哪些?**
    - reference: https://www.jianshu.com/p/c1015f5ffa74

    - 无名管道/匿名管道(Pipes): 半双工的通信方式, 数据只能单向流动, 用于具有亲缘父子关系的进程通信.
    - 有名管道(Named Pipes) : 有名管道严格遵循 先进先出(First In First Out) . 有名管道以磁盘文件的方式存在, 可以实现本机任意两个进程通信.
    - 高级管道(Popen): 将另一个程序当做一个新的进程在当前程序进程中启动, 则它算是当前程序的子进程, 这种方式我们成为高级管道方式.
    - 信号(Signal) : 信号是一种比较复杂的通信方式, 用于通知接收进程某个事件已经发生.
    - 消息队列(Message Queue) : 消息队列是消息的链表, 具有特定的格式, 存放在内存中并由消息队列标识符标识. 管道和消息队列的通信数据都是先进先出的原则. 与管道不同的是消息队列存放在内核中, 只有在内核重启(即, 操作系统重启)或者显式地删除一个消息队列时, 该消息队列才会被真正的删除. 消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取. 比 FIFO 更有优势. 消息队列克服了信号承载信息量少, 管道只能承载无格式字 节流以及缓冲区大小受限等缺点. 
    - 信号量(Semaphores): 信号量是一个计数器, 用于控制多进程对共享资源的访问, 信号量的意图在于进程间同步. 这种通信方式主要用于解决与同步相关的问题并避免竞争条件. 
    - 共享内存(Shared memory): 使得多个进程可以访问同一块内存空间, 不同进程可以及时看到对方进程中对共享内存中数据的更新. 这种方式需要依靠某种同步操作, 如互斥锁和信号量等. 可以说这是最有用的进程间通信方式. 
    - 套接字(Sockets): 此方法主要用于在客户端和服务器之间通过网络进行通信. 套接字是支持 TCP/IP 的网络通信的基本操作单元, 可以看做是不同主机之间的进程进行双向通信的端点, 简单的说就是通信的两方的一种约定, 用套接字中的相关函数来完成通信过程. 

- 17 进程的调度算法有哪些?
    - 先到先服务调度算法(FCFS, First Come, First Served): 从就绪队列中选择一个最先进入该队列的进程为之分配资源, 使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度. 
    - 短作业优先的调度算法(SJF, Shortest Job First): 从就绪队列中选出一个估计运行时间最短的进程为之分配资源, 使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度. 
    - 时间片轮转调度算法(RR, Round-Robin): 时间片轮转调度是一种最古老, 最简单, 最公平且使用最广的算法. 每个进程被分配一个时间段, 称作它的时间片, 即该进程允许运行的时间. 
    - 多级反馈队列调度算法(MFQ, Multi-level Feedback Queue): 前面介绍的几种进程调度的算法都有一定的局限性. 如短进程优先的调度算法, 仅照顾了短进程而忽略了长进程. 多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业(进程)迅速完成, 因而它是目前被公认的一种较好的进程调度算法, UNIX 操作系统采取的便是这种调度算法. 
    - 优先级调度算法(Priority): 为每个流程分配优先级, 首先执行具有最高优先级的进程, 依此类推. 具有相同优先级的进程以 FCFS 方式执行. 可以根据内存要求, 时间要求或任何其他资源要求来确定优先级. 

- 18 什么是僵尸进程和孤儿进程?
    - 在 Unix/Linux 系统中, 子进程通常是通过 fork()系统调用创建的, 该调用会创建一个新的进程, 该进程是原有进程的一个副本. 子进程和父进程的运行是相互独立的, 它们各自拥有自己的 PCB, 即使父进程结束了, 子进程仍然可以继续运行. 
    - 当一个进程调用 exit()系统调用结束自己的生命时, 内核会释放该进程的所有资源, 包括打开的文件, 占用的内存等, 但是该进程对应的 PCB 依然存在于系统中. 这些信息只有在父进程调用 wait()或 waitpid()系统调用时才会被释放, 以便让父进程得到子进程的状态信息. 
    - 这样的设计可以让父进程在子进程结束时得到子进程的状态信息, 并且可以防止出现"僵尸进程"(即子进程结束后 PCB 仍然存在但父进程无法得到状态信息的情况).
    - 僵尸进程: 子进程已经终止, 但是其父进程仍在运行, 且父进程没有调用 wait()或 waitpid()等系统调用来获取子进程的状态信息, 释放子进程占用的资源, 导致子进程的 PCB 依然存在于系统中, 但无法被进一步使用. 这种情况下, 子进程被称为"僵尸进程". 避免僵尸进程的产生, 父进程需要及时调用 wait()或 waitpid()系统调用来回收子进程. 
    - 孤儿进程: 一个进程的父进程已经终止或者不存在, 但是该进程仍在运行. 这种情况下, 该进程就是孤儿进程. 孤儿进程通常是由于父进程意外终止或未及时调用 wait()或 waitpid()等系统调用来回收子进程导致的. 为了避免孤儿进程占用系统资源, 操作系统会将孤儿进程的父进程设置为 init 进程(进程号为 1), 由 init 进程来回收孤儿进程的资源.

- 19 如何查看是否有僵尸进程?
    - Linux 下可以使用 Top 命令查找, zombie 值表示僵尸进程的数量, 为 0 则代表没有僵尸进程. 
    - 定位僵尸进程以及该僵尸进程的父进程:
        `ps -A -ostat,ppid,pid,cmd |grep -e '^[Zz]'`


- 20 **什么是死锁?**
    - 死锁(Deadlock)描述的是这样一种情况: 多个进程/线程同时被阻塞, 它们中的一个或者全部都在等待某个资源被释放. 由于进程/线程被无限期地阻塞, 因此程序不可能正常终止.
    - 假设有两个进程 A 和 B, 以及两个资源 X 和 Y, 它们的分配情况如下: 

    | 进程 | 占用资源 | 需求资源 |
    |-----|-----|-----|
    | A | X | Y |
    | B | Y | X |
    - 此时, 进程 A 占用资源 X 并且请求资源 Y, 而进程 B 已经占用了资源 Y 并请求资源 X. 两个进程都在等待对方释放资源, 无法继续执行, 陷入了死锁状态. 

- 21 **产生死锁的四个必要条件是什么?**
    - 互斥: 资源必须处于非共享模式, 即一次只有一个进程可以使用. 如果另一进程申请该资源, 那么必须等待直到该资源被释放为止. 
    - 占有并等待: 一个进程至少应该占有一个资源, 并等待另一资源, 而该资源被其他进程所占有. 
    - 非抢占: 资源不能被抢占. 只能在持有资源的进程完成任务后, 该资源才会被释放. 
    - 循环等待: 有一组等待进程 ${P_0, P_1, ..., P_n}$,  $P_0$ 等待的资源被 $P_1$ 占有, $P_1$ 等待的资源被 $P_2$ 占有, ..., $P{n-1}$ 等待的资源被 $P_n$ 占有, $P_n$ 等待的资源被 $P_0$ 占有. 
    - 注意: 这四个条件是产生死锁的 必要条件 , 也就是说只要系统发生死锁, 这些条件必然成立, 而只要上述条件之一不满足, 就不会发生死锁. 
    ![Untitled](operating_system/deadlock.png)
    - 模拟产生死锁的代码
        ```
        public class DeadLockDemo {
            private static Object resource1 = new Object();//资源 1
            private static Object resource2 = new Object();//资源 2

            public static void main(String[] args) {
                new Thread(() -> {
                    synchronized (resource1) {
                        System.out.println(Thread.currentThread() + "get resource1");
                        try {
                            Thread.sleep(1000);
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                        System.out.println(Thread.currentThread() + "waiting get resource2");
                        synchronized (resource2) {
                            System.out.println(Thread.currentThread() + "get resource2");
                        }
                    }
                }, "线程 1").start();

                new Thread(() -> {
                    synchronized (resource2) {
                        System.out.println(Thread.currentThread() + "get resource2");
                        try {
                            Thread.sleep(1000);
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                        System.out.println(Thread.currentThread() + "waiting get resource1");
                        synchronized (resource1) {
                            System.out.println(Thread.currentThread() + "get resource1");
                        }
                    }
                }, "线程 2").start();
            }
        }
        Output

        Thread[线程 1,5,main]get resource1
        Thread[线程 2,5,main]get resource2
        Thread[线程 1,5,main]waiting get resource2
        Thread[线程 2,5,main]waiting get resource1
        ```

    - 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁, 然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁. 线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源, 然后这两个线程就会陷入互相等待的状态, 这也就产生了死锁. 

- 22 **解决死锁的方法**
    - 预防: 采用某种策略, 限制并发进程对资源的请求, 从而使得死锁的必要条件在系统执行的任何时间上都不满足. 
    - 避免: 系统在分配资源时, 根据资源的使用情况提前做出预测, 从而避免死锁的发生
    - 检测: 系统设有专门的机构, 当死锁发生时, 该机构能够检测死锁的发生, 并精确地确定与死锁有关的进程和资源. 
    - 解除: 与检测相配套的一种措施, 用于将进程从死锁状态下解脱出来.
    
- 23 **死锁的预防**
    - 申请共享资源, 破坏第一个条件互斥条件, 使得资源是可以同时访问的, 这是种简单的方法, 磁盘就可以用这种方法管理, 但是我们要知道, 有很多资源 往往是不能同时访问的, 所以这种做法在大多数的场合是行不通的. 
    - 申请可剥夺资源, 采用*剥夺式调度算法*, 但剥夺式调度方法目前一般仅适用于*主存资源*和*处理器资源*的分配, 并不适用于所有的资源, 会导致*资源利用率下降*. 
    - 静态分配: 静态分配策略静态分配策略可以破坏死锁产生的第二个条件(占有并等待). 所谓静态分配策略, 就是指一个进程必须在执行前就申请到它所需要的全部资源, 并且知道它所要的资源都得到满足之后才开始执行. 进程要么占有所有的资源然后开始执行, 要么不占有资源, 不会出现占有一些资源等待一些资源的情况. 静态分配策略逻辑简单, 实现也很容易, 但这种策略 严重地降低了资源利用率, 因为在每个进程所占有的资源中, 有些资源是在比较靠后的执行时间里采用的, 甚至有些资源是在额外的情况下才使用的, 这样就可能造成一个进程占有了一些 几乎不用的资源而使其他需要该资源的进程产生等待 的情况.
    - 层次分配策略: 层次分配策略破坏了产生死锁的第四个条件(循环等待). 在层次分配策略下, 所有的资源被分成了多个层次, 一个进程得到某一次的一个资源后, 它只能再申请较高一层的资源;当一个进程要释放某层的一个资源时, 必须先释放所占用的较高层的资源, 按这种策略, 是不可能出现循环等待链的, 因为那样的话, 就出现了已经申请了较高层的资源, 反而去申请了较低层的资源, 不符合层次分配策略.

- 24 **死锁的避免**
    - 上面提到的*破坏*死锁产生的四个必要条件之一就可以成功预防系统发生死锁, 但是会导致低效的进程运行和资源使用率. 而死锁的避免相反, 它的角度是允许系统中同时存在四个必要条件, 只要掌握并发进程中与每个进程有关的资源动态申请情况, 做出明智和合理的选择, 仍然可以避免死锁, 因为四大条件仅仅是产生死锁的必要条件.
    - 我们将系统的状态分为*安全状态*和*不安全状态*, 每当在为申请者分配资源前先测试系统状态, 若把系统资源分配给申请者会产生死锁, 则拒绝分配, 否则接受申请, 并为它分配资源.
        - 如果操作系统能够保证所有的进程在有限的时间内得到需要的全部资源, 则称系统处于安全状态, 否则说系统是不安全的. 很显然, 系统处于安全状态则不会发生死锁, 系统若处于不安全状态则可能发生死锁.
    - 如何保证系统保持在安全状态呢
        - 通过算法, 其中最具有代表性的 避免死锁算法 就是 Dijkstra 的银行家算法, 银行家算法用一句话表达就是: 当一个进程申请使用资源的时候, 银行家算法 通过先 试探 分配给该进程资源, 然后通过 安全性算法 判断分配后系统是否处于安全状态, 若不安全则试探分配作废, 让该进程继续等待, 若能够进入到安全的状态, 则就 真的分配资源给该进程.
        - reference: 银行家算法, https://blog.csdn.net/qq_33414271/article/details/80245715
        - 死锁的避免(银行家算法)改善了 资源使用率低的问题 , 但是它要不断地检测每个进程对各类资源的占用和申请情况, 以及做 安全性检查 , 需要花费较多的时间. 

- 25 **死锁的检测**
    - 对资源的分配加以限制可以预防和避免死锁的发生, 但是都不利于各进程对系统资源的充分共享. 解决死锁问题的另一条途径是死锁检测和解除.
    - 这种方法对资源的分配不加以任何限制, 也不采取死锁避免措施, 但系统定时地运行一个"死锁检测" 的程序, 判断系统内是否出现死锁, 如果检测到系统发生了死锁, 再采取措施去解除它. 

    - 操作系统中的每一刻时刻的系统状态都可以用进程-资源分配图来表示, 进程-资源分配图是描述进程和资源申请及分配关系的一种有向图, 可用于检测系统是否处于死锁状态. 
    - 用一个方框表示每一个资源类, 方框中的黑点表示该资源类中的各个资源, 用一个圆圈表示每一个进程, 用 有向边 来表示进程申请资源和资源被分配的情况. 
    - 图中 2-21 是进程-资源分配图的一个例子, 其中共有三个资源类, 每个进程的资源占有和申请情况已清楚地表示在图中. 在这个例子中, 由于存在占有和等待资源的环路, 导致一组进程永远处于等待资源的状态, 发生了死锁.

    ![Untitled](operating_system/resource_allocation.png)

    - 进程-资源分配图中存在环路并不一定是发生了死锁. 因为循环等待资源仅仅是死锁发生的必要条件, 而不是充分条件. 图 2-22 便是一个有环路而无死锁的例子. 虽然进程 P1 和进程 P3 分别占用了一个资源 R1 和一个资源 R2, 并且因为等待另一个资源 R2 和另一个资源 R1 形成了环路, 但进程 P2 和进程 P4 分别占有了一个资源 R1 和一个资源 R2, 它们申请的资源得到了满足, 在有限的时间里会归还资源, 于是进程 P1 或 P3 都能获得另一个所需的资源, 环路自动解除, 系统也就不存在死锁状态了. 

- 27 **死锁检测步骤**
    - 如果进程-资源分配图中无环路, 则此时系统没有发生死锁.
    - 如果进程-资源分配图中有环路, 且每个资源类仅有一个资源, 则系统中已经发生了死锁.
    - 如果进程-资源分配图中有环路, 且涉及到的资源类有多个资源, 此时系统未必会发生死锁. 如果能在进程-资源分配图中找出一个*既不阻塞又非独立的进程*, 该进程能够在有限的时间内归还占有的资源, 也就是把边给消除掉了, 重复此过程, 直到能在有限的时间内消除所有的边, 则不会发生死锁, 否则会发生死锁. (消除边的过程类似于拓扑排序)

- 28 死锁的解除
    - 当死锁检测程序检测到存在死锁发生时, 应设法让其解除, 让系统从死锁状态中恢复过来, 常用的解除死锁的方法有以下四种:
        - 立即结束所有进程的执行, 重新启动操作系统: 这种方法简单, 但以前所在的工作全部作废, 损失很大. 
        - 撤销涉及死锁的所有进程, 解除死锁后继续运行: 这种方法能彻底打破死锁的循环等待条件, 但将付出很大代价, 例如有些进程可能已经计算了很长时间, 由于被撤销而使产生的部分结果也被消除了, 再重新执行时还要再次进行计算. 
        - 逐个撤销涉及死锁的进程, 回收其资源直至死锁解除. 
        - 抢占资源: 从涉及死锁的一个或几个进程中抢占资源, 把夺得的资源再分配给涉及死锁的进程直至死锁解除. 

- 29 内存管理
    - 内存的分配与回收: 对进程所需的内存进行分配和释放, malloc 函数: 申请内存, free 函数: 释放内存. 
    - 地址转换: 将程序中的虚拟地址转换成内存中的物理地址. 
    - 内存扩充: 当系统没有足够的内存时, 利用虚拟内存技术或自动覆盖技术, 从逻辑上扩充内存. 
    - 内存映射: 将一个文件直接映射到进程的进程空间中, 这样可以通过内存指针用读写内存的办法直接存取文件内容, 速度更快. 
    - 内存优化: 通过调整内存分配策略和回收算法来优化内存使用效率. 
    - 内存安全: 保证进程之间使用内存互不干扰, 避免一些恶意程序通过修改内存来破坏系统的安全性. 

- 30 什么是内存碎片?
    - 内存碎片是由内存的申请和释放产生的, 通常分为下面两种: 
        - 内部内存碎片(Internal Memory Fragmentation, 简称为内存碎片): 已经分配给进程使用但未被使用的内存. 导致内部内存碎片的主要原因是, 当采用固定比例比如 2 的幂次方进行内存分配时, 进程所分配的内存可能会比其实际所需要的大. 举个例子, 一个进程只需要 65 字节的内存, 但为其分配了 128(2^7) 大小的内存, 那 63 字节的内存就成为了内部内存碎片.
        - 外部内存碎片(External Memory Fragmentation, 简称为外部碎片): 由于未分配的连续内存区域太小, 以至于不能满足任意进程所需要的内存分配请求, 这些小片段且不连续的内存空间被称为外部碎片. 也就是说, 外部内存碎片指的是那些并未分配给进程但又不能使用的内存. 我们后面介绍的分段机制就会导致外部内存碎片.
    - 内存碎片会导致内存利用率下降, 如何减少内存碎片是内存管理要非常重视的一件事情. 

- 31 常见的内存管理方式有哪些?
    - 内存管理方式可以简单分为下面两种:
        - 连续内存管理: 为一个用户程序分配一个连续的内存空间, 内存利用率一般不高. 
        - 非连续内存管理: 允许一个程序使用的内存分布在离散或者说不相邻的内存中, 相对更加灵活一些. 

    - 连续内存管理
        - 块式管理: 早期计算机操作系统的一种连续内存管理方式, 存在严重的内存碎片问题. 块式管理会将内存分为几个固定大小的块, 每个块中只包含一个进程. 如果程序运行需要内存的话, 操作系统就分配给它一块, 如果程序运行只需要很小的空间的话, 分配的这块内存很大一部分几乎被浪费了. 这些在每个块中未被利用的空间, 我们称之为内部内存碎片. 除了内部内存碎片之外, 由于两个内存块之间可能还会有外部内存碎片, 这些不连续的外部内存碎片由于太小了无法再进行分配.
        - 在 Linux 系统中, 连续内存管理采用了*伙伴系统(Buddy System)算法*来实现, 这是一种经典的连续内存分配算法, 可以有效解决外部内存碎片的问题. 伙伴系统的主要思想是将内存按 2 的幂次划分(每一块内存大小都是 2 的幂次比如 $2^6=64$ KB), 并将相邻的内存块组合成一对伙伴(必须是相邻的才是伙伴).
        - 当进行内存分配时, 伙伴系统会尝试找到大小最合适的内存块. 如果找到的内存块过大, 就将其一分为二, 分成两个大小相等的伙伴块. 如果还是大的话, 就继续切分, 直到到达合适的大小为止.
        - 假设两块相邻的内存块都被释放, 系统会将这两个内存块合并, 进而形成一个更大的内存块, 以便后续的内存分配. 这样就可以减少内存碎片的问题, 提高内存利用率. 
        虽然解决了外部内存碎片的问题, 但伙伴系统仍然存在内存利用率不高的问题(内部内存碎片). 这主要是因为伙伴系统只能分配大小为 $2^n$ 的内存块, 因此当需要分配的内存大小不是 2^n 的整数倍时, 会浪费一定的内存空间. 举个例子: 如果要分配 65 大小的内存快, 依然需要分配 $2^7=128$ 大小的内存块. 
        
        ![Untitled](operating_system/bud_system.png)

- 32 非连续内存管理
    - 非连续内存管理存在下面 3 种方式: 
        - 段式管理: 以段(一段连续的物理内存)的形式管理/分配物理内存. 应用程序的虚拟地址空间被分为大小不等的段, 段是有实际意义的, 每个段定义了一组逻辑信息, 例如有主程序段 MAIN, 子程序段 X, 数据段 D 及栈段 S 等. 
        - 页式管理: 把物理内存分为连续等长的物理页, 应用程序的虚拟地址空间也被划分为连续等长的虚拟页, 是现代操作系统广泛使用的一种内存管理方式. 
        - 段页式管理机制: 结合了段式管理和页式管理的一种内存管理机制, 把物理内存先分成若干段, 每个段又继续分成若干大小相等的页. 

- 33 什么是虚拟内存?有什么用?
    - 虚拟内存(Virtual Memory)是计算机系统内存管理非常重要的一个技术, 本质上来说它只是逻辑存在的, 是一个假想出来的内存空间, 主要作用是作为进程访问主存(物理内存)的桥梁并简化内存管理. 虚拟内存作为进程访问主存的桥梁.
        ![Untitled](operating_system/virtual_memory.png)
        - 隔离进程: 物理内存通过虚拟地址空间访问, 虚拟地址空间与进程一一对应. 每个进程都认为自己拥有了整个物理内存, 进程之间彼此隔离, 一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存. 
        - 提升物理内存利用率: 有了虚拟地址空间后, 操作系统只需要将进程当前正在使用的部分数据或指令加载入物理内存. 
        - 简化内存管理: 进程都有一个一致且私有的虚拟地址空间, 程序员不用和真正的物理内存打交道, 而是借助虚拟地址空间访问物理内存, 从而简化了内存管理. 
        - 多个进程共享物理内存: 进程在运行过程中, 会加载许多操作系统的动态库. 这些库对于每个进程而言都是公用的, 它们在内存中实际只会加载一份, 这部分称为共享内存. 
        - 提高内存使用安全性: 控制进程对物理内存的访问, 隔离不同进程的访问权限, 提高系统的安全性. 
        - 提供更大的可使用内存空间: 可以让程序拥有超过系统物理内存大小的可用内存空间. 这是因为当物理内存不够用时, 可以利用磁盘充当, 将物理内存页(通常大小为 4 KB)保存到磁盘文件(会影响读写速度), 数据或代码页会根据需要在物理内存与磁盘之间移动. 

- 34 没有虚拟内存有什么问题?
    - 如果没有虚拟内存的话, 程序直接访问和操作的都是物理内存, 看似少了一层中介, 但多了很多问题.
        - 用户程序可以访问任意物理内存, 可能会不小心操作到系统运行必需的内存, 进而造成操作系统崩溃, 严重影响系统的安全.
        - 同时运行多个程序容易崩溃. 比如你想同时运行一个微信和一个 QQ音乐, 微信在运行的时候给内存地址 1xxx 赋值后, QQ 音乐也同样给内存地址 1xxx 赋值, 那么 QQ 音乐对内存的赋值就会覆盖微信之前所赋的值, 这就可能会造成微信这个程序会崩溃.
        - 程序运行过程中使用的所有数据或指令都要载入物理内存, 根据局部性原理, 其中很大一部分可能都不会用到, 白白占用了宝贵的物理内存资源.

- 35 什么是虚拟地址和物理地址?
    ![Untitled](operating_system/physical_address.png)
    - 物理地址(Physical Address)是真正的物理内存中地址, 更具体点来说是内存地址寄存器中的地址. 程序中访问的内存地址不是物理地址, 而是 虚拟地址(Virtual Address). 
        - 我们编程开发的时候实际就是在和虚拟地址打交道. 在c语言中, 指针里面存储的数值就可以理解成为内存里的一个地址, 这个地址也就是我们说的虚拟地址.
        - 操作系统一般通过CPU芯片中的一个重要组件*MMU(Memory Management Unit, 内存管理单元)*将虚拟地址转换为物理地址, 这个过程被称为*地址翻译/地址转换(Address Translation)*.
    - 地址翻译过程
        通过 MMU 将虚拟地址转换为物理地址后, 再通过总线传到物理内存设备, 进而完成相应的物理内存读写请求.
        MMU 将虚拟地址翻译为物理地址的主要机制有两种: 分段机制 和 分页机制 . 

- 36 什么是虚拟地址空间和物理地址空间?
    - 虚拟地址空间是虚拟地址的集合, 是虚拟内存的范围. 每一个进程都有一个一致且私有的虚拟地址空间.
    - 物理地址空间是物理地址的集合, 是物理内存的范围.

- 37 虚拟地址与物理内存地址是如何映射的?
    - MMU 将虚拟地址翻译为物理地址的主要机制有3种: 分段机制, 分页机制, 端页机制.

- 38 分段机制

    - 分段机制(Segmentation)以段(一段 连续 的物理内存)的形式管理/分配物理内存. 应用程序的虚拟地址空间被分为大小不等的段, 段是有实际意义的, 每个段定义了一组逻辑信息, 例如有主程序段 `MAIN`, 子程序段 `X`, 数据段 `D` 及栈段 `S` 等.
    - 分段管理通过 段表(Segment Table) 映射虚拟地址和物理地址.
    - 分段机制下的虚拟地址由两部分组成: 
        - 段号: 标识着该虚拟地址属于整个虚拟地址空间中的哪一个段. 
        - 段内偏移量: 相对于该段起始地址的偏移量. 
    - 地址翻译:
        - MMU 首先解析得到虚拟地址中的段号;
        - 通过段号去该应用程序的段表中取出对应的段信息(找到对应的段表项);
        - 从段信息中取出该段的起始地址(物理地址)加上虚拟地址中的段内偏移量得到最终的物理地址. 
    
    ![Untitled](operating_system/segment_virtual.png)

    - 段表中还存有诸如段长(可用于检查虚拟地址是否超出合法范围), 段类型(该段的类型, 例如代码段, 数据段等)等信息. 

    - 通过段号不一定能找到对应的段表项吗, 最终的物理地址对应的物理内存也不一定存在.
        - 段表项被删除: 软件错误, 软件恶意行为等情况可能会导致段表项被删除. 
        - 段表项还未创建: 如果系统内存不足或者无法分配到连续的物理内存块就会导致段表项无法被创建. 

    - 分段机制会导致内存外部碎片
        - 分段机制容易出现外部内存碎片, 即在段与段之间留下碎片空间(不足以映射给虚拟地址空间中的段). 从而造成物理内存资源利用率的降低.
        
        ![Untitled](operating_system/segment_fragmentation.png)
        - 假设可用物理内存为 5G 的系统使用分段机制分配内存. 现在有 4 个进程, 每个进程的内存占用情况如下: 
            - 进程 1: 0~1G(第 1 段)
            - 进程 2: 1~3G(第 2 段)
            - 进程 3: 3~4.5G(第 3 段)
            - 进程 4: 4.5~5G(第 4 段)
        - 此时, 我们关闭了进程 1 和进程 4, 则第 1 段和第 4 段的内存会被释放, 空闲物理内存还有 1.5G. 由于这 1.5G 物理内存并不是连续的, 导致没办法将空闲的物理内存分配给一个需要 1.5G 物理内存的进程.

- 39 分页机制
    - 分页机制(Paging) 把主存(物理内存)分为连续等长的物理页, 应用程序的虚拟地址空间划也被分为连续等长的虚拟页. 现代操作系统广泛采用分页机制. 这里的页是连续等长的, 不同于分段机制下不同长度的段.
    - 在分页机制下, 应用程序虚拟地址空间中的任意虚拟页可以被映射到物理内存中的任意物理页上, 因此可以实现物理内存资源的离散分配. 分页机制按照固定页大小分配物理内存, 使得物理内存资源易于管理, 可有效避免分段机制中外部内存碎片的问题. 

- 40 页表有什么用?地址翻译过程是怎样的?
    - 分页管理通过 页表(Page Table) 映射虚拟地址和物理地址. 下图是基于单级页表进行地址翻译的示意图. 
    ![Untitled](operating_system/page_table.png)

    - 在分页机制下, 每个进程都会有一个对应的页表.
    - 分页机制下的虚拟地址由两部分组成:
        - 页号: 通过虚拟页号可以从页表中取出对应的物理页号;
        - 页内偏移量: 物理页起始地址+页内偏移量=物理内存地址. 
    - 具体的地址翻译过程如下: 
        - MMU 首先解析得到虚拟地址中的虚拟页号;
        - 通过虚拟页号去该应用程序的页表中取出对应的物理页号(找到对应的页表项);
        - 用该物理页号对应的物理页起始地址(物理地址)加上虚拟地址中的页内偏移量得到最终的物理地址. 
    页表中还存有诸如访问标志(标识该页面有没有被访问过), 脏数据标识位等信息. 

- 40 单级页表有什么问题?为什么需要多级页表?
    - 以 32 位的环境为例, 虚拟地址空间范围共有 $2^32$(4G). 假设 一个页的大小是 $2^12$(4KB), 那页表项共有 $\frac{4G}{4K} = 2^20$ 个. 每个页表项为一个地址, 占用 4 字节, $2^{20} \times \frac{2^2}{1024} \times 1024 $ = 4MB. 也就是说一个程序啥都不干, 页表大小就得占用 4M. 

    - 系统运行的应用程序多起来的话, 页表的开销还是非常大的. 而且, 绝大部分应用程序可能只能用到页表中的几项, 其他的白白浪费了. 为了解决这个问题, 操作系统引入了 多级页表 , 多级页表对应多个页表, 每个页表与前一个页表相关联. 32 位系统一般为二级页表, 64 位系统一般为四级页表. 

    - 这里以二级页表为例进行介绍: 二级列表分为一级页表和二级页表. 一级页表共有 1024 个页表项, 一级页表又关联二级页表, 二级页表同样共有 1024 个页表项. 二级页表中的一级页表项是一对多的关系, 二级页表按需加载(只会用到很少一部分二级页表), 进而节省空间占用. 
    假设只需要 2 个二级页表, 那两级页表的内存占用情况为: 4KB(一级页表占用) + 4KB * 2(二级页表占用) = 12 KB. 

    ![Untitled](operating_system/multilevel_page_table.png)

    多级页表属于时间换空间的典型场景, 利用增加页表查询的次数减少页表占用的空间. 

- 41 **TLB(Translation Lookaside Buffer)有什么用? 使用 TLB 之后的地址翻译流程是怎样的?**
    - 为了提高虚拟地址到物理地址的转换速度, 操作系统在*页表方案*基础之上引入了*转址旁路缓存(Translation Lookaside Buffer, TLB, 也被称为快表)*.
    ![Untitled](operating_system/tlb.png)

    - 在主流的AArch64和x86-64体系结构下, TLB属于MMU(Memory Management Unit, 内存管理单元)内部的单元, 本质上就是一块高速缓存(Cache), 缓存了虚拟页号到物理页号的映射关系, 你可以将其简单看作是存储着键(虚拟页号)值(物理页号)对的哈希表.

    - 使用 TLB 之后的地址翻译流程是这样的:
        - 用虚拟地址中的虚拟页号作为 key 去 TLB 中查询;
        - 如果能查到对应的物理页的话, 就不用再查询页表了, 这种情况称为 TLB 命中(TLB hit). 
        - 如果不能查到对应的物理页的话, 还是需要去查询主存中的页表, 同时将页表中的该映射表项添加到 TLB 中, 这种情况称为 TLB 未命中(TLB miss). 
        - 当 TLB 填满后, 又要登记新页时, 就按照一定的淘汰策略淘汰掉快表中的一个页. 
    ![Untitled](operating_system/tlb_translate.png)

    - 由于页表也在主存中, 因此在没有 TLB 之前, 每次读写内存数据时 CPU 要访问两次主存. 有了 TLB 之后, 对于存在于 TLB 中的页表数据只需要访问一次主存即可.
    - TLB 的设计思想非常简单, 但命中率往往非常高, 效果很好. 这就是因为被频繁访问的页就是其中的很小一部分.

- 42 **换页机制有什么用?**
    - 换页机制的思想是当物理内存不够用的时候, 操作系统选择将一些物理页的内容放到磁盘上去, 等要用到的时候再将它们读取到物理内存中. 也就是说, 换页机制利用磁盘这种较低廉的存储设备扩展的物理内存.
    - 这也就解释了一个日常使用电脑常见的问题: 为什么操作系统中所有进程运行所需的物理内存即使比真实的物理内存要大一些, 这些进程也是可以正常运行的, 只是运行速度会变慢.
    - 这同样是一种时间换空间的策略, 你用 CPU 的计算时间, 页的调入调出花费的时间, 换来了一个虚拟的更大的物理内存空间来支持程序的运行. 

- 43 **什么是页缺失?**
    - 页缺失(Page Fault, 又名硬错误, 硬中断, 分页错误, 寻页缺失, 缺页中断, 页故障等)指的是当软件试图访问已映射在虚拟地址空间中, 但是目前并未被加载在物理内存中的一个分页时, 由 MMU 所发出的中断.
    - 常见的页缺失有下面这两种: 
        - 硬性页缺失(Hard Page Fault): 物理内存中没有对应的物理页. 于是, Page Fault Handler 会指示 CPU 从已经打开的磁盘文件中读取相应的内容到物理内存, 而后交由 MMU 建立相应的虚拟页和物理页的映射关系.
        - 软性页缺失(Soft Page Fault): 物理内存中有对应的物理页, 但虚拟页还未和物理页建立映射. 于是, Page Fault Handler 会指示 MMU 建立相应的虚拟页和物理页的映射关系.
    - 发生上面这两种缺页错误的时候, 应用程序访问的是有效的物理内存, 只是出现了物理页缺失或者虚拟页和物理页的映射关系未建立的问题. 如果应用程序访问的是无效的物理内存的话, 还会出现*无效缺页错误(Invalid Page Fault)*.

- 常见的页面置换算法有哪些?
    - 当发生硬性页缺失时, 如果物理内存中没有空闲的物理页面可用的话. 操作系统就必须将物理内存中的一个物理页淘汰出去, 这样就可以腾出空间来加载新的页面了.
    - 用来选择淘汰哪一个物理页的规则叫做*页面置换算法*, 我们可以把页面置换算法看成是淘汰物物理页的规则. 
    - 页缺失太频繁的发生会非常影响性能, 一个好的页面置换算法应该是可以减少页缺失出现的次数.
    ![Untitled](operating_system/page_switch.png)
    - 常见的页面置换算法有下面这 5 种(其他还有很多页面置换算法都是基于这些算法改进得来的):
        - 最佳页面置换算法(OPT, Optimal): 优先选择淘汰的页面是以后永不使用的, 或者是在最长时间内不再被访问的页面, 这样可以保证获得最低的缺页率. 但由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间内不再被访问的, 因而该算法无法实现, 只是理论最优的页面置换算法, 可以作为衡量其他置换算法优劣的标准. 
        - 先进先出页面置换算法(FIFO, First In First Out) : 最简单的一种页面置换算法, 总是淘汰最先进入内存的页面, 即选择在内存中驻留时间最久的页面进行淘汰. 该算法易于实现和理解, 一般只需要通过一个 FIFO 队列即可满足需求. 不过, 它的性能并不是很好. 
        - 最近最久未使用页面置换算法(LRU , Least Recently Used): LRU 算法赋予每个页面一个访问字段, 用来记录一个页面自上次被访问以来所经历的时间 T, 当须淘汰一个页面时, 选择现有页面中其 T 值最大的, 即最近最久未使用的页面予以淘汰. LRU 算法是根据各页之前的访问情况来实现, 因此是易于实现的. OPT 算法是根据各页未来的访问情况来实现, 因此是不可实现的. 
        - 最少使用页面置换算法(LFU, Least Frequently Used) : 和 LRU 算法比较像, 不过该置换算法选择的是之前一段时间内使用最少的页面作为淘汰页. 
        - 时钟页面置换算法(Clock): 可以认为是一种最近未使用算法, 即逐出的页面都是最近没有使用的那个. 

- 45 **FIFO 页面置换算法性能为何不好?**
    - 1 经常访问或者需要长期存在的页面会被频繁调入调出: 较早调入的页往往是经常被访问或者需要长期存在的页, 这些页会被反复调入和调出. 
    - 2 存在 Belady 现象: 被置换的页面并不是进程不会访问的, 有时就会出现分配的页面数增多但缺页率反而提高的异常现象. 出现该异常的原因是因为 FIFO 算法只考虑了页面进入内存的顺序, 而没有考虑页面访问的频率和紧迫性. 

- 46 **哪一种页面置换算法实际用的比较多?**
    - LRU 算法是实际使用中应用的比较多, 也被认为是最接近 OPT 的页面置换算法. 
    - 不过, 需要注意的是, 实际应用中这些算法会被做一些改进, 就比如 InnoDB Buffer Pool( InnoDB 缓冲池, MySQL 数据库中用于管理缓存页面的机制)就改进了传统的 LRU 算法, 使用了一种称为"Adaptive LRU"的算法(同时结合了 LRU 和 LFU 算法的思想). 

- 47 **分页机制和分段机制有哪些共同点和区别?**
    - 共同点
        - 都是非连续内存管理的方式. 
        - 都采用了地址映射的方法, 将虚拟地址映射到物理地址, 以实现对内存的管理和保护. 
    
    - 区别
        - 分页机制以页面为单位进行内存管理, 而分段机制以段为单位进行内存管理. 页的大小是固定的, 由操作系统决定, 通常为 2 的幂次方. 而段的大小不固定, 取决于我们当前运行的程序. 
        - 页是物理单位, 即操作系统将物理内存划分成固定大小的页面, 每个页面的大小通常是 2 的幂次方, 例如 4KB, 8KB 等等. 而段则是逻辑单位, 是为了满足程序对内存空间的逻辑需求而设计的, 通常根据程序中数据和代码的逻辑结构来划分. 
        - 分段机制容易出现外部内存碎片, 即在段与段之间留下碎片空间(不足以映射给虚拟地址空间中的段). 分页机制解决了外部内存碎片的问题, 但仍然可能会出现内部内存碎片. 
        - 分页机制采用了页表来完成虚拟地址到物理地址的映射, 页表通过一级页表和二级页表来实现多级映射; 而分段机制则采用了段表来完成虚拟地址到物理地址的映射, 每个段表项中记录了该段的起始地址和长度信息. 
        - 分页机制对程序没有任何要求, 程序只需要按照虚拟地址进行访问即可; 而分段机制需要程序员将程序分为多个段, 并且显式地使用段寄存器来访问不同的段. 


- 48 **段页机制**
    - 结合了段式管理和页式管理的一种内存管理机制, 把物理内存先分成若干段, 每个段又继续分成若干大小相等的页.
    - 在段页式机制下, 地址翻译的过程分为两个步骤: 
        - 段式地址映射.
        - 页式地址映射.

- 49 **局部性原理**
    - 要想更好地理解虚拟内存技术, 必须要知道计算机中著名的*局部性原理(Locality Principle)*. 另外, 局部性原理既适用于程序结构, 也适用于数据结构, 是非常重要的一个概念.
    - 局部性原理是指在程序执行过程中, 数据和指令的访问存在一定的空间和时间上的局部性特点. 其中, *时间局部性*是指一个数据项或指令在一段时间内被反复使用的特点, *空间局部性*是指一个数据项或指令在一段时间内与其相邻的数据项或指令被反复使用的特点. 
    - 在分页机制中, 页表的作用是将虚拟地址转换为物理地址, 从而完成内存访问. 在这个过程中, 局部性原理的作用体现在两个方面:
        - 时间局部性: 由于程序中存在一定的循环或者重复操作, 因此会反复访问同一个页或一些特定的页, 这就体现了时间局部性的特点. 为了利用时间局部性, 分页机制中通常采用缓存机制来提高页面的命中率, 即将最近访问过的一些页放入缓存中, 如果下一次访问的页已经在缓存中, 就不需要再次访问内存, 而是直接从缓存中读取.
        - 空间局部性: 由于程序中数据和指令的访问通常是具有一定的空间连续性的, 因此当访问某个页时, 往往会顺带访问其相邻的一些页. 为了利用空间局部性, 分页机制中通常采用预取技术来预先将相邻的一些页读入内存缓存中, 以便在未来访问时能够直接使用, 从而提高访问速度.
    - 总之, 局部性原理是计算机体系结构设计的重要原则之一, 也是许多优化算法的基础. 在分页机制中, 利用时间局部性和空间局部性, 采用缓存和预取技术, 可以提高页面的命中率, 从而提高内存访问效率.

- 50 **文件系统主要做了什么?**
    - 文件系统主要负责管理和组织计算机存储设备上的文件和目录, 其功能包括以下几个方面:
        - 存储管理: 将文件数据存储到物理存储介质中, 并且管理空间分配, 以确保每个文件都有足够的空间存储, 并避免文件之间发生冲突. 
        - 文件管理: 文件的创建, 删除, 移动, 重命名, 压缩, 加密, 共享等等. 
        - 目录管理: 目录的创建, 删除, 移动, 重命名等等. 
        - 文件访问控制: 管理不同用户或进程对文件的访问权限, 以确保用户只能访问其被授权访问的文件, 以保证文件的安全性和保密性. 

- 51 **硬链接和软链接有什么区别?**
    - 在 Linux/类 Unix 系统上, 文件链接(File Link)是一种特殊的文件类型, 可以在文件系统中指向另一个文件. 常见的文件链接类型有两种: 
        - 1 硬链接(Hard Link)
            - 在 Linux/类 Unix 文件系统中, 每个文件和目录都有一个唯一的*索引节点(inode)*号, 用来标识该文件或目录. 硬链接通过 inode 节点号建立连接, 硬链接和源文件的 inode 节点号相同, 两者对文件系统来说是完全平等的(可以看作是互为硬链接, 源头是同一份文件), 删除其中任何一个对另外一个没有影响, 可以通过给文件设置硬链接文件来防止重要文件被误删. 
            - 只有删除了源文件和所有对应的硬链接文件, 该文件才会被真正删除. 
            - 硬链接具有一些限制, 不能对目录以及不存在的文件创建硬链接, 并且硬链接也不能跨越文件系统. 
            - ln 命令用于创建硬链接.
        - 2 软链接(Symbolic Link 或 Symlink)
            - 软链接和源文件的 inode 节点号不同, 而是指向一个文件路径. 
            - 源文件删除后, 软链接依然存在, 但是指向的是一个无效的文件路径. 
            - 软连接类似于 Windows 系统中的快捷方式. 
            - 不同于硬链接, 可以对目录或者不存在的文件创建软链接, 并且, 软链接可以跨越文件系统. 
            - ln -s 命令用于创建软链接.

- 52 **硬链接为什么不能跨文件系统?**
    - 硬链接是通过inode节点号建立连接的, 而硬链接和源文件共享相同的 inode 节点号.
    - 然而, 每个文件系统都有自己的独立inode表, 且每个inode表只维护该文件系统内的inode. 如果在不同的文件系统之间创建硬链接, 可能会导致inode节点号冲突的问题, 即目标文件的inode节点号已经在该文件系统中被使用.

- 53 **提高文件系统性能的方式有哪些?**
    - 优化硬件: 使用高速硬件设备(如 SSD, NVMe)替代传统的机械硬盘, 使用 RAID(Redundant Array of Inexpensive Disks)等技术提高磁盘性能. 
    - 选择合适的文件系统选型: 不同的文件系统具有不同的特性, 对于不同的应用场景选择合适的文件系统可以提高系统性能. 
    - 运用缓存: 访问磁盘的效率比较低, 可以运用缓存来减少磁盘的访问次数. 不过, 需要注意缓存命中率, 缓存命中率过低的话, 效果太差. 
    - 避免磁盘过度使用: 注意磁盘的使用率, 避免将磁盘用满, 尽量留一些剩余空间, 以免对文件系统的性能产生负面影响. 
    - 对磁盘进行合理的分区: 合理的磁盘分区方案, 能够使文件系统在不同的区域存储文件, 从而减少文件碎片, 提高文件读写性能. 

- 54 **常见的磁盘调度算法有哪些?**
    - 磁盘调度算法是操作系统中对磁盘访问请求进行排序和调度的算法, 其目的是提高磁盘的访问效率. 

    ![Untitled](operating_system/disk_scheduling.png)

    - 一次磁盘读写操作的时间由磁盘寻道/寻找时间, 延迟时间和传输时间决定. 磁盘调度算法可以通过改变到达磁盘请求的处理顺序, 减少磁盘寻道时间和延迟时间.
        - (First-Come First-Served, FCFS): 按照请求到达磁盘调度器的顺序进行处理, 先到达的请求的先被服务. FCFS 算法实现起来比较简单, 不存在算法开销. 不过, 由于没有考虑磁头移动的路径和方向, 平均寻道时间较长. 同时, 该算法容易出现饥饿问题, 即一些后到的磁盘请求可能需要等待很长时间才能得到服务. 
        - (Shortest Seek Time First, SSTF): 也被称为最佳服务优先(Shortest Service Time First, SSTF)算法, 优先选择距离当前磁头位置最近的请求进行服务. SSTF 算法能够最小化磁头的寻道时间, 但容易出现饥饿问题, 即磁头附近的请求不断被服务, 远离磁头的请求长时间得不到响应. 实际应用中, 需要优化一下该算法的实现, 避免出现饥饿问题. 
        - (SCAN): 也被称为电梯(Elevator)算法, 基本思想和电梯非常类似. 磁头沿着一个方向扫描磁盘, 如果经过的磁道有请求就处理, 直到到达磁盘的边界, 然后改变移动方向, 依此往复. SCAN 算法能够保证所有的请求得到服务, 解决了饥饿问题. 但是, 如果磁头从一个方向刚扫描完, 请求才到的话. 这个请求就需要等到磁头从相反方向过来之后才能得到处理. 
        - (Circular Scan, C-SCAN): SCAN 算法的变体, 只在磁盘的一侧进行扫描, 并且只按照一个方向扫描, 直到到达磁盘边界, 然后回到磁盘起点, 重新开始循环. 
        - (LOOK): SCAN 算法中磁头到了磁盘的边界才改变移动方向, 这样可能会做很多无用功, 因为磁头移动方向上可能已经没有请求需要处理了. LOOK 算法对 SCAN 算法进行了改进, 如果磁头移动方向上已经没有别的请求, 就可以立即改变磁头移动方向, 依此往复. 也就是边扫描边观察指定方向上还有无请求, 因此叫 LOOK. 
        - (C-LOOK): C-SCAN 只有到达磁盘边界时才能改变磁头移动方向, 并且磁头返回时也需要返回到磁盘起点, 这样可能会做很多无用功. C-LOOK 算法对 C-SCAN 算法进行了改进, 如果磁头移动的方向上已经没有磁道访问请求了, 就可以立即让磁头返回, 并且磁头只需要返回到有磁道访问请求的位置即可.

- 55 **在程序运行过程中, 不同类型的变量都存储在哪里?**  

    - 1 局部变量(Local Variables), 存储位置: 栈(Stack), 栈区.
        - 局部变量通常是在**函数内部**定义的变量, 只在函数调用期间存在. 
        - 存储在**栈**中, 分配速度快, 调用函数时自动分配, 函数结束后自动释放.   
        - 由于栈的大小有限, 过度使用会导致**栈溢出(Stack Overflow)**.   
        - 例子: 
            ```c
            void func() {
                int a = 10;  // a 是局部变量, 存储在栈上
            }
            ```
        - 栈内存分配快, 但大小有限, 适用于临时变量.

    - 2 全局变量(Global Variables), 存储位置: 数据段(Data Segment), 静态区.
        - 全局变量在程序的整个生命周期内存在, 可在多个函数之间共享.   
        - 存储在*数据段(Data Segment)*, 其中:
            - 已初始化的全局变量存储在`.data`段**(可读写).
            - 未初始化的全局变量存储在`.bss 段`(在程序启动时被初始化为 0).
        - 例子: 
            ```c
            int global_var = 100;  // 存在于 .data 段
            int uninitialized_var; // 存在于 .bss 段
            ```
        - 全局变量提高了数据共享, 但可能导致命名污染, 降低代码可维护性.

    - 3 动态申请数据(Dynamically Allocated Data), 存储位置: 堆(Heap), 堆区.
        - 通过 `malloc()`(C), `new`(C++), `malloc()` 或 `PyObject_Malloc()`(Python C 扩展)等动态分配函数分配的内存.   
        - 存储在堆(Heap)中, 其生命周期由程序员控制, 需要手动释放(如 `free()` 或 `delete`), 否则会造成内存泄漏.   
        - 适用于大数据存储, 但分配/释放速度比栈慢.
        - 例子: 
            ```c
            int* p = (int*)malloc(sizeof(int) * 10);  // p 指向堆上的内存
            free(p);  // 释放内存
            ```
        - 堆内存适合存储大对象, 但管理不当易造成内存泄漏.

    - Summary
        | 变量类型 | 存储位置 | 生命周期 | 访问方式 | 释放方式 |
        |----------|----------|----------|----------|----------|
        | **局部变量** | **栈(Stack)** | 仅在函数调用期间存在 | 仅函数内部可访问 | 函数返回时自动释放 |
        | **全局变量** | **数据段(Data Segment)** | 程序运行期间始终存在 | 所有函数可访问 | 程序退出时释放 |
        | **动态申请数据** | **堆(Heap)** | 直到 `free()` / `delete` | 由指针访问 | 需要手动释放 |

- 56 **什么是操作系统里的上下文切换(Context Switching)?**
    - 上下文切换(Context Switching)是指*操作系统在不同进程或线程之间切换 CPU 执行权*时, 保存当前任务的执行状态(上下文), 然后加载新任务的执行状态, 使CPU继续运行新的任务. 上下文切换是*多任务操作系统*(如 Linux, Windows)实现*并发执行*的核心机制, 使多个进程或线程可以共享CPU资源, 看起来像是*同时运行*.

    - 1 上下文(Context)是什么
        - 在 CPU 运行进程时, 它需要维护一系列状态信息, 包括: 
            - **通用寄存器(Registers)**: 如 `eax`, `ebx`, `ecx`(x86 架构).
            - **程序计数器(Program Counter, PC)**: 指向下一条要执行的指令地址.
            - **堆栈指针(Stack Pointer, SP)** 和 **基址指针(Base Pointer, BP)**: 用于管理函数调用和局部变量.
            - **CPU 状态寄存器(Flags, EFLAGS)**: 记录进程的状态(如中断标志, 零标志等).
            - **内存页表(Page Table)**: 映射进程的虚拟地址到物理地址.
        - 当系统执行上下文切换时, 需要**保存当前进程的这些状态, 并加载下一个进程的状态**, 从而恢复执行. 

    - 2 上下文切换的类型
        根据切换的对象不同, 上下文切换可分为以下三种类型: 
        - (1)进程上下文切换
            - 切换当前运行的进程, 即从进程 A 切换到进程 B. 
            - 需要切换虚拟内存页表(Page Table), 代价较高. 
            - 常见触发情况: 
                - 进程主动调用 `sleep()`, `wait()` 等操作, 进入等待状态.
                - 进程时间片(Time Slice)到达, 被调度器剥夺 CPU.
                - 进程发生 I/O 阻塞(如读取磁盘, 网络通信).
        - (2)线程上下文切换**
            - 切换**同一进程内的不同线程**. 
            - **不需要**切换页表, 因为线程共享进程的地址空间. 
            - 仅需保存和恢复**寄存器, 程序计数器, 堆栈指针**等信息, 因此开销比进程切换小. 
            - 发生在 **多线程调度** 时, 如: 
                ```c
                pthread_create(&thread, NULL, myFunction, NULL); // 线程创建
                ```
        - (3)中断上下文切换
            - 发生在**外部设备(如键盘, 磁盘, 网卡)触发中断**时. 
            - **中断处理程序(ISR, Interrupt Service Routine)** 会打断当前进程, 优先执行中断任务. 
            - 需要保存**当前进程的状态**, 执行完中断后再恢复. 
            - 例如: 
                - 用户按下键盘时, CPU 会**暂停当前任务**, 处理键盘输入, 然后继续执行原任务.

    - 3 上下文切换的过程
        假设当前运行的是进程 A, 操作系统需要切换到进程 B, 流程如下: 
        (1) 保存 A 的状态
        - 记录 A 的 **寄存器信息, 程序计数器, 堆栈指针** 等. 
        - 保存到 **进程控制块(PCB, Process Control Block)**. 
        
        (2) 加载 B 的状态
        - 从 PCB 中读取 B 的寄存器, 程序计数器, 堆栈指针等. 
        - 切换 **页表**, 以加载 B 的虚拟内存地址空间. 

        (3) 恢复执行 B
        - CPU 执行 B 的指令, 从上次停下的地方继续运行. 

    - 4 上下文切换的开销
        - 上下文切换需要**保存和恢复 CPU 状态**, 涉及**寄存器切换, 页表切换, 调度算法决策**, 因此它是**有开销的**.   
        - 影响开销的主要因素: 
            - **进程 vs. 线程**: 进程切换涉及页表切换, **比线程切换更慢**. 
            - **CPU 架构**: 不同 CPU 结构对切换效率有不同优化(如 ARM 处理器支持更快的寄存器切换). 
            - **操作系统**: Linux 和 Windows 在进程调度上的实现有所不同, 影响切换效率. 
        - 通常, 过多的上下文切换会导致 **CPU 开销增加**, 降低系统性能. 优化方法: 
            - **减少不必要的任务切换**(减少线程/进程数量). 
            - **提高任务的 CPU 时间片**(降低切换频率). 
            - **使用线程池** 代替频繁创建和销毁线程.

    - 5 现实中的例子
        - 多任务系统
            - 当你在电脑上**同时运行多个程序**(如浏览器, 音乐播放器), CPU 通过**快速上下文切换**实现“并行”效果. 
        - Web 服务器
            - Web 服务器(如 Nginx)处理大量请求时, **使用多线程或异步 I/O** 以减少上下文切换, 提高吞吐量. 

        - Linux 进程调度**
            - 在 Linux 下, 可以使用 `top` 或 `htop` 命令查看 CPU 上的**进程切换**情况: 
                ```bash
                top
                ```
            - 或使用 `vmstat` 观察 `cs`(context switch)次数: 
                ```bash
                vmstat 1
                ```

    - Summary
        | **概念** | **作用** | **开销** |
        |----------|----------|----------|
        | **进程上下文切换** | 进程 A → 进程 B, 需要切换内存页表 | **高**(涉及页表切换) |
        | **线程上下文切换** | 线程 A → 线程 B, 属于同一进程 | **较低**(不涉及页表切换) |
        | **中断上下文切换** | 处理外部设备中断 | **视情况而定** |
        - 上下文切换是**多任务操作系统的核心**, 但过多的切换会导致**性能下降**, 应通过合理的**任务调度, 线程池管理, 减少 I/O 阻塞**等方式优化.








- 57 **ROM(只读存储器) vs RAM(随机存取存储器)的区别**
    - ROM(Read-Only Memory, **只读存储器**)和 RAM(Random-Access Memory, **随机存取存储器**)是计算机存储系统的两种主要类型, 它们的作用和特性各不相同.

    - 1 基本区别
        | **对比项** | **ROM(只读存储器)** | **RAM(随机存取存储器)** |
        |-----------|-----------------|-----------------|
        | **定义** | 只能读取数据, 通常用于存储固件 | 可以读写数据, 作为程序运行的临时存储 |
        | **数据存储** | 断电后仍能保留数据(**非易失性**) | 断电后数据丢失(**易失性**) |
        | **读写方式** | 主要用于读取, 通常不能修改或修改受限 | 可以快速读写, 支持频繁修改 |
        | **用途** | 存储 BIOS、固件、系统引导信息 | 存储运行中的程序、缓存数据 |
        | **速度** | 速度较慢, 一般用于存储长期不变的数据 | 速度快, CPU 直接访问 |
        | **价格** | 相对便宜 | 相对较贵 |

    - 2 详细对比
        - (1) ROM(只读存储器)
            - 非易失性存储: 断电后数据仍然存在.
            - 存储固件(Firmware), 如BIOS 嵌入式设备固件 微控制器程序等.
            - 读取速度较快, 但通常不能修改(或修改方式受限).

            | 类型 | 特点 | 能否修改 |
            |-----|-----|-----|
            | MROM(掩模 ROM) | 由厂家制造时写入, 不能更改 | 不可修改 |
            | PROM(可编程 ROM) | 用户可写入一次, 但不能修改 | 一次性写入 |
            | EPROM(可擦除 ROM) | 需要紫外线擦除数据后才能重新写入 | 可多次擦除/写入 |
            | EEPROM(电可擦除 ROM) | 可用电信号擦除并重新写入, 速度比RAMß慢 | 可多次擦除/写入 |
            | Flash ROM(闪存) | 现代计算机最常用的 ROM, 可快速读写, 如 SSD U 盘 | 可多次擦除/写入 |

            应用场景: BIOS(基本输入输出系统), 嵌入式设备固件, 游戏机存储芯片, SSD U盘

        - (2) RAM(随机存取存储器)
            - 易失性存储: 断电后数据会丢失.
            - CPU 直接访问, 速度快, 影响计算机运行效率.
            - 主要用于存储操作系统 应用程序和数据缓存.

        | 类型 | 特点 | 用途 |
        |---------|---------|---------|
        | SRAM(静态 RAM) | 速度快, 功耗低, 价格高 | CPU 缓存(L1/L2/L3 Cache) |
        | DRAM(动态 RAM) | 需要不断刷新数据, 速度较慢但便宜 | 计算机主内存(DDR4/DDR5) |

        应用场景: 计算机主内存(DDR4 DDR5), 显存(VRAM GDDR6), 高速缓存(CPU L1/L2/L3 Cache)

