# PCA

- Q1 **什么是PCA?**
    - PCA(Principal Component Analysis, 主成分分析)是一种常用的数据降维技术, 它通过线性变换将原始数据投影到一个新的坐标系中, 使得新的坐标系的基(即主成分)是方差最大的方向. PCA 的目标是减少数据的维度, 同时保留数据中尽可能多的变异性(信息). 它对高维度数据集合寻找尽可能少的正交矢量(主成分)表征数据信息特征. 主成分是一系列单位向量, 主成分的方向是使投影数据的方差最大化(代表最大信息量)的方向.

    - 步骤:
        - 1 标准化数据
            - 如果数据的各个特征的量纲不一致(例如, 一个特征的单位是米, 另一个是秒), 首先需要对数据进行标准化处理, 使得每个特征的均值为 0, 方差为 1. 标准化的公式为: 
                \[ z = \frac{x - \mu}{\sigma} \]
            - 其中
                - \(x\) 是原始数据
                - \(\mu\) 是均值
                - \(\sigma\) 是标准差

        - 2 计算协方差矩阵
            - 计算数据的协方差矩阵是为了捕捉各个特征之间的关系(线性关系). 协方差矩阵的每个元素表示两个特征之间的协方差. 对于一个标准化的数据集, 协方差矩阵可以通过以下公式计算: 
                \[ Cov(X) = \frac{1}{n-1} X^T X \]
            - 其中
                - \(X\) 是数据集(每一列是一个特征), 
                - \(n\) 是样本数.

            - 协方差矩阵是对称的, 且其维度等于原始数据特征的数量.

        - 3 计算特征值和特征向量
            - 协方差矩阵的特征值代表了每个主成分的方差大小, 特征向量代表了主成分的方向. 特征值越大, 说明该主成分能保留的数据变异性越多. 
            - 特征值反映了每个主成分的“重要性”, 即它解释了数据中变异性的多少；特征向量则确定了新的坐标轴(主成分的方向). 
            - 协方差矩阵 \(C\) 的特征值 \(\lambda\) 和特征向量 \(v\) 满足以下方程: 
                \[ C v = \lambda v \]
            - 计算得到的特征向量就是新的主成分方向, 而特征值的大小反映了对应主成分的方差大小. 

        - 4 选择主成分
            - 特征值越大的特征向量表示主成分包含的数据变异性越多. 根据特征值的大小排序, 选择前 k 个特征值对应的特征向量. 通常选择最大的前几个特征值, 目的是尽可能多地保留原始数据的方差. 

        - 5 将数据映射到新的特征空间
            最后, 使用选定的特征向量将原始数据投影到新的坐标系中, 得到降维后的数据. 
            通过将原始数据投影到选定的主成分上, 得到降维后的数据. 假设我们选择了 \(k\) 个主成分, 那么新的数据表示为: 
                \[ Y = X V_k \]
            - 其中: 
                - \(Y\) 是降维后的数据, 
                - \(X\) 是标准化后的原始数据, 
                - \(V_k\) 是前 \(k\) 个特征向量(构成的矩阵). 

    - PCA 的主要特点
        - **线性降维**: PCA 只适用于线性降维, 它假设数据在低维空间中的分布是线性的. 
        - **去相关性**: 通过 PCA 转换后的新特征是互相独立的, 不再具有相关性. 
        - **减少维度**: 在保证尽量多信息的前提下, PCA 可以有效减少数据的维度, 去除冗余信息, 提高计算效率. 

    - PCA 的应用: 
        - **数据可视化**: 当数据维度非常高时, PCA 可以帮助将数据投影到二维或三维空间, 以便于可视化. 
        - **噪声去除**: PCA 可以去除一些低方差的特征, 这些特征往往是噪声. 
        - **特征压缩**: 在机器学习中, PCA 可以用来降低特征空间的维度, 减少计算负担. 

- Q2 **可以使用*PCA*进行特征选择吗?**  

- 特征选择是指从完整的特征集中选择特征的子集, 而PCA通过定义主成分, 定义了一组新的特征来反映数据集中的大部分信息, 新特征不等同于原始特征.因此PCA不是一种特征选择方法.

Q3: **PCA中的第一, 第二个主成分轴是如何选择的?**  

- 最简单的方法是找到使方差最大化的投影.第一主成分是**空间中投影具有最大方差的方向**.第二个主成分是在与第一个正交的所有方向中最大化方差的方向.

Q4: **为什么在执行PCA之前标准化数据很重要?**

- 标准化数据使得特征处于同一数量级, 具有不同数量级值的特征会影响 PCA 计算最佳主成分, 因为PCA对变量的方差非常敏感

Q5: **PCA和随机投影(Random Projection)方法有什么区别?**

- 两者都是降维操作, 将数据空间从高维空间到低维空间映射, 即在高维空间中选取一小组基向量, 低维数据由数据在基向量上投影得到.主要区别在于 PCA 会通过寻找原始数据方差最大的方向来选择最佳基向量, 而随机投影就是随机选择方向.
- **Random Projection**方法在高维空间数据中计算效率较高.

- 引用: [https://www.quora.com/Is-there-any-difference-between-PCA-Principal-component-analysis-and-random-projection-when-preprocessing-data](https://www.quora.com/Is-there-any-difference-between-PCA-Principal-component-analysis-and-random-projection-when-preprocessing-data)

Q6:  *****PCA和LDA(Linear Discriminant Analysis)方法有什么区别?*****

- LDA是有监督的降维方法, 而PCA是无监督的降维方法, LDA降维最多降到类别数k-1的维数, 而PCA没有这个限制, LDA选择分类性能最好的投影方向, 而PCA选择样本点投影具有最大方差的方向.

- Q7 **举例说明PCA的输入输出都是怎样的?**  

    - PCA 的输入输出示例
        - 为了清晰地理解 PCA 的输入和输出, 我们可以通过一个简单的例子来说明. 
        - 假设我们有一个二维数据集, 其中每个数据点有两个特征: **身高**(Height)和**体重**(Weight). 我们希望通过 PCA 将这个数据集从二维空间降维到一维空间. 

    - 1 输入数据
        - 输入数据是一个 $n \times m$ 的矩阵, 其中: 
            - \(n\) 是样本数(数据点数), 
            - \(m\) 是特征数(变量的维度). 

        - 假设我们有 5 个数据点, 每个数据点有 2 个特征(身高和体重), 数据集如下: 

        | 身高 (Height) | 体重 (Weight) |
        |---------------|---------------|
        | 170           | 65            |
        | 160           | 60            |
        | 180           | 75            |
        | 155           | 50            |
        | 165           | 55            |

        - 输入数据矩阵 \(X\) 为: 
            \[
            X = \begin{pmatrix}
            170 & 65 \\
            160 & 60 \\
            180 & 75 \\
            155 & 50 \\
            165 & 55
            \end{pmatrix}
            \]
        - 这个矩阵的形状是 \(5 \times 2\)(5 个样本, 2 个特征). 

    - 2 数据标准化
        - 由于 PCA 是基于协方差矩阵的, 如果数据的特征量纲不同(比如身高是以厘米为单位, 体重是以公斤为单位), 首先需要进行标准化处理. 假设我们将每个特征的均值为 0, 标准差为 1. 经过标准化处理后, 我们得到的矩阵是: 
            \[
            X_{std} = \begin{pmatrix}
            0.654 & 0.674 \\
            -0.654 & -0.674 \\
            1.309 & 1.346 \\
            -1.309 & -1.346 \\
            -0.000 & -0.000
            \end{pmatrix}
            \]

    - 3 计算协方差矩阵
        - 协方差矩阵 \(C\) 可以反映各特征之间的关系. 假设通过计算得到协方差矩阵: 

        \[
        C = \begin{pmatrix}
        1 & 0.9 \\
        0.9 & 1
        \end{pmatrix}
        \]

        - 协方差矩阵的每个元素表示两个特征之间的相关性, 这里 1 表示每个特征与自身的协方差, 0.9 表示身高和体重之间的高相关性. 

    - 4 计算特征值和特征向量
        - 通过对协方差矩阵 \(C\) 进行特征值分解, 我们得到两个特征值和特征向量: 
            - 特征值: \(\lambda_1 = 1.8, \lambda_2 = 0.2\)
            - 特征向量: 
                \[
                v_1 = \begin{pmatrix}
                0.707 \\
                0.707
                \end{pmatrix}, 
                v_2 = \begin{pmatrix}
                -0.707 \\
                0.707
                \end{pmatrix}
                \]

            - 其中, 特征值 \(\lambda_1 = 1.8\) 对应的是第一主成分, 特征值 \(\lambda_2 = 0.2\) 对应的是第二主成分. 第一个主成分 \(v_1\) 表示身高和体重变化方向, 第二个主成分 \(v_2\) 代表次要的变化方向. 

    - 5 选择前 k 个主成分
        - 根据特征值的大小, 我们选择前 \(k = 1\) 个主成分, 因为 \(\lambda_1 = 1.8\) 比 \(\lambda_2 = 0.2\) 大得多. 也就是说, 我们将数据从二维降到一维, 选择第一主成分 \(v_1\) 作为新特征. 

    - 6 将数据投影到主成分上
        - 使用选择的特征向量(第一主成分 \(v_1\))来将数据从原始空间映射到新的主成分空间. 这个步骤是通过计算原始数据矩阵与特征向量的内积来实现的: 
            \[ Y = X_{std} v_1 \]

        - 对于每一个数据点, 我们得到投影后的新坐标, 表示数据在第一主成分方向上的值. 例如, 计算第一个数据点的投影: 
            \[ Y_1 = (0.654 \times 0.707) + (0.674 \times 0.707) = 0.929 \]

        - 所有数据点投影后的结果 \(Y\) 如下: 
            \[
            Y = \begin{pmatrix}
            0.929 \\
            -0.929 \\
            1.872 \\
            -1.872 \\
            -0.000
            \end{pmatrix}
            \]

        - 这是降维后的数据, 只有一个维度(即一个主成分). 

    - 7 输出数据

        - 输出数据是经过降维后的数据, 它的维度由原来的二维降到了1D(即降到一个主成分), 可以看做是一个新的样本集, 每个样本只有一个特征: 
            \[
            Y = \begin{pmatrix}
            0.929 \\
            -0.929 \\
            1.872 \\
            -1.872 \\
            -0.000
            \end{pmatrix}
            \]
    这就是降维后的输出, 它包含了所有数据点在第一主成分上的投影值. 

    - Summary
        - **输入**: 一个包含多个样本和特征的矩阵(例如, \(n \times m\)), 每一行是一个样本, 每一列是一个特征. 
        - **输出**: 降维后的数据, 一个较低维度的矩阵(例如, \(n \times k\), 其中 \(k\) 是选择的主成分数量). 这个新矩阵的每一列对应一个主成分的投影. 
        - 在实际应用中, PCA 的输出通常是降维后的数据, 可以用于数据可视化、特征选择、降噪等任务. 

Q8:  **在大型数据集上, 你会选择使用PCA吗?还是有更好的选择?**

- 当观测值和变量的数量非常大时, 经典的 PCA 方法由于需要计算矩阵的奇异值分解, 需要相当大的内存和巨大的计算时间
- 在大型数据集上, 降维的更好选择有random projection, flash PCA等, flash PCA源自2014年论文《Fast Principal Component Analysis of Large-Scale Genome-Wide Data》, 对代表大部分数据方差的较小数据子集执行主成分分析.

- 引用:[https://www.elucidata.io/blog/principal-component-analysis-on-big-data](https://www.elucidata.io/blog/principal-component-analysis-on-big-data)


Q10: ***PCA*和*t-SNE*有什么区别?**  

- PCA是一种线性降维方法, t-SNE(t-distributed Stochastic Neighbor Embedding)则是一种非线性降维方法
- PCA考虑数据中最大方差的方向, 保留数据的全局结构, t-SNE通过考虑点之间的距离保留数据的局部集群结构
- PCA不涉及超参数, t-SNE涉及诸如困惑度, 学习率和步数等超参数
- PCA受异常值影响大, t-SNE受异常值影响小
- 相比PCA, t-SNE的效果更好, 是最好的降维算法之一


引用: [https://www.geeksforgeeks.org/difference-between-pca-vs-t-sne/](https://www.geeksforgeeks.org/difference-between-pca-vs-t-sne/)

Q11 **PCA 是否会检查哪些特征是多余的并丢弃它们?**

- PCA 不会选择某些特征并丢弃其他特征.相反, 它构建了一些新特征(主成分), 这些特征很好地总结了我们的一些数据点列表.当然, 这些新特征是使用旧特征构建的, 

*问题12*:  ****什么是*稀疏 PCA*?**  

- 稀疏PCA是一种专门用于统计分析和多变量数据集分析的技术.它通过向输入变量引入稀疏结构来降低数据的维度.
- 经典PCA和稀疏 PCA之间的区别在于, 经典 PCA通常是所有输入变量的线性组合, 而稀疏 PCA发现仅包含几个变量的线性组合.
- 因为稀疏 PCA 找到仅包含少量输入变量的线性组合, 所以它在一定程度上减少了原始特征空间, 但不像普通 PCA 那样紧凑(换句话说, 它创建了一个稀疏特征向量).



*问题13*:  *****PCA*如何用于*异常检测*?**  

- PCA将原始数据映射到低维空间之后, 也可以根据数据在低维空间里的坐标来重构原始数据, 假设数据样本$x_{i}$映射到$k$维空间的坐标为${y_{1},y_{2},...,y_{k}}$, 基于该$k$维坐标重构的数据为$x_{ik}^{'}=\sum_{i=1}^{k}y_{i}e_{i}$, $e_{i}$为对应方向上的单位向量
- 基于异常检测的PCA依赖于重构误差.直观上理解, PCA提取了数据的主要特征, 如果一个数据样本不容易被重构出来, 表示这个数据样本的特征跟整体数据样本的特征不一致, 那么它显然就是一个异常的样本.



*问题14*:  *****主成分分析*和*独立成分分析*有什么区别?**  

- 主成分分析(PCA)通常用于压缩信息, 即降维.它在数学上等同于对原始变量矩阵的协方差矩阵执行特征向量分解, 选取特征值最大(代表含有信息量最大)的几个维度构成主成分空间, 将原始数据向主成分空间投影*.*
- 独立分量分析 (ICA)旨在通过将输入空间转换为最大独立基来分离信息.为此, ICA假设独立基之间必须是
    - 统计独立的, 独立基之间互相不相关
    - Non-Gaussian, 即独立分量具有非高斯分布

*问题15*:  *****k-Means算法*和*PCA*之间有什么关系?** 

- 直觉是 PCA 试图将所有数据向量表示为少量特征向量的线性组合, 并且这样做是为了最小化均方重建误差.而K-means 试图用少量质心向量的线性组合代表原始数据向量, 其中线性组合权重必须全为零,  这样做也是为了最小化均方重建误差.因此, K-means算法可以看作是一个超稀疏的PCA.

引用: [https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca](https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca)
