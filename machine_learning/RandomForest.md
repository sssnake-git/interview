# Random Forest

Q1 **怎样定义一个随机森林?**

- 随机森林是一种用于分类, 回归或者其他任务的集成学习方法.
- 随机森林通过在训练时构建许多决策树来工作.它的工作方式是对同一训练集不同部分的多个决策树进行平均.

Q2 **解释随机森林如何为分类和回归问题提供输出?**

- 分类: 随机森林的输出是被最多树选择的那个.
- 回归: 随机森林的输出是单个树的平均或平均预测.

Q3 **有哪些集成学习的方法?**

- 集成学习方法结合了多个机器学习子模型作为一种技术来减少偏差或方差并提高一般模型性能.*集成学习*的方法包括: 
- **Bagging**(*bootstrap aggregating*的缩写): 如果有K个子模型, 用K个单独的数据集, 训练集成的每个子模型.每个单独的数据集, 都是从原始训练数据集中随机抽样(有放回抽样)构建的.数据集之间可能存在重复.
    
    ![https://miro.medium.com/max/1400/1*-PXzSlXtFEGTxgcmCyMkjQ.png](https://miro.medium.com/max/1400/1*-PXzSlXtFEGTxgcmCyMkjQ.png)
    
- boosting: 迭代地构建模型集合, 每个后续模型都专注于学习前一个模型出错的示例.简而言之, boosting 迭代地改进一系列弱学习器, 采用加权平均产生最终的强学习器.
    
    ![https://miro.medium.com/max/1400/1*trWLNnfyqdRklFmLGcF_Zw.png](https://miro.medium.com/max/1400/1*trWLNnfyqdRklFmLGcF_Zw.png)
    
- stacking: 是分层的结构, 第一层, 组合一组模型的输出以进行预测.不同的模型结构在完整的训练数据集上训练完成.然后, 使用第一层的**模型输出作为特征来训练二级模型**.二级模型的学习目标是如何最好地结合初始模型的结果以减少训练错误.
    
    ![https://miro.medium.com/max/1400/1*IaqmYUSmQXE8AJw7moqwkA.png](https://miro.medium.com/max/1400/1*IaqmYUSmQXE8AJw7moqwkA.png)
    

Q4 **随机森林算法需要剪枝吗?**

- 剪枝是机器学习和搜索算法中的一种模型压缩技术, 它通过删除树中非关键和冗余的部分来减少决策树的大小以对实例进行分类.
- 随机森林通常不需要剪枝, 因为它不会像单个决策树那样过拟合, 因为树是自举的, 多个随机树使用随机特征, 各个树都很强大而彼此不相关.

Q5 **随机森林和决策树的关系?**

- 随机森林是一种通过构建大量决策树来工作的集成学习方法.可以为分类和回归任务构建随机森林.
- 随机森林优于决策树, 而且它也不像决策树那样有过度拟合数据的习惯.
- 在特定数据集上训练的决策树将变得非常深并导致过度拟合.要创建随机森林, 可以在训练数据集的不同子集上训练决策树, 然后可以对不同的决策树进行平均, 以降低方差.

Q6 **随机森林有哪些超参数?**

- 森林中决策树的数量.
- 分裂节点时每棵树考虑的特征数.
- 单个树的最大深度.
- 在内部节点拆分的最小样本.
- 叶节点的最大数量.
- 随机特征的数量.
- 自举数据集的大小.

Q7 **是否有必要在随机森林中进行*交叉验证*?**

- 随机森林属于bagging集成算法, 采用Bootstrap, 理论和实践发现Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中. 故没有参加决策树的建立, 这些数据称为袋外数据oob
- 在训练随机森林时, 由于森林的构建使用了Bootstrap, 因此评估准确度的一种方法是使用袋外误差估计(OOB), 而不是交叉验证.

Q8 **随机森林如何处理缺失值?**

- 对于分类问题, 随机森林使用每个特征的平均值或中位数来填充缺失值.
- 对于回归问题, 随机森林使用每个特征的平均值或中位数来填充缺失值, 或者使用类似于CART回归树的方法来进行填充.

Q9 **如何提高随机森林的性能?**

- 增加决策树的数量, 但是如果树的数量过多, 可能会导致过拟合.
- 选择更好的特征子集.可以使用特征选择算法来确定哪些特征最重要, 然后只选择最重要的特征.
- 调整超参数.例如, 可以通过增加每棵树的最大深度或减小每棵树的最小叶节点数量来调整随机森林.

Q10 **随机森林生成步骤介绍**

- 1 自助抽样生成样本集 
  - 从原始训练数据集中, 应用`Bootstrap`(有放回地随机抽取样本), 生成`k`个新的**自助样本集**(`Bootstrap Samples`).  
  - 每个样本集中包含与原始数据集相同数量的样本, 但由于是有放回地抽样, 部分样本会重复出现, 而另一些样本可能完全未被抽到.  
  - **未被抽到的样本**构成了对应的袋外数据集(`Out-of-Bag`, `OOB`), 用于评估模型性能.  
  - 以这些自助样本集为基础, 分别构建`k`棵分类回归树(`CART`, `Classification and Regression Tree`).

- 2 节点分裂中的随机特征选择
  - 在每棵树的每个节点处, 从总特征数`n`中随机抽取$m_{try}$个特征.
  - **随机特征选择的意义**: 引入随机性, 减少树之间的相关性, 从而提高整体模型的泛化能力.
  - 对这$m_{try}$个特征, 根据某种指标(如信息增益, 基尼指数, 或均方误差), 选取最优特征用于节点分裂.

- 3 树的生长与不剪枝策略
  - 每棵决策树在训练时都会**最大限度地生长**, 直到所有叶节点都纯净或达到最小样本分裂数量为止.  
  - **不剪枝的好处**: 减少人为干预, 使每棵树尽可能地适应样本数据, 提高多样性.  
  - 单棵树可能会过拟合训练数据, 但在随机森林中, 通过多个树的集成能显著减少过拟合.

- 4 多棵树组成随机森林  
  - 随机森林将所有生成的`k`棵树组合在一起.  
  - **分类任务**:   
    - 输入新的数据样本时, 每棵树独立对其进行分类.  
    - 最终的分类结果由所有树的**投票结果**决定, 类别得票最多者即为预测输出(多数投票原则).  

  - **回归任务**:   
    - 每棵树输出一个预测值.  
    - 最终预测值为所有树的预测结果的平均值.

- 5 总结
  - 随机森林通过两种随机性(样本和特征的随机化)实现了强大的泛化能力:   
    - 样本随机化通过`Bootstrap`方法生成多样化的训练集,   
    - 特征随机化通过限制每次分裂时可用的特征数量, 降低树间相关性.  
    - 这使得随机森林在处理高维数据, 减少过拟合以及提高分类和回归精度方面表现优异.

Q11 **随机森林不会发生过拟合的原因是什么**

- 在建立每一棵决策树的过程中, 有两点需要注意-采样与完全分裂.首先是两个随机采样的过程, `random forest`对输入的数据要进行行, 列的采样.对于行采样, 采用有放回的方式, 也就是在采样得到的样本集合中, 可能有重复的样本.
- 对于行采样, 采用有放回的方式, 也就是在采样得到的样本集合中, 可能有重复的样本.假设输入样本为`N`个, 那么采样的样本也为`N`个.这样使得在训练的时候, 每一棵树的输入样本都不是全部的样本, 使得相对不容易出现`over-fitting`.然后进行列采样, 从`M`个`feature`中, 选择`m`个$(m << M)$.之后就是对采样之后的数据使用完全分裂的方式建立出决策树, 这样决策树的某一个叶子节点要么是无法继续分裂的, 要么里面的所有样本的都是指向的同一个分类.一般很多的决策树算法都一个重要的步骤*剪枝*, 但是这里不这么做, 由于之前的两个随机采样的过程保证了随机性, 所以就算不剪枝, 也不会出现`over-fitting`.

Q12 **随机森林与SVM的比较**

- 1 不需要调节过多的参数, 因为随机森林只需要调节树的数量, 而且树的数量一般是越多越好, 而其他机器学习算法, 比如`SVM`, 有非常多超参数需要调整, 如选择最合适的核函数, 正则惩罚等.
- 2 分类较为简单, 直接.随机森林和支持向量机都是非参数模型(复杂度随着训练模型样本的增加而增大).相较于一般线性模型, 就计算消耗来看, 训练非参数模型因此更为耗时耗力.分类树越多, 需要更耗时来构建随机森林模型.同样, 我们训练出来的支持向量机有很多支持向量, 最坏情况为, 我们训练集有多少实例, 就有多少支持向量.虽然, 我们可以使用多类支持向量机, 但传统多类分类问题的执行一般是`one-vs-all`(所谓`one-vs-all`就是将`binary`分类的方法应用到多类分类中.比如我想分成K类, 那么就将其中一类作为`positive`), 因此我们还是需要为每个类训练一个支持向量机.相反, 决策树与随机深林则可以毫无压力解决多类问题.
- 3 比较容易入手实践.随机森林在训练模型上要更为简单.你很容易可以得到一个又好且具鲁棒性的模型.随机森林模型的复杂度与训练样本和树成正比.支持向量机则需要我们在调参方面做些工作, 除此之外, 计算成本会随着类增加呈线性增长.
- 4 小数据上, `SVM`优异, 而随机森林对数据需求较大.就经验来说, 我更愿意认为支持向量机在存在较少极值的小数据集上具有优势.随机森林则需要更多数据但一般可以得到非常好的且具有鲁棒性的模型.

- 引用: [https://zhuanlan.zhihu.com/p/45040640](https://zhuanlan.zhihu.com/p/45040640)

Q13 **随机森林有什么优缺点**

- 优点: 
  - 在当前的很多数据集上, 相对其他算法有着很大的优势, 表现良好.
  - 它能够处理很高维度(feature很多)的数据, 并且不用做特征选择(因为特征子集是随机选择的).
  - 在训练完后, 它能够给出哪些feature比较重要.
  - 训练速度快, 容易做成并行化方法(训练时树与树之间是相互独立的).
  - 在训练过程中, 能够检测到feature间的互相影响.
  - 对于不平衡的数据集来说, 它可以平衡误差.
  - 如果有很大一部分的特征遗失, 仍可以维持准确度.

- 缺点: 
  - 随机森林在某些噪音较大的分类或回归问题上会过拟合.
  - 对于有不同取值的属性的数据, 取值划分较多的属性会对随机森林产生更大的影响, 所以随机森林在这种数据上产出的属性权值是不可信的.
