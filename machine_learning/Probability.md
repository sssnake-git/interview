# Probability

Q1 **描述什么是概率分布**

- 概率分布是一种统计函数, 它描述了随机变量在给定范围内可能取的所有可能值和可能性.
- 概率分布有两种主要类型: 
   - 离散概率分布: 用于描述结果离散的随机变量, 例如连续五次抛硬币正面朝上的次数.
   - 连续概率分布: 用于描述具有连续结果的随机变量, 例如房价.

Q2 **伯努利分布和二项分布有什么区别?**

- 伯努利分布是随机变量的离散概率分布, 它采用二进制输出: 1的概率为p, 0的概率为(1-p).
- 二项分布是n个独立的成功/失败试验中成功的次数的离散概率分布, 其中每次试验的成功概率为p, n=1时, 二项分布就是伯努利分布.

Q3 **贝叶斯理论中的似然和后验的区别?**

- 通俗来讲, 后验概率是当观察到了“果”然后推算“因”的条件概率, 似然概率是由“因”而导致“果”的可能性.

Q4 **什么是泊松过程?**

- 泊松过程是一系列离散事件的模型, 事件之间的平均时间是已知的(确定的), 但事件发生的确切时间是随机的.一个事件的到来与之前的事件无关(事件之间的等待时间是无记忆的).
- 泊松过程符合以下标准: 
1. 事件是相互独立的.一个事件的发生并不影响另一个事件发生的概率.
2. 平均速率(每个时间段的事件)是恒定的.
3. 两个事件不可能在同一时间发生.

Q5 **disjoint(互斥)事件和互相独立事件有什么区别?**

- 互斥事件是指两者不可能同时发生, 两个相互独立事件是指一个事件发生对 另一个事件发生的概率没有影响.
- 试验的次数不同.互斥事件是一次试验下出现的不同事件 , 互相独立事件是两次或多次不同试验下出现的不同事件.
- 若A与B为互斥事件, 则有概率加法公式$P(A+B)=P(A)+P(B)$, 若A与B为相互独立事件, 则有概率乘法公式$P(AB)=p(A)P(B)$.

Q6 **什么时候, 一个事件独立于自身?**

- 唯一独立于自身的事件是那些以概率$0$或概率$1$发生的事件.独立于自身意味着$P(A)=(P(A))^2$, 这种情况下,  $P(A)$取$0$或$1$.

Q7 **二项分布和几何分布有什么区别?**

- 在二项分布中, 试验次数是固定的, 统计“成功”的次数, 而在几何分布和负二项分布中, “成功”的数量是固定的, 计算获得所需“成功”数量所需的试验次数

Q8 **随机变量的常用特征**

- 期望: 表示随机变量$X$的平均水平.
- 方差/标准差: 用来刻画随机变量$X$的波动大小, 方差越大, 结果的未知性越大.
- 分位数, 对于随机变量$X$ , 若$P(X \in t) = \alpha$ , 则称$t$为$X$的$ \alpha $分位数.分位数通常用来监控异常数据.
- 协方差/相关系数: 衡量两个变量之间的关系.

Q9 **如何给没有学过统计学的人解释正态分布**

- 正态分布有点像$2/8$原则.大多数人在中间, 越中间人越多, 小部分人在一头一尾且数量相当.从大多数人到少数人的过度是平滑的, 非跳崖式突变.

- 引用: [https://blog.csdn.net/](https://blog.csdn.net/qq_34069667/article/details/107941951)

Q10 **什么是参数估计?**

- 参数估计是根据从总体中抽取的样本估计总体分布中包含的未知参数的方法.它是统计推断的一种基本形式, 是数理统计学的一个重要分支, 分为点估计和区间估计两部分.

Q11 **参数估计的常用方法**

- 常用点估计的方法为矩估计和最大似然估计, 还有最小二乘估计.
  - 矩估计基于大数定理, 用样本矩估计总体矩.
  - 最大似然估计是利用已知的样本结果信息, 反推最具有可能(最大概率)导致这些样本结果出现的模型参数值.
- 区间估计是指在点估计的基础上, 给出总体参数估计的一个区间范围, 该区间通常由样本统计量加减估计误差得到.

Q12 **最大似然估计的优缺点**

- 优点: 
    - 可解释性强, 有理论基础, 
    - 对于参数和模型的标准化设定不敏感, 
    - 易于实现
    - 方差低于其他方法(即受抽样误差影响最小的估计方法)并且随着样本量的增加而无偏
- 缺点: 
    - 需要对数据结构做强假设(比如假设服从高斯分布)
    - 估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布

- 引用: [https://www.exploredatabase.com/2020/05/advantages-and-disadvantages-of-maximum-likelihood-methods.html](https://www.exploredatabase.com/2020/05/advantages-and-disadvantages-of-maximum-likelihood-methods.html)

Q13 **最大似然估计的一般步骤**

- 1 根据对数据假设的分布, 写出其似然函数.
- 2 对似然函数取对数.
- 3 求似然函数的导数.
- 4 解似然方程.

Q14 **如何将偏态分布转换为近似的正态分布?**

- 许多模型对数据做了大量的假设, 正态分布是常见的一种, 如果预测变量和目标变量呈正态分布, 则模型能进行更可靠的预测, 当原始数据不满足正态分布时, 需要对它做一定变换, 使得它趋于正态分布.
- 常见的数据转换方式包括: 
  - 对数变换.
  - 平方根变换.
  - $Box-Cox$方法, 执行一系列幂变换, 包括对数和平方根, 消除数据中的偏度.

- 引用: [https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45)

Q15 **累积分布函数(CDF, Cumulative Distribution Function)和概率密度函数(PDF, probability density function)有什么区别?**

- 概率密度函数描述了连续随机变量的概率分布.累积分布函数是一种处理连续和离散随机变量的概率分布.

|  | PDF | CDF |
| --- | --- | --- |
| 定义 | PDF是随机变量(X)取值与随机变量(Y)完全相等的概率. | CDF 是随机变量 (X) 取值小于或等于随机变量 (Y) 的概率. |
| 变量 | 仅适用于连续随机变量. | 适用于连续和离散随机变量. |
| 取值 | 该值介于 0 和 1 之间. | 该值介于 0 和 1 之间. |

Q16 **贝叶斯估计和最大似然估计有什么区别?**

- **最大似然是对点估计, 贝叶斯推断是对分布估计**: 假设需要求解参数$ \theta $, 最大似然是求出最有可能的$ \theta $值, 而贝叶斯推断则是求解$ \theta $的分布.在公式上, 贝叶斯推断还引入了先验, 通过先验和似然来求解后验分布, 而最大似然直接使用似然函数, 通过最大化其来求解.

Q17 **如何检查两个事件是否独立?**

- 从定义上看, 若事件$A$的发生对事件$B$的发生概率没有影响, 反之亦然, 则这两个事件是相互独立的.
- 从数学上来看, 若对两个事件$A$, $B$, 有$P(AB)=P(A) \dot{} P(B)$, 则事件$A$, $B$相互独立.

Q18 **说出一些用于估计概率分布参数的方法**

- 最大似然ML
- 最小二乘法OLS
- 矩估计法

- 引用: [https://www.itl.nist.gov/](https://www.itl.nist.gov/)

Q19 **基于频率的推理和贝叶斯推理有什么异同?**

- 统计推断有两种主要方法, 频率和贝叶斯, 它们对不确定性的解释不同, 频率主义方法处理长期概率(即, 给定原假设的数据集的概率有多大), 而贝叶斯方法处理给定特定数据集的假设概率.
- 贝叶斯分析将先验信息纳入分析, 而频率分析纯粹由数据驱动
- 贝叶斯方法可以计算特定假设为真的概率, 而频率主义方法计算获得另一个数据集的概率, 其极端程度至少与收集的数据集一样极端(给出 *P* 值).
- 与频率主义方法相比, 贝叶斯方法对结果的解释更直观, 频率主义方法经常被误解.
- 频率主义方法将概率分配给数据, 而不是假设, 而贝叶斯方法将概率分配给假设.此外, 贝叶斯模型将先验知识纳入分析中, 随着更多数据的出现, 会更新假设概率.

- 引用: [Understanding the Differences Between Bayesian and Frequentist Statistics - International Journal of Radiation Oncology, Biology, Physics (redjournal.org)](https://www.redjournal.org/article/S0360-3016(21)03256-9/fulltext)
