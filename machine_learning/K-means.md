# K-means

Q1: **K-means聚类有哪些应用?**

- 文档分类:  k-means 可以根据标签, 主题和文档内容将文档聚类为多个类别.
- 保险欺诈检测: 利用过去关于欺诈索赔的历史数据, 可以根据新索赔与表明欺诈模式的集群的接近程度来隔离新索赔.
- 网络分析罪犯: 这是从个人和团体收集数据以识别重要相关性的过程.网络画像的概念源自犯罪画像, 它提供有关调查部门的信息, 以对在犯罪现场的罪犯类型进行分类

Q2: **K-means和KNN算法有什么区别?**

- KNN是一种**监督**分类算法.这意味着我们需要事先标记好的数据来对未标记数据点进行分类.KNN根据数据点与特征空间中其他数据点的接近程度对其进行分类.
- K-means Clustering是一种**无监督**分类算法.它只需要一组未标记的点和一个阈值`K`, 它将数据聚类为K个集群.

Q3: **解释什么是K-means聚类?**

- K-means聚类是一种**聚类**分析方法, 把数据空间中的n个点划分到k个簇中, 聚类目标是, 让簇内的点尽量紧密的连在一起, 而让簇间的距离尽量的大(**类内方差最小, 类间方差最大**), 

![Untitled](K-means/Untitled.png)

Q4: **K-means聚类的停止标准是什么?**

- 常见的停止条件有: 
    - **收敛**, 聚类没有进一步的变化, 点留在同一个集群中.
    - **最大迭代次数**.当达到最大迭代次数时, 算法将停止.
    - 方差的改善<x(x为预设参数)

Q5: **描述K-means算法的步骤**

1. 从数据点中选择随机的 k 个样本作为初始聚类中心 $a=a_{1},a_{2},...a_{k}$
2. 对数据集中每个样本, 计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中, 
3. 对每个类别, 重新计算它的聚类中心$a_{j}=\frac{1}{c_{i}}\sum_{x\epsilon c_{i}}^{}x$
4. 重复上面 2 3 两步操作, 直到达到某个中止条件(迭代次数, 最小误差变化等).

Q6: **为什么K-means算法使用欧式距离度量**

- K-means 聚类目标是最小化**簇内方差**.簇内方差与数据点到聚类中心的欧几里德距离平方和相同.

Q7: ***k-Means*和*k-Nearest Neighbors*之间的主要区别是什么?**

- **k-Means**是一种聚类算法, 它试图将一组点划分为k个聚类集合, 每个聚类中的点往往彼此靠近.它是无监督的, 这些点没有事先被标注.
- k-Nearest Neighbors是一种分类(或回归)算法, 根据k个最近点的分类, 确定一个点的分类.它属于监督算法, 因为它试图根据其他点的已知分类对一个点进行分类.

Q8: **在使用K-means聚类算法时, 如何选择K的值?**

- 经验性选择: 将样本量除以2再开方出来的值作为K值: $K=\sqrt{n/2}$
- elbow methods: 基于K-means聚类框架下的聚类算法在最小化目标函数时本质上就是最小化整个簇内距离.根据簇内距离的定义可知, 当K值越小时, 那么簇内距离便会越大, 而当K值越大时, 那么簇内距离便会越小.在K值由小变大的过程中, 随着K值的增大簇内距离便会逐步减小, 但是当K值取得最优解后簇内距离便不会出现明显的降幅.如下图, 经验性地选取K=3为数据集的最优解.

![Untitled](K-means/Untitled1.png)

Q9: ***比较分层聚类(Hierarchical Clustering)和K-means聚类算法***

- 层次聚类法先将每个样本分别作为一个独立的类, 然后通过距离计算, 将距离相近的两个样本合并为一类, 其他样本仍然各自为一类.不断重复这个过程, 直到达到聚类数或者设定的目标.
- 和层次聚类相比, K-Means计算量较小.即使样本量大或变量多的情况下, 仍然可以快速给出聚类结果.但是这个方法应用范围比较有限.对初始点位置很敏感, 容易导致聚类结果与数据真实分类出现差异, 对异常值也很敏感.同时K-Means聚类的变量也必须是连续型变量, 对变量的标准度要求较高, 否则可能产生无意义的结果.

Q10: ***在使用K-means算法之前, 你会怎样预处理数据?***

- K-means 的本质是基于欧式距离的数据划分算法, 均值和方差大的维度将对数据的聚类产生决定性影响.所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的.因此在K-means算法处理前需要做数据归一化, 数据标准化
- 此外, 离群点或者噪声数据会对均值产生较大的影响, 导致中心偏移, 因此我们还需要对数据进行异常点检测.

- 引用: [https://blog.csdn.net/qq_45898579/article/details/118379182](https://blog.csdn.net/qq_45898579/article/details/118379182)

Q11: **在聚类算法中, 使用曼哈顿距离和使用欧式距离有什么区别?**

- 欧式距离指两个点之间的真实距离, 它将样本的不同维度之间的差别等同看待, 这一点有时不能满足实际要求, 
- 曼哈顿距离指在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和, 计算中, 曼哈顿距离的计算速度更快.
- 欧氏距离和曼哈顿距离的区别在于: 它们对向量之间差异的计算过程中, 各个维度差异的权值不同

Q12: **列举几种情况, 在这些情况下K-means算法难以取得较好效果**

- 若样本数据是各向异性的, 那么k-means算法的效果较差. 通俗讲就是样本数据落在各个方向的概率是相等的K-mean算法才适用, 不相等的话不适用.

![Untitled](K-means/Untitled2.png)

- 若样本数据是非凸数据集的时候, k-means算法的效果较差. 非凸数据可以理解为簇类中心是一条线.

![Untitled](K-means/Untitled3.png)

- 若各簇类的样本数相差比较大, 聚类性能较差.

![Untitled](K-means/Untitled4.png)

Q13: **怎样在非常大的数据集上执行K-means算法?**

- 使用mini-batch K-means, 采用小批量的数据子集减小计算时间, 优化目标函数, 小批量是指每次训练算法时所随机抽取的数据子集, 采用这些随机产生的子集进行训练算法, 大大减小了计算时间.

Q14: **K-means算法的优化目标函数是?**

- 目标函数是最小化每个点到其聚类中心的距离: $minimize(D_{1}+D_{2}+...+D_{n}$)

Q15: **K-means和K-medians算法的区别是什么?什么情况下你会选择K-means算法(或选择K-medians算法)?**

- k-medians算法使用曼哈顿距离替换欧式距离, 使用中位数替换均值, 
- K-medians算法使用中位数作为质心, 对异常点的鲁棒性更好, 因此在数据中存在较多异常点时选用K-median算法, **在较大的数据集时进行聚类时, K-medians算法由于每次迭代都需要排序, 计算速度较慢, 因此选用K-means**

Q16: **可以利用K-means算法找到数据中的离群值吗?**

- 可以, 先使用K-means算法对数据进行聚类, 把数据聚集为几个簇, 然后再计算每个元素至簇中心的距离, 最后选择最远距离的几个点, 视为异常值

Q17: **聚类算法中, 如何判断数据是否被“充分”地聚类, 以便算法产生有意义的结果?**

- 对于k-means聚类算法, 可以使用Gap statistics.基本上, 这个方法是根据与越来越多的集群的参考分布相比的平均分散度来计算聚类度量的优度.
- 或者, 使用重采样研究集群稳定性.重新采样数据(通过随机抽样或通过向其添加小噪声)并计算结果聚类分区的“接近度”, 接近度可以用[Jaccard](http://en.wikipedia.org/wiki/Jaccard_index)相似度衡量.

- 引用: [https://stats.stackexchange.com/questions/11691/how-to-tell-if-data-is-clustered-enough-for-clustering-algorithms-to-produce-m](https://stats.stackexchange.com/questions/11691/how-to-tell-if-data-is-clustered-enough-for-clustering-algorithms-to-produce-m)

Q18: **维度灾难问题会如何影响K-means算法?**

- 当数据维度过高时, 几乎所有的高维空间都远离其中心, 任意两点的距离会趋向收敛, 意思是任意两点的最大距离和最小距离会变为相同.因此基于欧式距离的k-means算法, 会无法进行聚类(因为距离会趋于收敛)
- 在数据维度过高时, 可以先采用降维算法(PCA)将数据映射至低维空间后再聚类

Q19: **K-means算法与PCA算法之间有什么联系?**

- PCA 将所有的数据向量表示为少量特征向量的线性组合, 优化的目标是最小化均方重建误差.而K-means 将所有的数据向量表示为少量簇质心向量的线性组合, 优化的目标也是最小化均方重建误差.**所以 K-means 可以看作是一个超稀疏的 PCA.**

- 引用: [https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca](https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca)