# Transformer

## Concepts

- Q **Transformer是什么?**
    - Transformer架构是一种基于自注意力机制(Self-Attention)的神经网络模型, 广泛应用于自然语言处理(NLP)和计算机视觉(CV)等领域. 其核心思想是利用注意力机制来建模输入序列中的全局依赖关系, 而不依赖于传统的循环神经网络(RNN)或卷积神经网络(CNN).

    - 1 Transformer的整体架构
        - Transformer由编码器(Encoder)和解码器(Decoder)两部分组成, 适用于序列到序列(seq2seq)任务, 如机器翻译.   
        其中:
            - **编码器(Encoder)** 负责提取输入序列的语义信息, 输出高维表示.
            - **解码器(Decoder)** 负责根据编码器的输出生成目标序列.

        在"Attention Is All You Need"中:
            - 编码器和解码器均由多个相同的层堆叠而成, 每层包括多头自注意力(Multi-Head Self-Attention)和前馈神经网络(Feed Forward Network, FFN).
            - 解码器比编码器多一个掩码(Masked Multi-Head Attention)来防止看到未来的词.

    - 2 关键组成部分
        - 1 输入表示
            - Transformer的输入首先经过**词嵌入(Embedding)**, 然后加入**位置编码(Positional Encoding)**, 因为Transformer不具备RNN的序列信息, 因此需要显式加入位置信息. 

        - 2 自注意力机制(Self-Attention)
            - 自注意力的核心思想是计算输入序列中每个单词(Token)对其他单词的重要性.   
            - 在**自注意力计算**中, 每个输入token都会计算三个向量: 
                - **查询向量(Query, Q)**
                - **键向量(Key, K)**
                - **值向量(Value, V)**
            - 公式如下: 
            \[ \text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V \]
            - 含义:
                - \( QK^T \) 计算的是相似度(注意力分数). 
                - \( \frac{1}{\sqrt{d_k}} \) 用于防止梯度消失或爆炸. 
                - Softmax 归一化后得到注意力权重. 
                - 乘以 \( V \) 得到加权的输出. 

        - 3 多头注意力(Multi-Head Attention)
            - 在单头注意力中, 每个词只学习一种注意力模式, 而**多头注意力**(MHA)允许模型关注多个不同的语义关系. 其实现方式: 
                - 先对 \( Q, K, V \) 进行多个线性变换, 分成多个注意力头. 
                - 每个头独立执行自注意力计算. 
                - 最后将所有头的结果拼接, 并再经过一个线性变换得到最终结果. 

            - 公式如下: 
                \[ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O \]

        - 4 前馈神经网络(Feed Forward Network, FFN)
            - 每个编码器和解码器层中都包含一个**前馈神经网络**, 其作用是对每个词的向量表示进行非线性变换: 
                \[ \text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2 \]
            - 这里的 \( \max(0, \cdot) \) 是ReLU激活函数. 

        - 5 层归一化(Layer Normalization)
            - 为了稳定训练, Transformer在**多头注意力和FFN之后**都会进行**层归一化(LayerNorm)**, 提高训练稳定性. 

        - 6 残差连接(Residual Connection)
            - Transformer在每个子层(自注意力, 多头注意力, FFN)外都加了残差连接: 
                \[ \text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x)) \]
            - 残差连接有助于信息流动和梯度传播. 

        - 7 掩码(Masking)
            - Padding Mask: 防止模型关注填充(Padding)的位置. 
            - Look-Ahead Mask(未来信息屏蔽): 用于解码器, 避免预测时看到未来信息. 

    - 3 Transformer的训练
        - Transformer的训练通常使用自回归(Auto-regressive)解码, 即在解码时, 模型只能看到当前和过去的输出.

        - 训练时, 使用交叉熵损失(Cross-Entropy Loss), 并结合Teacher Forcing技巧, 使得训练更稳定.

        - 此外, 常用的优化方法: 
            - Adam 优化器
            - 学习率调度(Learning Rate Warmup + Decay)

    - 4 Transformer的优势
        相比RNN和CNN, Transformer具有以下优势(后面有详细介绍为什么Transformer替代了LSTM)
            - 并行计算: 不像RNN需要序列化处理, Transformer可以利用矩阵计算并行处理所有词. 
            - 长距离依赖: 自注意力可以建模长距离依赖, 而RNN存在梯度消失问题. 
            - 更强的特征表达能力: 多头注意力能学习不同层次的语义关系. 

    - 5 Transformer的变体
        - Transformer架构被广泛应用, 并产生多个变体: 
            - BERT(双向编码): 适用于NLP任务(如问答, 情感分析).
            - GPT(自回归解码): 用于生成文本(如ChatGPT).
            - Vision Transformer (ViT): 用于计算机视觉任务(如图像分类). 
            - T5, BART: 用于文本生成和翻译任务. 

    - Summary
        - Transformer是一种基于*自注意力*的架构, 核心组成部分包括*多头注意力, 前馈神经网络, 残差连接, 层归一化和位置编码*, 具备*并行计算, 高效建模长距离依赖*的能力, 已经成为现代深度学习的主流架构.

- Q **Self attention**

- Q **Transformer的多头注意力机制**
    - Transformer使用多头注意力机制(Multi-Head Attention, MHA)的主要目的是增强模型的表达能力和学习不同语义关系**. 多头注意力是Transformer成功的关键之一, 使其在NLP, CV, 语音处理等多个领域取得了突破性的成果. 相比于单一注意力机制, 多头注意力具有以下几个关键优势:   

    - 1 提供多种表示信息
        - 单个注意力头只能关注输入序列中的一种关系, 比如长距离依赖或局部上下文. **多头注意力允许模型从多个不同的子空间学习不同的表示**, 从而捕捉更丰富的信息. 例如: 一个头可能关注**语法结构**(如主语与动词的关系), 另一个头可能关注**语义信息**(如代词和前文的对应关系),一个头可能关注**长距离依赖**(如翻译时保持主语一致).
        - 假设输入句子是: "The cat sat on the mat." head_1可能关注 "The" 和 "cat" 的关系(主谓), head_2可能关注 "sat" 和 "on the mat" 的关系(动作-地点), head_3可能关注 "cat" 和 "mat" 之间的长距离联系(词义相似性). 如果只有一个注意力头, 则只能捕捉到其中一种关系, 信息会丢失. 

    - 2 改善模型的泛化能力
        不同的注意力头可以关注不同的特征, 从而增强模型的鲁棒性, 提高泛化能力. 
        - 单头注意力可能过度拟合某一类模式, 而多头机制通过不同的注意力分布降低了过拟合风险. 
        - 在**不同任务(如翻译, 问答, 文本分类)中, 多头注意力可以适应不同的数据模式**, 使得模型更加通用.

    - 3 提升学习能力, 避免信息瓶颈**
        如果只使用单头注意力, 则**所有信息都必须通过一个低维投影矩阵传递**, 信息可能丢失. 而**多头注意力可以使用多个投影矩阵, 将信息投影到多个不同的子空间**, 避免单一表示的限制.

        - 数学分析
            假设输入是维度为$d$的向量, 使用单个注意力头: 
                \[ Q, K, V \in \mathbb{R}^{d \times d} \]
            则注意力计算:
                \[ \text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d}} \right) V \]
            这意味着所有注意力计算都是在同一子空间中进行的, 信息可能受限. 

            如果使用**h个头的多头注意力**, 则:
                \[ Q_i, K_i, V_i \in \mathbb{R}^{d/h \times d/h} \quad \text{(每个头计算的投影维度降低)} \]
            然后:
                \[ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O \]
            这样, **多个头可以在不同的子空间计算注意力**, 然后合并信息, 增强整体表达能力. 

    - 4 允许并行计算, 提高计算效率
        与RNN不同, Transformer本身就支持并行计算, 而**多头注意力机制进一步提高了计算效率**: 
        - **每个头的计算可以在不同的计算核(如GPU核)上并行执行**, 不会影响推理速度. 
        - **相比于单个大头, 多头注意力拆分计算后可以降低计算复杂度**, 同时仍然保持足够的表达能力. 

        - 计算复杂度分析
            假设序列长度为 \( n \), 单头注意力的计算复杂度是: \[ O(n^2 d) \]
            如果直接扩大单头注意力的维度, 计算复杂度会更高. 而**多头注意力拆分后, 每个头的计算量减少, 整体计算仍然在 \( O(n^2 d) \) 级别**, 但信息表达能力大幅提升. 

    - 5 增强对长距离依赖的建模能力
        对于长文本或长序列任务, 单头注意力可能会受到信息瓶颈的限制, 导致模型无法有效捕捉远程依赖. 而**多头注意力可以让不同的头专注于不同范围的依赖关系**: 
        - **某些头可以关注短距离关系**(如词法和语法结构). 
        - **某些头可以关注长距离关系**(如代词指代, 句间逻辑等). 
        这使得Transformer比RNN更擅长捕捉全局信息, 适用于长文本任务(如机器翻译, 摘要生成等). 

- Q **为什么基于Transformer的大模型目前处于主导地位?**

    - 2018年GPT(Generative Pre-trained Transformer), BERT(Bi-Directional Encoder Representation of Transformer)在100M数量级上已经表现出了远优于LSTM等传统模型的性能, 导致研究者们大量转移至Transformer.
        - 在Transformer出来之前, 以及刚出来的那段时间, 学术界主要是在尝试LSTM(Long Short Term Memory)进行大规模预训练语言模型, 其中最出名的应该是2018年的ELMo(Embeddings from Language Model).
        - ELMo是标准的双向LSTM, 通过自回归的方式进行模型训练. 当需要应用到下游的具体任务时, 会使用额外的线性层来融合不同的token和embedding, 从而得到最终的token或者整个句子的embeddeding. 这些额外的线性层就是与下游具体任务相关的, 可学习的参数, 所以ELMo就是典型的Pre train + Fine tune模式.
        - ELMo在2018年时在6个NLP任务上最大参数量到了93.6M, 但是很快就被同年的GPT和BERT打败. GPT和BERT也是标准的Pre train + Fine Tune模式, GPT的参数量是117M, BERT-base的参数量是110M, BERT-large的参数量是340M. 所以GPT和BERT-base的参数量与ELMo相当, 但是在性能上比ELMo强不少.
        - 2018年是一个很重要的时间点, 基于Transformer的GPT和BERT因为其强大的表现, 让大量的研究者放弃了RNN系列语言模型. 所以, 在实际效果上的绝对优势是Transformer成为主流模型结构的最重要原因.

    - Transformer优于LSTM的重要原因是, Transformer能够解决长依赖的问题.
        - RNN在理论上来说, 具有捕捉长依赖的能力, 因为信息始终会沿着时间线向后传递. 但实际上, 因为基于梯度的优化方法, 在RNN的特殊结构上容易出现梯度消失, 梯度爆炸, 所以导致非常难以训练, 其表现就是远距离信息直接丢失.
        - LSTM通过引入一个cell state, 希望解决RNN的长依赖问题. 同样在2018年, 一篇名为"Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"的论文分析了LSTM based LLM对于远距离token的依赖性. 这篇论文的结论很简单, 大多数LSTM based LLM只能够有效利用过去的200个token, 并且模型只对最近的50个token的位置信息敏感.
        - 引用: https://arxiv.org/pdf/1805.04623
        - 简单来说, LSTM based LLM解决不了长距离依赖问题, 因为最长也就能利用200个token左右的上下文信息.
        - 另外一种关于LSTM无法解决长距离依赖的原因是, RNN模型仅仅依靠一个低维的hidden state或cell state来存储过去所有的信息显然是不合理的.
        - 所以, transformer使用了self attention, 建立了token与token之间的"超距"关系, 所谓"超距", 指的是在transformer中根本没有距离的概念.
        - 在transformer的encoder结构中, 每个token可以attend到其他任意的token. 在transformer的decoder结构中, 每个token可以attend到这个token之前的任意一个token. 并且, 这种attention的计算过程没有任何关于位置信息的先验知识, 赋予了模型很大的自由度. 既然都没有距离的概念了, 那么"长距离"依赖的问题也就不存在了. 这是transformer优于LSTM的最重要的原因.

    - LSTM难以并行化影响了研究者将其sacle到更大规模模型的动力.
        - LSTM的难以并行化也限制了很多顶级团队的研究动力. 尤其是在transformer在1B规模上表现出了明显优于LSTM的特性之后, 大多数研究人员没有动力去做一个1B或者10B规模的LSTM based LLM.
        - 在transformer的结构中, attention本身写法比较简单, 是一个单纯的矩阵乘法, arithmetic intensity是可调的, 方便适配硬件的拓扑结构, 方便scale up.
        - Transformer的可扩展性非常好, 只要简单的堆模型, 让模型的单层并行变大, 或者把模型层数增加, 效果就会持续变好. 这个特性RNN, LSTM都不具备, 从几层到十几层, 架构都是不一样的.

- Q **Transformer的self attention为什么Q和K使用不同的权重矩阵生成, 为何不能使用同一个值进行自身的点乘?**
    - 在Transformer的*self-attention*中, Q, K和V的选取和计算是基于输入的词嵌入(或先前层的输出), 并且通过不同的权重矩阵进行转换. 它们之间的差异以及为什么不能使用相同的矩阵进行计算, 主要涉及到以下几个方面: 

    - 1 Q和K的选取
        - 在Transformer的*self-attention*机制中, Q和K是通过输入向量(通常是词嵌入或者前一层的输出)与不同的权重矩阵进行线性变换获得的. 具体步骤如下: 

        - 假设输入序列为 \( X = [x_1, x_2, \dots, x_n] \), 其中每个 \( x_i \) 是一个词的嵌入表示. 
            - 查询(Query, Q): 用于表示当前词对其他词的"关注"程度. 
            - 键(Key, K): 用于表示输入序列中的每个词的特征, 查询通过与键的匹配计算注意力. 
            - 值(Value, V): 表示实际的信息内容, 最终会根据计算出的注意力权重进行加权求和. 
        - 在**self-attention**机制中, 我们通过以下公式生成Q, K和V: 
            \[ Q = X W_Q \quad K = X W_K \quad V = X W_V \]
            - 其中: 
                - \( W_Q, W_K, W_V \) 是训练的权重矩阵, 分别对应查询, 键和值的转换.
                - \( X \) 是输入序列的词嵌入或前一层的输出. 

            - 注意, 这些权重矩阵是不同的, 具体为: 
                - **\( W_Q \)** 用于将输入嵌入转换为查询向量. 
                - **\( W_K \)** 用于将输入嵌入转换为键向量. 
                - **\( W_V \)** 用于将输入嵌入转换为值向量. 

    - 2 计算过程: 
        接下来, 我们通过查询向量\( Q \) 和键向量 \( K \) 计算注意力权重. 具体计算如下: 

        \[ \text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V \]
        其中:
            \( Q K^T \) 计算查询向量与键向量的相似度, 得出每个词对其他词的注意力分数. 
            **softmax** 将这些相似度转换为概率分布(即注意力权重). 
            **\( \frac{1}{\sqrt{d_k}} \)** 是缩放因子, 用来避免内积值过大导致梯度消失或爆炸. 

    - 3 为什么Q和K使用不同的权重矩阵生成: 
        - 1 用途不同: 
            - **Q(查询)** 代表的是当前词想要"查询"其他词的方式, 它描述了当前词的注意力需求. 
            - **K(键)** 代表的是序列中每个词的特征, 它描述了每个词的特征可以被查询的信息. 

            通过不同的权重矩阵生成Q和K, 实际上是在生成**不同类型的特征空间**: 
                Q的权重矩阵 \( W_Q \) 会根据当前词的"意图"来映射其查询特征. 
                K的权重矩阵 \( W_K \) 会根据输入词的"全局特征"来映射其键特征. 

        - 2 增强表达能力: 
            通过分别为Q和K使用不同的权重矩阵, Transformer可以**学习到更加丰富的映射关系**, 使得模型能够在查询时关注到输入中不同的特征. Q和K是用来捕捉不同的语义信息, 因此它们的变换方式应该不同. 简单来说: 
            Q通过权重矩阵 \( W_Q \) 生成, 是对当前词"查询"能力的表达. 
            K通过权重矩阵 \( W_K \) 生成, 是对输入序列中每个词的特征的表达. 

        - 3 如果Q和K使用相同的权重矩阵: 
            如果Q和K使用相同的权重矩阵 \( W \), 那么我们会得到以下关系: 
            \[ Q = K = X W \]
            这意味着查询和键在同一空间中, 而这种情况下, Q和K之间的匹配程度仅仅依赖于输入本身的相似性, 并且不能有效地对不同的语义进行区分, 丧失了模型在特征空间中的灵活性和表达能力. **这样会限制模型的学习能力**, 无法有效地捕捉输入之间的细粒度关系.
    - Summary: 
        - **Q, K, V的生成**: Q, K, V分别是通过输入的词嵌入与不同的权重矩阵(\( W_Q, W_K, W_V \))线性变换得到的. 
        - **Q和K使用不同的权重矩阵**是因为它们的功能不同: Q用于表示当前词的查询意图, K用于表示输入词的特征. 不同的权重矩阵可以帮助模型从不同的空间中学习信息, 使得模型具有更强的表达能力和灵活性. 
        - **不能使用相同权重矩阵**: 如果Q和K使用相同的权重矩阵, 查询和键就会在同一特征空间中, 这会限制模型捕捉不同特征的能力, 降低模型的表达能力. 
        - 使用不同的权重矩阵生成QKV可以保证word embedding在不同空间进行投影, 增强了表达能力, 提高了泛化能力.

- Q **Transformer中的mask机制**

    - transformer中包含padding mask与sequence mask, 前者的目的是让padding(不够长补0)的部分不参与attention操作, 后者的目的是保证decoder生成当前词语的概率分布时, 只看到过去的信息, 不看到未来的信息(保证训练与测试时的一致性)

- Q **transformer在音视频领域落地时需要注意的问题**
    - 1 性能
        - Transformer的attention计算需要全局信息, 会造成系统的高延时以及巨大的存储开销, 在实时的音视频处理中难以落地.
        - 解决方案: 采用Chunk-wise Attention, 将输入分割成固定大小的块(chunk), 仅在块内或相邻块间计算Attention, 降低全局依赖.
        - 流式解码: 设计流式处理机制, 逐块处理输入数据, 确保实时性. 例如, 使用Causal Attention或滑动窗口Attention, 限制Attention范围到当前及历史块.
    - 2 Chunk wise
        将输入序列划分为多个固定大小的块(chunks), 然后在每个块上独立地计算注意力. 与传统的全局注意力机制不同, Chunk-wise attention仅关注每个块内的信息以及块与块之间的有限关联, 而不是对整个输入序列进行全面的注意力计算. 这样可以降低计算复杂度, 特别是在处理长序列数据时, 能够更高效地利用计算资源.

- Q **除了绝对位置编码技术之外, 还有哪些位置编码技术?**

    - **相对位置编码**(RPE)技术, 具体又分三种: 1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数.2.在生成多头注意力时, 把对key来说将绝对位置转换为相对query的位置3.复数域函数, 已知一个词在某个位置的词向量表示, 可以计算出它在任何位置的词向量表示.前两个方法是词向量+位置编码, 属于亡羊补牢, 复数域是生成词向量的时候即生成对应的位置信息.
    - 学习位置编码, 学习位置编码跟生成词向量的方法相似, 对应每一个位置学得一个独立的向量.

- Q **使用Transformer的架构与使用RNN的架构相比有什么优势?**

    - 在Transformer出现之前, 通常使用`LSTM`与`encoder-decoder`架构完成序列到序列的任务, `LSTM`模型考虑到了单词之间的相互依赖, 但是该模型有一个局限性: 训练速度相对较慢, 并且输入序列无法并行传递. Transformer的想法是在不使用循环网络的情况下维持序列中单词的相互依赖性, 而仅使用处于其架构中心的注意力机制. 注意力衡量两个序列的两个元素的相关程度.
    - 在基于Transformer的架构中, 使用`self-attention`. 自注意力层确定同一序列中不同单词的相互依赖关系, 以将相关表示与其相关联. 以这句话为例: "狗没有过马路, 因为它太累了". 对于人类来说, 显然"它"指的是"狗"而不是"街道". 因此, 自注意力过程的目标是检测"狗"和"它"之间的联系". 与`RNN`相比, `self-attention`可以并行训练, 使得Transformer的训练速度更快, 并且已被证明对噪声和丢失数据具有更强的鲁棒性.
    - 另外, 在上下文嵌入中, Transformer可以从上下文中提取信息来纠正丢失或嘈杂的数据, 这是其他神经网络无法提供的.

- **Transformer推理的缓存计算都有哪些?**
    - 在 Transformer 模型推理时, 缓存的计算主要涉及以下几个方面, 尤其是针对自注意力(self-attention)机制和大规模输入数据时的中间缓存. 

    - 1 模型参数的缓存
        - Transformer的主要参数包括: 
            - **权重矩阵**: 每个Transformer层的Q(查询), K(键), V(值)矩阵, 通常是$d_{model}$(特征维度)和$d_{model}$之间的矩阵乘法. 
            - **偏置项**: 通常是每一层都有偏置项. 
        - 如果Transformer的层数为L, d_{model}为每层的特征维度, 则模型的参数缓存可以计算为: 
            - 每一层的Q, K, V 权重矩阵大小为 $d_{model} × d_{model}$, 每层偏置项大小为 d_{model}. 
            - 总的参数量约为$L × 3 × (d_{model} × d_{model} + d_{model})$. 
        例如, 假设模型有 12 层, d_{model} = 768, 则每层的参数量大约是 $3 \times (768 \times 768 + 768)$, 全模型的参数量就是12层的总和. 

    - 2 输入和中间层缓存
        - 对于每个输入, Transformer会依次经过多个注意力机制和前馈网络, 每层都生成中间输出(Activations). 
        - 输入缓存: 
            假设输入序列的长度是T, 每个位置的输入向量维度是$d_{model}$, 那么输入的缓存大小为: 
            $T \times d_{model}$, 即每个位置的向量需要缓存. 

        - 自注意力缓存
            自注意力机制涉及Q, K, V的计算, 每个输入的Q, K, V向量会被计算并缓存. 因为 Transformer 模型通常使用多头注意力(Multi-Head Attention), 所以每个头部都会有独立的Q, K, V矩阵, 但它们共享相同的d_{model}总维度. 

        - 计算每层的自注意力缓存: 
            - Q, K, V向量: 对于每一层, 自注意力机制会生成与输入序列长度一致的Q, K, V向量. 每个Q, K, V向量大小为$T \times d_{model} \times 3$. 
            - 注意力权重矩阵: 每层都会计算一个$T \times T$的注意力权重矩阵, 用于表示不同位置之间的依赖关系. 

        - 因此, 每层自注意力机制的缓存需求为
            - Q, K, V向量缓存: $T \times d_{model} \times 3$(因为 Q, K, V 都有)
            - 注意力权重矩阵缓存: $T \times T$

        - 前馈网络缓存
            前馈网络部分包含两个线性层及激活函数, 其输入和输出的维度通常为d_{model}, 所以它的中间激活值的缓存需求也为$T \times d_{model}$. 

    - 3 渐进缓存与序列推理
        - 对于序列推理(尤其是在生成任务中, 如语言模型生成), Transformer 会将前一时刻的输出作为下一时刻的输入进行推理. 因此, 生成过程中的每个步骤都需要保存某些缓存(例如, 某些中间层的激活值)以便下一个时刻使用. 

        - 对于每个位置的预测, 通常会缓存以下内容
            - 上一时刻的Q, K, V 向量(这与训练时的全序列输入不同, 推理时是逐步生成的)
            - 注意力权重: 需要逐步计算, 但为了加速推理过程, 部分框架(如 GPT 系列)会在缓存中保留之前计算过的 K 和 V 向量, 避免重复计算. 

    - 4 推理过程中的缓存量估算
        - 假设模型有L层, 序列长度T, d_{model}为每个位置的特征维度: 
            - 每层的 Q, K, V 向量的缓存量是 $T \times d_{model} \times 3$. 
            - 每层的注意力权重矩阵缓存是 $T \times T$. 
            - 前馈网络的中间激活缓存是$T \times d_{model}$. 

        推理过程中, 如果是逐步生成的任务, 每一步都可能会缓存新的 Q, K, V 向量以及中间的注意力权重. 

        总的缓存量大约是: 
            - 每层的缓存量: $(T \times d_{model} \times 3 + T \times T + T \times d_{model})$
            - 模型总缓存量: $L \times (T \times d_{model} \times 3 + T \times T + T \times d_{model})$

    - 5 硬件与优化
        现代硬件(尤其是 GPU 和 TPU)会进行优化来减少缓存需求, 例如通过 **分层缓存**, **显存优化** 和 **层间计算合并** 等技术来提高效率. 某些框架也支持 **缓存共享** 和 **渐进推理**, 使得在生成任务中不会保存整个输入序列的中间激活值, 而是逐步计算. 