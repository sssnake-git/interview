# Train

## Training principals

- **LLM的训练步骤**
    - 在深度学习模型, 尤其是像GPT BERT这样的预训练模型的训练过程中, 通常包括几个主要阶段, 分别是: 预训练(Pre-training), 微调(Fine-Tuning, 简称SFT), 奖励建模(Reward Modeling)和强化学习(Reinforcement Learning, 简称RL). 每个阶段都执行不同的任务, 并且它们的输入和输出也各不相同. 以下是这些阶段的详细解释以及每一步的输入和输出.

    - 1 预训练(Pre-training)
        - 目标
            预训练阶段的主要目标是让模型学习到语言的普遍规律和知识(或其他任务的知识), 使模型能够理解大量的背景信息. 这通常是通过大量无监督学习来进行的. 预训练阶段的产出是基础模型(base model), 基础模型一般不会直接使用, 因为它只能完成读写, 无法完成特定的任务. 比如提问中国的首都是哪里? 模型可能会输出一系列的选项, 如: a 上海, b 北京, c 巴黎等, 因为训练语料可能就包括了这样的选择题.

        - 输入
            - **大量的无标签数据**, 通常是文本数据(例如: Wikipedia, BooksCorpus, Common Crawl等). 这些数据并没有手工标注, 仅用于学习词汇, 语法, 上下文等基本语言特征. 

            过程
            - 任务设置: 在预训练中, 模型通常使用自监督学习的方式进行训练, 常见的任务包括: 
                - 语言模型任务(Language Modeling), 自回归语言模型: 通过预测给定上下文下的下一个词(如GPT), 或者遮蔽某些词并预测其值(如BERT)来训练模型. 
                - 掩蔽语言建模(Masked Language Modeling, MLM): 在BERT的训练中, 部分单词被随机掩蔽, 模型需要预测这些掩蔽单词. 
                - 下一句预测(Next Sentence Prediction, NSP): 例如, BERT会接收两句话, 模型需要预测第二句是否与第一句相关联. 

        - 输出
            - 通用的语言表示, 即模型学习到的词向量, 句向量和更高层次的上下文表示. 此时模型尚未专注于特定任务, 只是对输入数据的广泛特征做了建模. 

        - MLM(Masked Language Models), 由BERT(Bidirectional Encoder Representations from Transformers)等双向模型使用, 其中训练集中一定比例的单词被屏蔽, 模型的任务是预测这些缺失的单词.请注意, 在此任务中, 模型可以看到缺失单词之前和之后的单词, 这就是它被称为双向的原因.

        - 自回归(Auto Regressive Model)(例如GPT), 它们是单向的, 预训练时看不到之后的单词, 预测下一个单词. 这是因为这些自回归模型是专门为更好的语言生成而设计的, 这使得模型有必要以单向的方式进行预训练.

        - 预训练阶段是最消耗算力的阶段, 基本上99%的算力用在这个阶段, 主要因为这个阶段训练的数据量巨大, 最近发布的大模型训练数据基本都达到了2T~3T的token. 这些语料形式多样, 包括网络内容, 论文, 代码, 再加上多语种, 目前国内开源大模型的训练语料一般是中英双语. 根据MPT公开的资料, 训练一个MPT-7B Base基础模型动用了440张A100共训练了9.5天, 对应的成本达到20w刀. 为了让大模型能具备特定的能力, 如对话, 就必须对大模型进行微调, 那么就进入到下一个阶段: 监督微调或叫指令微调.

    - 2 微调(SFT, Supervised Fine-Tuning)
        - 目标
            微调是将预训练模型应用到特定任务上(如文本分类, 命名实体识别, 问答等). 通过微调, 模型能在目标任务上优化, 提升其任务特定的表现. 
        - 输入  
            - 带标签的数据集, 每个样本都包含输入和目标输出. 例如, 在情感分析任务中, 输入是文本, 目标是标签(如正面或负面). 常见的输入输出组合包括: 
                - 输入: 一段文本(句子, 段落或文档). 
                - 输出: 标签(如情感分析中的"正面"/"负面"标签, 或问答任务中的正确答案). 
        - 过程
            - 监督学习: 模型会通过优化目标任务的损失函数(如交叉熵损失)来调整模型的参数, 通常是在预训练模型的基础上进行微调. 
            - 训练: 使用标注数据训练模型, 使其能更好地拟合目标任务的特定需求. 
            - 这个阶段的难点不再是对算力的高要求, 转而对微调所需的语料质量有非常高的要求, 对语料的总体要求是少而精, 如上图所示问答对(prompt和答案)的数量一般在10K~100K, 这些语料通常是人工编写的, 也有利用chat-gpt这种超牛大模型输出问答语料, 目前网上也能找到比较丰富的开源的指令微调语料. 对于一个chat模型来说, 它应该能回答用户的提问或根据用户的指令进行输出, 所以语料中包括了各种问答对, 指令集等内容, 例如Alpaca的语料形式如下: 
            ```
            {
                "instruction": "Create a classification task by clustering the given list of items.".
                "input":
                    "Apples, oranges, bananas, strawberries, pineapples".
                "output":
                    "Class 1: Apples, Oranges",
                    "Class 2: Bananas, Strawberries",
                    "Class 3: Pineapples".
                "text":
                    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

                Instruction:
                    Create a classification task by clustering the given list of items.
                Input:
                    Apples, oranges, bananas, strawberries, pineapples
                Response:
                    Class 1: Apples, Oranges,
                    Class 2: Bananas, Strawberries,
                    Class 3: Pineapples".
            }
            ```
            - "text"部分是一个prompt模板, 将instruction, input及output进行格式化. 该prompt作为微调语料, 微调后的模型就能接受这样的prompt格式, 完成用户的指令.
            - 这个阶段完成后其实已经能获得一个可以上线的大模型了(SFT模型). 对于那些需要私有化部署并只要求大模型完成特定任务的场景, 指令微调出来的大模型已经完全满足要求. 比如nl2sql的场景.
        - 输出
            - 任务特定的模型, 此时模型不仅具备通用的语言理解能力, 还能在特定任务上进行优化, 输出目标任务的结果.

    - 3 奖励建模(Reward Modeling)
        - 目标
            奖励建模的目标是通过收集人类反馈来构建一个奖励函数, 用于衡量模型输出的质量. 奖励建模是强化学习微调(RLHF)中的一部分.
        - 输入 
            模型的生成输出: 例如模型生成的对话回复, 文本段落等. 
            人类反馈: 人类为生成的输出打分或提供偏好. 反馈通常是数值(评分)或偏好对(哪一个输出更好). 
        - 过程 
            使用监督学习训练一个奖励模型, 奖励模型的任务是根据人类反馈预测一个数字值, 表示模型生成输出的质量. 
        - 输出
            奖励值: 根据模型的输出和人类反馈, 奖励模型为每个生成的输出分配一个奖励值, 通常是一个标量, 表示该输出的质量或符合度.

    - 4 基于强化学习的微调(RLHF, Reinforcement Learning from Human Feedback)
        - 目标
            RLHF旨在通过强化学习的方式进一步微调模型. 例如: 通过构建奖励函数建模, 衡量模型输出的质量. 目的是使模型能够更好地符合人类的价值观, 偏好或道德判断. 这个过程常见于对话系统或生成模型中, 如ChatGPT. 通过人类的反馈, 强化学习帮助模型进行更细致的调整, 提升模型的行为质量. 
        - 输入  
            - 人类反馈数据: 这些反馈数据通常包含模型输出(如对话回复, 文章生成等), 以及人类的评价(如人工标注的好坏, 偏好分数等). 常见的输入和输出组合包括: 
            - 输入: 模型生成的文本(如对话回复, 文章等). 
            - 输出: 人类对生成文本的评分或偏好(如"有帮助"/"不太相关"评分, 或者"这段话很有趣"/"这段话无意义"之类的反馈). 
        - 过程
            - 奖励值: 根据模型的输出和人类反馈, 奖励模型为每个生成的输出分配一个奖励值, 通常是一个标量, 表示该输出的质量或符合度. 
            - 奖励模型(Reward Model): 首先通过收集人类反馈来构建奖励模型. 奖励模型根据人类反馈对模型的输出进行评分. 
            - 强化学习训练(Reinforcement Learning)**: 然后, 使用强化学习(例如PPO, Proximal Policy Optimization)对模型进行进一步训练, 优化其输出, 使模型输出与人类的期望更加一致. 模型通过最大化获得的奖励来学习如何生成更符合人类偏好的输出. 
        - 输出
            - 符合人类偏好的模型, 此时模型不仅能执行特定任务, 还能够根据人类的反馈调整其输出, 使其更符合道德, 伦理, 和人类的长期目标. 

    - Summary
        - 预训练: 模型通过无标签数据学习语言的通用规律. 
        - 微调(SFT):通过带标签的数据优化模型以执行特定任务. 
        - 奖励建模: 通过人类反馈训练奖励模型, 用于衡量生成输出的质量. 
        - 强化学习(RL): 通过奖励模型的反馈, 使用强化学习优化模型输出, 使其更符合人类期望. 

        | **阶段** | **输入** | **输出** | **过程描述** |
        |-----|-----|-----|-----|
        | **预训练** | 大量无标签文本数据 | 通用语言表示(词向量, 上下文表示) | 学习语言的通用规律和知识, 训练自监督任务 |
        | **微调(SFT)** | 带标签的任务特定数据(如分类标签) | 任务特定的模型(如分类器, 问答系统)| 通过监督学习调整模型, 以优化特定任务 |
        | **奖励建模** | 模型生成的输出, 人类反馈 | 奖励值(根据输出预测的质量)| 训练奖励模型, 通过人类反馈构建奖励函数 |
        | **强化学习(RL)** | 模型生成的输出, 奖励模型的奖励 | 优化后的模型 | 通过强化学习根据奖励信号优化模型行为 |

- **RLHF完整训练过程是什么?**

    - RLHF 完整训练过程
        RLHF (Reinforcement Learning from Human Feedback), 即基于人类反馈的强化学习, 是一种用于训练大型语言模型 (LLM) 的方法, 旨在使模型生成的文本更符合人类偏好和价值观.RLHF 的核心思想是利用人类的反馈作为奖励信号, 指导模型学习生成高质量的文本.
        RLHF 完整训练过程主要包括以下几个步骤: 

        - 1 预训练语言模型 (Pre-trained Language Model)
            - 首先, 使用大量的文本数据预训练一个大型语言模型, 例如 GPT-3.
            - 预训练的目标是使模型能够预测给定文本序列的下一个词语, 从而学习到丰富的语言知识和生成文本的能力.

        - 2 监督微调 (Supervised Fine-tuning)
            - 在预训练模型的基础上, 使用人工标注的高质量对话数据进行监督微调.
            - 微调的目标是使模型生成的文本更加符合人类的表达习惯和偏好.

        - 3 构建奖励模型 (Reward Model)
            - 收集人类对模型生成文本的偏好数据, 例如对不同回复进行排序或评分.
            - 使用这些数据训练一个奖励模型, 用于预测给定文本的质量或符合人类偏好的程度.

        - 4 强化学习 (Reinforcement Learning)
            - 使用强化学习算法 (例如 Proximal Policy Optimization, PPO) 对模型进行训练.
            - 训练的目标是使模型生成的文本能够最大化奖励模型的预测得分, 即生成的文本越符合人类偏好, 获得的奖励越高.

        - 5 迭代优化 (Iterative Optimization)
            - 重复步骤 3 和 4, 不断收集人类反馈并更新奖励模型和语言模型.
            - 通过迭代优化, 使模型生成的文本越来越符合人类的期望.

    - RLHF 训练过程中的关键技术
        - 奖励模型设计: 如何设计一个能够准确反映人类偏好的奖励模型是 RLHF 的关键挑战之一.
        - 强化学习算法: 如何选择合适的强化学习算法以及如何调整超参数对训练效果有重要影响.
        - 数据效率: 如何在有限的人工反馈下训练出高质量的模型是一个重要的研究方向.

    - RLHF 的优势
        - 生成文本质量高: 通过人类反馈的指导, 模型生成的文本更加自然, 流畅, 符合人类偏好.
        - 可控性强: 可以通过调整奖励模型来控制模型生成文本的风格和内容.
        - 鲁棒性好: 模型在面对不同的输入时, 能够生成更加稳定和可靠的文本.

    - RLHF 的应用
        - 对话系统: RLHF 可以用于训练对话系统, 使其能够生成更加自然, 流畅, 有逻辑的回复.
        - 文本生成: RLHF 可以用于生成各种类型的文本, 例如新闻报道, 小说, 诗歌等.
        - 机器翻译: RLHF 可以用于改进机器翻译的质量, 使其翻译结果更加符合人类表达习惯.

    总而言之, RLHF 是一种有效的训练大型语言模型的方法, 可以显著提高模型生成文本的质量和符合人类偏好的程度.