# Reinforcement learning

- Q1 **什么是强化学习?它与其他机器学习技术相比有什么不同?**

    - 强化学习(`RL`)是机器学习的一个子集, 它允许`AI`驱动的系统使用其操作的反馈通过反复试验来学习. 这种反馈要么是消极的, 要么是积极的, 表示为惩罚或奖励, 当然, 目的是使奖励功能最大化.
    - 在学习方法方面, `RL`与监督学习的相似之处在于它使用输入和输出之间的映射关系学习, 但这是它们唯一的共同点. 强化学习与监督学习的不同之处在于, 在监督学习中, 训练数据具有答案(`label`), 因此模型本身使用正确答案进行训练, 而在强化学习中, 没有答案, 代理决定自己要做什么才能正确执行任务
    - 与无监督学习相比, `RL`有不同的目标. 无监督学习的目标是找到数据点之间的相似点或不同点. `RL`的目标是找到最合适的动作模型来最大化`RL Agent`的总累积奖励. 在没有训练数据集的情况下, `RL`问题是通过代理自身的动作和来自环境的输入来解决的.

    - 引用: [https://www.geeksforgeeks.org/what-is-reinforcement-learning/](https://www.geeksforgeeks.org/what-is-reinforcement-learning/)

- Q2 **什么是马尔科夫决策过程?**

    一个状态是马尔科夫决策过程, 意味着当前状态只依赖于上一个状态, 而不依赖于之前的任何状态

    $$
    P(S_{t+1}|S_{t})=P(S_{t+1}|S_{1},...,S_{t})
    $$

- Q3 **直观地解释什么是强化学习中的策略**

- 典型的强化学习(`RL`)问题具有一些基本要素, 例如:
    - 环境: 代理运行的物理世界.
    - 状态: 代理的当前状态.
    - 奖励: 来自环境的反馈.
    - 策略: 将代理的状态映射到操作的方法.
- 我们可以将策略视为代理的行为方式.例如, 想象一个机器人穿过房间的世界, 任务是到达目标点(x, y), 并在那里获得奖励.这里: 
    - 一个房间就是一个环境.
    - 机器人的当前位置是一个状态.
    - 策略是代理(机器人)为完成此任务所做的事情.机器人有几个选项: 
        - 策略1: 愚蠢的机器人只是随机四处游荡, 直到它们意外地出现在正确的地方.
        - 策略2: 由于某种原因, 其他机器人可能会学会沿着大部分路线的墙壁走.
        - 策略3 : 智能机器人在“头脑”中规划路线, 直奔目标.
    
    显然, 有些策略比其他策略更好, 并且有多种方法可以评估它们,  RL 的目标是学习最佳策略.在这个例子中, 最好的策略是 option 3.策略定义了学习代理在给定时间的行为方式, 并且通常由代理用来决定应该执行什么操作.
    

Q4 **折扣因子在强化学习中的作用是什么?**

- 折扣因子$\gamma$是一个真实的价值, $ \gamma \in [0, 1]$, 关心智能体在过去, 现在和未来获得的奖励. 换句话说, 它将奖励与时域联系起来.让我们探讨以下两种情况:
    - 设置折扣因子𝛾 = 1时, 意味着我们考虑了所有未来的回报.然后代理会永远学习, 寻找所有未来的奖励, 这可能会导致无穷大.
    - 设置折扣因子𝛾 = 0时, 这意味着我们只考虑即时奖励, 而不考虑从未来时间步获得的奖励.那么代理将永远不会学习, 因为它只会考虑即时奖励.
    - 因此, 折扣因子的最优值介于 之间0.2, 0.8它根据任务设置即时和未来奖励的重要性.在某些任务中, 未来奖励比即时奖励更可取, 反之亦然.
    
    例如, 在象棋游戏中, 目标是打败对方的王.如果我们更加重视即时奖励, 即通过我们的棋子击败任何对手棋子等行动获得的奖励, 那么代理将学习执行此子目标而不是学习实际目标.所以, 在这种情况下, 我们更看重未来的奖励而不是即时的奖励, 而在某些情况下, 我们更喜欢即时的奖励而不是未来的奖励.例如, 您可能更喜欢今天吃一块新鲜的巧克力片, 而不是几天后.
    

Q5 **说一些你知道的解决强化学习问题的方法或算法**

- 动态规划(DP): 当模型完全已知时, 根据贝尔曼方程, 我们可以使用DP迭代评估价值函数并改进政策.
- 蒙特卡洛 (MC) 方法: 它从原始经验的片段中学习, 而不对环境动态进行建模, 并将观察到的平均回报计算为预期回报的近似值.这里的一件重要事情是剧集必须是完整的, 这意味着所有剧集最终都必须终止.
- 时间差分 (TD) 学习: 与蒙特卡洛方法类似, TD学习是无模型的, 可以从经验中学习.然而, TD 学习可以从不完整的情节中学习, 因此我们不需要跟踪情节直到终止.
- 策略梯度: 所有以前的方法都旨在学习状态/动作值函数, 然后相应地选择动作.Policy Gradient方法直接根据某些参数学习策略函数`θ`, 因此我们的目标是找到`θ`产生最高回报的最佳方法.
- Evolution Strategies (ES): 它通过模仿达尔文的物种进化理论通过自然选择来学习最优解.应用ES的两个先决条件: (i)我们的解决方案可以自由地与环境交互, 看能否解决问题, (ii) 我们能够计算每个解决方案的适合度得分.我们不必知道环境配置来解决问题.

Q6: ****如何定义强化学习中的状态?****

- 强化学习(RL)中的**状态表示**问题类似于有监督或无监督学习中的特征表示, 特征选择和特征工程问题.
- 对复杂问题建模的常用方法是**离散化**.在基本层面上, 这是将一个复杂且连续的空间拆分成一个网格.然后, 您可以使用任何专为离散, 线性, 空间设计的经典 RL 技术.
- 使用**表格学习算法**是另一种定义状态的好方法, 因为它们具有合理的收敛理论保证, 这意味着如果您可以简化您的问题, 使其具有少于几百万个状态, 那么这值得尝试.
- 即使您将它们离散化, 大多数有趣的控制问题也不适合那么多状态.这是由于**维度的诅咒**.对于这些问题, 您通常会将您的状态表示为不同特征的**向量**——例如, 对于学习行走的机器人, 机械部件的各种位置, 角度, 速度.与监督学习一样, 您可能希望将这些用于特定的学习过程.例如, 通常你会希望它们都是数字, 如果你想使用神经网络, 你也应该将它们归一化到一个标准范围(例如`-1`到`1`).

Q7: **状态的奖励和价值之间有什么区别?**

- **奖励**是在马尔可夫决策过程的**某个步骤**返回的数字.如果用$r,s,a,s^{'}$代表奖励, 状态, 动作以及下一个状态, 奖励依赖于状态, 动作以及下一个状态: $r=R(s,a,s^{'})$
- 价值是衡量处于一种状态**的长期利益**的一种方式, 它们也被称为代理人从该状态开始并遵循特定策略的预期回报
- 因此, 我们可以看到**状态值**由许多**奖励** 组成, 这些奖励由它们在未来发生**的概率加权**

Q8: **怎么判断 Q-Learning 算法何时收敛?**

- 当学习曲线变得平坦并且不再增加时, 强化学习算法被认为收敛.但是由于探索参数ε不是逐渐增加的, 在达到最优策略之前, Q-Learning 会以过早的方式收敛.
- 因此, 必须满足两个条件才能保证极限收敛: 
    - **学习率接近于零, 但不能以太快的速度**.形式上, 这要求学习率之和必须发散, 但它们的平方和必须收敛.具有这些属性的示例序列是1/1, 1/2, 1/3, 1/4, ....
    - **每个状态-动作对必须被无限频繁地访问**.这有一个精确的数学定义: 每个动作在每个状态下都必须有一个非零的概率被策略选择, 即π(s, a) > 0对于所有(s, a).在实践中, 使用ε-贪心策略(where ε > 0) 可确保满足此条件.

引用: [https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q9: ****蒙特卡洛, TD, 动态规划的关系****

- 共同点: 三者都是用于进行值函数的描述与更新, 蒙特卡洛和TD算法隶属于model-free, 而动态规划属于model-based, TD算法和蒙特卡洛的方法因为都是基于model-free的方法, 因而对于后续状态的获知也都是基于试验的方法, TD算法和动态规划的策略评估都能基于当前状态的下一步预测情况来得到对于当前状态的值函数的更新
- TD算法和蒙特卡洛的方法的不同之处在于: TD算法不需要等到实验结束后 才能进行当前状态的值函数的计算与更新, 而蒙特卡洛的方法需要试验交互, 产生一整条的马尔科夫链(到最终状态)才能进行更新.

Q10: ****off-policy和on-policy的好与坏****

- on-policy优点因为是单纯的依照交互数据来进行更新, 所以直接了当.但速度不一定快.毕竟试验交互时候还有样本的采样效率的问题, 可以看到off-policy可以用到之前的经验, 而on-policy只能从新采样, 因而速度不一定快.同时劣势是不一定找到最优策略.
- 而off-policy优势在于更强的通用性.保证了探索性, 其强大是因为它确保了数据全面性, 所有行为都能覆盖.(在选取$s_{next}$对应action的时候off-policy需要选取max的一个, 那么需要全部action的q值, 于是覆盖了全部的action, 相比之下on-policy只是单纯的依照策略选取了某个action)但劣势也很明显, 就是每次对于状态的值函数的估计 都是过高的进行估计.同时毕竟是基于采样的方式的 所以会有些状态没有被采样到, 这就产生的偏差

Q11: ***典型的强化学习*算法涉及哪些*步骤?***

- 首先, 代理通过执行动作与环境交互.
- 代理执行一个动作并从一种状态移动到另一种状态.
- 然后代理将根据其执行的操作获得奖励.
- 基于奖励, 代理将了解该操作是好是坏.
- 如果行动是好的, 也就是说, 如果代理人收到了积极的回报, 那么代理人会更愿意执行那个行动, 否则代理人会尝试执行其他可以产生积极回报的行动.所以强化学习基本上是一个试错学习过程.

![Untitled](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%2033a57369e58e46a9a6e4d073a2230a5e/Untitled.png)

Q12: *****Stationary Dynamics*和*Stationary Policy*意味着什么?**

- Stationary Policy是一种不依赖于时间的策略.这意味着只要满足某些条件, 代理就会做出相同的决定.这种固定策略可能是概率性的, 这意味着选择动作的概率保持不变.它可能会做出不同的决定, 但概率保持不变.这通常意味着策略没有被学习算法更新.
- Stationary Dynamics指的是环境, 是环境规则不随时间变化的假设.环境规则通常表示为马尔可夫决策过程模型, 该模型由所有状态转移概率和奖励分布组成.因此, 在这种情况下, 奖励函数和转移概率保持不变, 或者变化足够慢, 代理可以找到足够的训练时间来学习环境中发生的变化.

Q13: ****Q Learning 中的*Alpha*和*Gamma*参数代表什么?**

- alpha 是一个权衡上一次学到结果和这一次学习结果的量, 即: `Q = (1-alpha)*Q_old + alpha*Q_current.`alpha 设置过低会导致机器人只在乎之前的知识, 而不能积累新的 reward.一般取 0.5 来均衡以前知识及新的 reward.
- gamma 是考虑未来奖励对于现在影响的因子, 是一个(0,1)之间的值.一般我们取0.9, 能够充分地对外来奖励进行考虑.实际上如果你将它调小了, 你会发现终点处的正奖励不能够“扩散”到周围, 也就是说, 机器人很有可能无法学习到一个到达终点的策略.

Q14: ***Q 函数*与强化学习中的*值函数有何不同?***

- 与价值函数类似, Q 函数是一种质量度量.与价值函数不同的是, Q 函数衡量的是在给定状态下采取特定行动的预期折扣奖励, 而价值函数衡量的是在策略 π 下状态 s 的预期总价值

引用: [https://core-robotics.gatech.edu/](https://core-robotics.gatech.edu/)

Q15: **DQN相比Q-learning有哪些改变?**

- DQN采用了深度卷积神经网络来进行值函数逼近, 这里选取卷积神经网络的原因也在于 原文是针对Atari游戏来作为environment的, 输入状态采用的是84x84的图片.CNN凭借着本身稀疏连接和参数共享 相比于FCN来说 计算量大大减少, 存储的参数量也大大减少, 进而本身有着的局部特征和平移等变性 也让CNN很适合用于处理图片.当然尽管说这么多, 这也不是什么新鲜操作了, 但加入了第二点 就完全不同了, 解决了RL里面的一个痛点, 
- 加入了经验回放的操作来训练强化学习, 如果直接借助强化学习交互产生的数据, 本身是带有关联性的, 而在神经网络中, 对于数据的基本要求就是独立同分布.因而这里引入了经验回放这个操作, 来打破了数据间的关联性, 具体操作就是: agent在与环境交互的时候 将交互数据存放在一个库里面, 然后训练的时候从中随机采样数据进行训练, 
- 加入了目标网络的概念来单独的处理TD算法中的TD偏差

Q16: ***Q-Learning*和*SARSA*之间有什么区别?**

- Q-L 是一种off-policy的方法, 使用贪婪的方法, 直接学习最优策略, 
- 而 SARSA 是一种on-policy方法, 使用当前策略执行的操作来学习 Q 值, 学习“接近”最优策略
- QL 是一种更激进的代理, 而 SARSA 更保守

Q17: **强化学习的损失函数(loss function)是什么?和深度学习的损失函数有何关系?**

- 累积回报.依赖于不同的问题设定, 累积回报具有不同的形式.比如对于有限长度的MDP问题直接用回报和作为优化目标.对于无限长的问题, 为了保证求和是有意义的, 需要使用折扣累积回报或者平均回报.深度学习的损失函数一般是多个独立同分布样本预测值和标签值的误差, 需要最小化.强化学习 的损失函数是轨迹上的累积和, 需要最大化.

Q18:  **POMDP是什么?马尔科夫过程是什么?马尔科夫决策过程是什么?里面的“马尔科夫”体现了什么性质?**

- POMDP是部分可观测的马尔科夫决策问题. 马尔科夫过程表示一个状态序列, 每一个状态是一个随机变量, 变量之间满足马尔科夫性, 表示为 一个元组<S, P>, S是状态, P表示转移概率. MDP表示为一个五元组$<S,A,P,R,\gamma>$, S是状态集合, A是动作集合, P表示转移概率, 即模型,  R是回报函数, $\gamma$表示折扣因子. 马尔科夫体现了无后效性, 也就是说未来的决策之和当前的状态有关, 和历史状态无关.

# Medium:

Q19: **最优值函数和最优策略为什么等价?**

- 最优值函数唯一的确定了某个状态或状态-动作对相对比其他状态和状态-动作对的利好, 我们可以依赖这个值唯一的确定当前的动作, 他们是对应的, 所以等价.

Q20: **值迭代和策略迭代的区别?**

- 策略迭代有两个循环, 一个是在策略估计的时候, 为了求当前策略的值函数需要迭代很多次, 另外一个是外面的大循环, 就是策略评估, 策略提升这个循环.值迭代算法则是一步到位, 直接估计最优值函数, 因此没有策略提升环节.

Q21: **如果不满足马尔科夫性, 当前时刻的状态和它之前很多很多个状态都有关系, 该如何处理问题?**

- 如果不满足马尔科夫性, 强行只用当前的状态来决策, 势必导致决策的片面性, 得到不好的策略. 为了解决这个问题, 可以利用RNN对历史信息建模, 获得包含历史信息的状态表征.表征过程可以使用注意力机制等手段.最后在表征状态空间求解MDP问题.

Q22: **简述蒙特卡罗估计值函数(MC)算法**

- 蒙特卡洛方法就是采样仿真, 蒙特卡洛估计值函数就是根据采集的数据, 利用值函数的定义来更新值函数.这里假设是基于表格的问题, 步骤如下: 
    - 初始化, 这里为每一个状态初始化了一个 Return(s) 的列表.可以想象列表中每一个元素就是一次累积回报.
    - 根据策略pi生成轨迹
    - 利用轨迹, 统计每个状态对应的后续累积回报, 并将这个值加入对应状态的Return(s)列表
    - 重复若干次, 每次都会往对应出现的状态回报列表中加入新的值.最后根据定义, 每个状态回报列表中数字的均值, 就是他的值估计

引用: [https://zhuanlan.zhihu.com/p/55487868](https://zhuanlan.zhihu.com/p/55487868)

Q23: **简述时间差分(TD)算法**

- TD,MC和DP算法都使用广义策略迭代来求解策略, 区别仅仅在于值函数估计的方法不同.DP使用的是贝尔曼 方程, MC使用的是采样法, 而TD方法的核心是使用自举(bootstrapping), 即值函数的更新为:  $V(s_t) <-- r_t + \gamma V(s_next)$, 使用了下一个状态的值函数来估计当前状态的值.

Q24: **简述Q-Learning, 写出其Q(s,a)更新公式.它是on-policy还是off-policy, 为什么?**

- Q学习是通过计算最优动作值函数来求策略的一种算法, 更新公式为: 

$$
Q(s_t, a_t) <--- Q(s_t, a_t) + \alpha [R_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
$$

是off-policy的, 由于值更新使用了下一个时刻的argmax_a Q, 所以我们只关心哪个动作使得 Q(s_{t+1}, a) 取得最大值,  而实际到底采取了哪个动作(行为策略), 并不关心. 这表明优化策略并没有用到行为策略的数据, 所以说它是off-policy的.

Q25: **value-based和policy-based的区别是什么?**

- value-based通过求解最优值函数间接的求解最优策略, policy-based的方法直接将策略参数化,  通过策略搜索, 策略梯度或者进化方法来更新策略的参数以最大化回报.基于值函数的方法不易扩展到连续动作空间, 并且当同时采用非线性近似, 自举和离策略时会有收敛性问题.策略梯度具有良好的收敛性证明.

Q26: **DQN的两个关键trick分别是什么?**

- 使用目标网络(target network)来缓解训练不稳定的问题, 经验回放, 
- 在DQN中某个动作值函数的更新依赖于其他动作值函数.如果我们一直更新值网络的参数, 会导致 更新目标不断变化, 也就是我们在追逐一个不断变化的目标, 这样势必会不太稳定.引入目标网络就是把更新目标中不断变化的值先稳定一段时间, 更新参数, 然后再更新目标网络.这样在一定的阶段内 目标是固定的, 训练也更稳定.
- 经验回放是为了消除样本之间的相关性.

Q27: **描述随机策略和确定性策略的特点?**

- 随机策略表示为某个状态下动作取值的分布, 确定性策略在每个状态只有一个确定的动作可以选. 从熵的角度来说, 确定性策略的熵为0, 没有任何随机性.随机策略有利于我们进行适度的探索, 确定性策略的探索问题更为严峻.

Q28: **策略迭代(Policy Iteration)与值迭代(Value Iteration)分别的优势是什么?**

- 策略迭代是在策略评估之后, 等到值函数收敛之后再进行策略改进, 而值迭代是在策略评估的时候, 只要值函数改变, 就直接进行策略改进
- 值迭代: 消除了策略迭代中对超参数的需求, 方法中策略评估只需要一次迭代, 需要的运算量更小, 应该比策略迭代更快收敛
- 策略迭代: 当策略满足终止条件时, 因为策略对应的值函数已经收敛, 可以保证策略是最优的

Q29: **蒙特卡洛方法是否适用于所有任务?**

- 蒙特卡洛方法通过分别取状态的平均回报和状态-动作对的平均回报来计算价值函数和Q函数.但蒙特卡洛方法的一个问题是它仅适用于情景任务.

Q30: **蒙特卡洛预测方法如何计算价值函数?**

- 蒙特卡洛计算值函数的方法有两种: first-visit MC和every-visit MC.
- 计算值函数最经典的方法是对状态的每个first visit进行采样, 然后计算平均值, 也就是first-visit MC prediction
- 另一种方法则是every-visit MC prediction, 即计算s的所有visit的平均值, 虽然两者有轻微不同, 但同样的, 如果visit次数够大, 它们最后会收敛到相似值.

Q31: **如何区分情景任务与连续任务?**

- 一个连续的任务可以永远持续下去, 一个情节任务至少有一个有限状态(即游戏结束).从数学上讲, 情节任务的状态转移概率为 1 到自身, 其他任何地方的转移概率为 0.

引用: [https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q32: **使用时间差异(TD)方法与蒙特卡洛方法进行强化学习时,  分别有哪些优势?**

- 

| 蒙特卡罗方法 | 时间差异学习 |
| --- | --- |
| MC必须等到episode结束. | TD可以在每一步后在线学习, 不需要等到episode结束. |
| MC 具有高方差和低偏差. | TD 具有低方差和一些不错的偏差. |
| MC 不利用马尔可夫属性. | TD 利用了 Markov 属性. |

引用: [https://www.oreilly.com/library/view](https://www.oreilly.com/library/view)

Q33: **使用动态规划方法与蒙特卡洛方法进行强化学习时,  分别有哪些区别?**

- 动态规划需要对环境或所有可能的转换有完整的了解, 而蒙特卡罗方法在一个事件的采样状态-动作轨迹上工作.DP 仅包括一步转换, 而 MC 会一直走到episode的结尾, 直到终端节点.

Q34: **Deep Q-Learning 中的episode和epoch有什么区别?**

- episode代表一系列状态, 动作和奖励, 以最终状态结束.例如, 玩整个游戏可以被认为是一个情节, 当一个玩家输/赢/平时达到最终状态, 一个epoch代表所有训练示例的一次前向传递和一次反向传递.可以将一个eopch视作多个episode的外部循环.

引用: [https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q35: **Deep Q-network和Categorical Deep Q-Network有什么区别?**

- 分类深度 Q 网络的主要目标是学习 Q 值的分布, 这与深度 Q 网络的其他变体不同, 后者的目标是尽可能接近 Q 值的期望.在复杂环境中, Q 值可以是随机的, 在这种情况下,  仅仅学习 Q 值的期望将无法捕获所需的所有信息(例如分布方差)以做出最佳决定.

引用: [https://genrl.readthedocs.io/](https://genrl.readthedocs.io/)

Q36: **什么情况下, 使用Deep Recurrent Q-network?**

- 深度递归的Q-learning 利用递归神经网络, 允许 Q 函数学习隐藏状态.这对于部分可观察的 MDP 很有用.这些是具有部分观测或高度时间相关状态的 MDP, 类似于像扑克(部分手牌隐藏)这样的游戏

Q37: **如果环境也是随机的, 最优策略是否总是随机的?**

- 并不是, 最佳策略通常是确定性的, 除非: 
    - 缺少重要的状态信息(POMDP).例如, 在不允许代理知道其确切位置或记住先前状态的地图中, 并且给定的状态不足以消除位置之间的歧义.如果目标是到达特定的终端位置, 则最佳策略可能包括一些随机移动, 以避免卡住.请注意, 在这种情况下, 环境可能是确定性的(从可以看到整个状态的人的角度来看), 但仍会导致需要随机策略来解决它.
    - 存在某种最小极大博弈论场景, 其中确定性策略可能受到环境或其他代理的惩罚
- 环境随机情况下, 任何可以通过MDP建模并通过基于值的方法(例如价值迭代, Q学习)解决的环境都具有确定性的最优策略.

引用: [https://ai.stackexchange.com/](https://ai.stackexchange.com/)

Q38: ****并行环境在强化学习中有什么影响?****

- 并行环境提高了智能体的学习能力, 从以下两个方面: 
    - 一次从多个轨迹收集数据会降低数据集中的相关性.这改善了神经网络等在线学习系统的收敛性
    - 数据收集总体上更快, 从而缩短了时间以获得相同的结果.这也可以更好地利用其他资源
- 并行环境需要保证训练的稳定性, 常用的措施是使用策略方法, 或者将函数近似器用于策略函数或值函数

Q39: **深度Q学习和策略梯度法有什么区别?**

- Deep-Q学习是一种基于值的方法, 计算动作空间中每个动作的 Q 值, 并选择最大值及其相应的动作, 而策略梯度是一种基于策略的方法
- 使用策略梯度法的优点是可以学习随机策略(输出每个动作的概率), 并且通常策略函数**π**比V或Q更简单

引用: [Ch:13: Deep Reinforcement learning — Deep Q-learning and Policy Gradients ( towards AGI ). | by Madhu Sanjeevi ( Mady ) | Deep Math Machine learning.ai | Medium](https://medium.com/deep-math-machine-learning-ai/ch-13-deep-reinforcement-learning-deep-q-learning-and-policy-gradients-towards-agi-a2a0b611617e)

Q40: **为什么我们需要深度Q网络中的目标网络?**

- 目标网络的作用是为了稳定学习.在Q学习中, 代理的大脑是Q value, 但在DQN中, 代理的大脑是一个深度神经网络.

引用: [deepmellow.pdf (brown.edu)](https://cs.brown.edu/people/gdk/pubs/deepmellow.pdf)

# HARD

Q41: **为什么常规 *Q 学习*和 *DQN* 会高估 Q 值?**

- 高估是由于Q 值估计值的随机初始化引入的正偏差造成的, 因为 Q 学习使用最大操作值作为最大预期操作值的近似值
- Q 值非常嘈杂, 因此当您对所有操作取最大值时, 您可能会得到一个高估的值: 

$$
Q(s, a) = r + \gamma \text{max}_{a'}[Q(s', a')]
$$

- 解决方案(双 Q 学习)是使用两个不同的函数近似器, 它们在不同的样本上训练, 一个用于选择最佳动作, 另一个用于计算该动作的值, 因为两个函数近似器看到不同的样本, 因此它们不太可能高估相同的动作.

Q42: ****SARSA可以用于部分可观察的马尔可夫决策过程吗?如果是(或不是), 为什么?****

- SARSA算法使用并更新状态$s$和$a,Q(s,a)$, 该算法会假设当前的状态$s$是已知的, 然而, 在POMDP中, 在每个时间步长, 代理都不知道当前状态, 但它保持当前状态的“信念”(在数学上表示为概率分布), 因此它不能最优化$Q(s,a)$, 因此, SARSA算法不可以直接用于部分可观察的马尔科夫过程

引用: [https://ai.stackexchange.com/](https://ai.stackexchange.com/)

Q43: **Advantage Actor-Critic (A2C) 和 Asynchronous Advantage Actor-Critic (A3C) 之间有什么区别?**

- (A3C)与A2C的主要区别在于异步部分.A3C 由多个具有自己权重的独立代理(网络)组成, 它们与环境的不同副本并行交互.