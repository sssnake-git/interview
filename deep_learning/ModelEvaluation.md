# Model Evaluation

Q1: ***解释机器学习中的过拟合现象?***

- **过拟合**是指模型对训练数据建模得太好.
- 当模型学习训练数据中的细节和噪声达到对模型在新数据上的性能产生负面影响的程度时, 就会发生**过度拟合.**这意味着训练数据中的噪声或随机波动被模型拾取并学习为概念, 问题是这些概念并不适用于新数据并对模型的泛化能力产生负面影响

![Untitled](Model%20Evaluation%20df5d033abf22444284cfa8ba84a1438a/Untitled.png)

Q2: ***解释机器学习中的欠拟合现象***

- **欠拟合**是指既不能对训练数据良好建模也不能泛化到新数据的模型.欠拟合的机器学习模型不是合适的模型, 因为它在训练数据上的表现不佳.
- **欠拟合**通常不被讨论, 因为在给定良好性能指标的情况下很容易检测到.欠拟合的补救措施是使用更适合的机器学习算法

Q3: ***什么是hyper-parameters?***

- 每个机器学习模型都有参数, 还*可以*有超参数, 参数通过模型训练更新.**超参数**是那些不能从常规训练过程中直接学习的参数**.**这些参数表示模型**的高级**属性, 例如它的*复杂性***或**它*应该学习的速度*.

Q4: ***如何检测到机器学习中的过拟合现象?***

- **过拟合**的常见模式可以在**学习曲线**图上看到, 其中模型在训练数据集上的性能不断提高, 但在测试或验证集上的性能提高到一个点然后开始变差.因此, 过拟合模型的**训练误差极低, 但测试误差却很高**.

![Untitled](Model%20Evaluation%20df5d033abf22444284cfa8ba84a1438a/Untitled%201.png)

Q5: ***什么是模型学习率?学习率高或低对模型学习有什么影响?***

- **学习率**是一个调整参数, 它决定了模型训练期间每次迭代(epoch)的步长.步长是您响应估计误差更新神经元权重的速度.使用反向传播误差方法更新模型权重时, 梯度将从模型的输入节点通过神经元流向输出节点, 然后确定误差并反向传播以更新神经元(模型)的权重.更新这些神经元权重的速度就是学习率.
- 如果学习率**很高**, 那么模型权重更新得很快而且很频繁, 模型会很快收敛, 但它可能会超过真正的误差最小值.**这意味着一个更快收敛但错误的模型.**
- 如果学习**率低**, 那么模型权重更新缓慢, 模型将需要很长时间才能收敛, 但不会超过真正的误差最小值.**这意味着一个更慢但更准确的模型.**

Q6: ***如何判断你的模型是否存在梯度爆炸问题?***

- 以下现象表明你训练的模型可能存在梯度爆炸的问题, 例如: 模型无法很好地学习训练数据(模型损失一直很高), 模型训练不稳定(每一次更新, loss变化很大), 训练时loss变为NaN, 
- 当发生以上现象时, 可以深入挖掘模型看看是否有梯度爆炸的问题, 以下现象可以确认发生梯度爆炸: 训练阶段模型权重变为NaN, 在训练期间, 每个节点和层的误差梯度值始终高于`1.0`, 

Q7: **知道哪些关于*Hyperparameters Tuning的*方法?**

- **随机搜索**: 为超参数创建了一个可能值的网格.每次迭代都会尝试从该网格中随机组合超参数, 记录性能, 最后返回提供最佳性能的超参数组合
- **网格搜索**: 类似于手动调优, 为网格中指定的所有给定超参数值的每个排列建立模型, 并评估和选择最佳模型.最后, 它返回具有最佳超参数的最佳模型
- **贝叶斯优化: **贝叶斯优化使用先前对损失的观测, 来确定下一个(最佳)的参数点来取样损失

引用: [https://www.geeksforgeeks.org/hyperparameter-tuning/](https://www.geeksforgeeks.org/hyperparameter-tuning/)

Q8: ***在设计神经网络时, 有哪些方法防止过拟合?***

- 简化模型: 为了降低复杂性, 我们可以简单地删除层或减少神经元数量以使网络更小.
- early-stopping: 正则化的一种形式.NN使用梯度下降更新网络参数, 但过了某个训练阶段, 改进模型对训练数据的拟合会导致泛化误差增加.过了那个点, 改进模型对训练数据的拟合会导致泛化误差增加.early-stopping规定了网络可以训练的迭代次数.

![Untitled](Model%20Evaluation%20df5d033abf22444284cfa8ba84a1438a/Untitled%202.png)

- 数据扩充: 增加数据集中存在的图像数量.一些流行的图像增强技术包括翻转, 平移, 旋转, 缩放, 改变亮度, 添加噪声
- 正则化: L1正则化以及L2正则化, L1正则化的目标是最小化权重的绝对值, L2正则化的目标是最小化权重的平方幅度
- 使用dropout策略, dropout在每次迭代的训练过程中随机从神经网络中删除神经元.当我们丢弃不同的神经元组时, 相当于训练了不同的神经网络.不同的网络会以不同的方式过拟合, 所以 dropout 的最终效果是减少过拟合

引用: [https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html)

Q9: ***如何解决逻辑回归中的过拟合问题?***

- 减少使用的特征数量, 丢弃那些提供信息量不大的特征
- 使用L2正则化

Q10: ***什么是混淆矩阵?***

- 混淆矩阵是机器学习分类问题的性能度量, 它是一个包含预测值和实际值的 4 种不同组合的表格, 对于测量召回率, 精度, 特异性, 准确性以及最重要的 AUC-ROC 曲线非常有用.
- TP表示实际值为真, 预测值也为真, FP表示实际值为假, 预测值为真, FN表示实际值为真, 预测值为假, TN表示实际值为假, 预测值也为假

![Untitled](Model%20Evaluation%20df5d033abf22444284cfa8ba84a1438a/Untitled%203.png)

Q11: ***解释ROC曲线***

- 首先介绍两个概念, TPR与FPR, $TPR = \frac{TP}{TP+FN}$, 代表分类器预测的**正类中**实际正实例占所有正实例的比例, $FPR = \frac{FP}{TN+FP}$, 代表分类器预测的**正类中**实际负实例占所有负实例的比例
- ROC 曲线是用 TPR 与 FPR 绘制的概率曲线, 其中 TPR 在 y 轴上, FPR 在 x 轴上.
- ROC曲线有个很好的特性: 当测试集中的正负样本的分布变化的时候, ROC曲线能够保持不变, 有助于解决数据类别不均衡的情况

Q12: ***解释AUC值***

- AUC(Area under Curve): ROC曲线下的面积, 介于0.1和1之间.Auc作为数值可以直观的评价分类器的好坏, 值越大越好.**AUC的含义为, 当随机挑选一个正样本和一个负样本, 根据当前的分类器计算得到的score将这个正样本排在负样本前面的概率.**

Q13: ***ROC曲线和AUC值如何帮助衡量模型的好坏?***

- 优秀模型的 AUC 接近 1, 这意味着它具有很好的可分离性度量.差模型的 AUC 接近 0, 这意味着它具有最差的可分离性度量.AUC为0时, 模型反向分类, 将 0 预测为 1, 将 1 预测为 0.当 AUC 为 0.5 时, 这意味着该模型没有任何类别分离能力.

Q14: ***使用AUC值衡量模型性能有什么好处和坏处?***

- 优点: 使用AUC衡量性能时, 对****正负类样本是否均衡并不敏感****, 另外, AUC 继承了 ROC 曲线评估指标无需手动设定阈值的优良特性, 直接从整体上(离线排序)方面衡量算法的表现.
- 缺点: AUC只能衡量二分类问题的性能, AUC只反映了模型的整体性能, 看不出在不同点击率区间上的误差情况, AUC只反映了排序能力, 关注的是概率值的相对大小, 与阈值和概率值的绝对大小没有关系, 没有反映预测精度, 

引用: [https://medium.com/@penggongting/understanding-roc-auc-pros-and-cons-why-is-bier-score-a-great-supplement-c7a0c976b679](https://medium.com/@penggongting/understanding-roc-auc-pros-and-cons-why-is-bier-score-a-great-supplement-c7a0c976b679)

[https://blog.csdn.net/Dby_freedom/article/details/89814644](https://blog.csdn.net/Dby_freedom/article/details/89814644)

Q15: ***解释什么是F1-Score***

- F1 分数, 是衡量模型在数据集上的准确性的指标, 它用于评估二元分类系统.F1-score是一种定义为模型精确率和召回率的调和平均值: 

$$
F1=\frac{2*precision*recall}{precision+recall}
$$

Q16: ***当训练数据存在类别间分布不平衡时, 如何选择评估指标?***

- 在不平衡类别的情况下, 准确性指标(Accuracy)可能会产生误导, 因为数据不均衡情况下, 准确性不会反映少数类别的预测能力
- 数据不平衡时, 可以选择AUC, F1 Score作为评估指标

Q17: ***使用早停策略时, 可能会有什么问题?***

- 使用早停策略时, 模型可能未完全学习到数据中的分布规律, 一种解决方法是将训练的epoch视为超参数, 并网格搜索一系列不同的值, 使用 k 折交叉验证, 找到最优的训练epoch以找到最终模型

引用: [https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/)

Q18: ***算法A具有更高的Accuracy, 算法B具有更高的Recall(召回率), 如何判断算法A和算法B哪个更好?***

- 高accuracy和高recall算法的选择, 视算法的应用场景而定
- 当需要对输出敏感的预测时, 模型需要高召回率.例如, 预测癌症或预测恐怖分子需要高召回率, 将非癌性肿瘤标记为癌性是可以的, 但癌性肿瘤不应标记为非癌性
- 在推荐引擎, 垃圾邮件检测等地方需要高精度.在这些地方你不关心漏报, 更关注真报和漏报.如果垃圾邮件进入收件箱文件夹是可以的, 但是非常重要的邮件不应该进入垃圾邮件文件夹

Q19: ***什么是AIC指标?***

- AIC指“赤池信息量”, 建立在熵的概念基础上, 是衡量统计模型拟合优良性的一种标准.AIC越小, 模型越好.如下式, **k是所拟合模型中参数的数量, L是对数似然值,n是观测值数目**.k小意味着模型简洁, L大意味着模型精确.

$$
AIC=(2k-2L)/n
$$

Q20: ***什么是BIC指标?***

- BIC指“贝叶斯信息准则”, 从拟合角度, 选择一个对现有数据拟合最好的模型, 从贝叶斯因子的解释来讲, 就是边际似然最大的那个模型

$$
BIC=ln(n)k-2ln(L)
$$