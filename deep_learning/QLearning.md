# Q-learning

# 初级:

Q1 **怎样定义强化学习中的状态?**

- 强化学习(`RL`)中的状态表示问题类似于有监督或无监督学习中的特征表示, 特征选择和特征工程问题.对复杂问题建模的常用方法是离散化, 但离散化有可能会带来维度灾难问题.对于这些问题, 通常会利用状态表示学习(SRL),将状态表示为不同特征的向量

Q2 **Q Learning 中的*Alpha*和*Gamma*参数代表什么?**

- `Alpha`是学习率, 随着继续获得越来越大的知识库, 它应该会降低.学习率应该在 的范围内`0-1`.学习率越高, 它会很快替换新的`Q`值, 因此我们需要以某种方式对其进行优化, 以便我们的智能体从以前的 Q 值中学习.学习率是一种工具, 可用于确定我们保留了多少我们需要为状态-动作对保留的先前经验知识.
- Gamma是未来奖励的价值.它会对学习产生很大影响, 可以是动态值或静态值.如果等于`1`, 则代理人对未来奖励的重视程度与对当前奖励的重视程度相同.这意味着, 在十个动作中, 如果代理人做了好事, 这与直接执行此动作一样有价值.所以学习在高gamma值下效果不佳.相反, gamma将`0`导致代理只重视即时奖励, 这只适用于非常详细的奖励功能.

Q3 **怎么判断 Q-Learning 算法何时收敛?**

- 当学习曲线变得平坦并且不再增加时, 强化学习算法被认为收敛.但是由于探索参数ε不是逐渐增加的, 在达到最优策略之前, Q-Learning 会以过早的方式收敛.
- 因此, 必须满足两个条件才能保证极限收敛: 
    - **学习率接近于零, 但不能以太快的速度**.形式上, 这要求学习率之和必须发散, 但它们的平方和必须收敛.具有这些属性的示例序列是1/1, 1/2, 1/3, 1/4, ....
    - **每个状态-动作对必须被无限频繁地访问**.这有一个精确的数学定义: 每个动作在每个状态下都必须有一个非零的概率被策略选择, 即π(s, a) > 0对于所有(s, a).在实践中, 使用ε-贪心策略(where ε > 0) 可确保满足此条件.

引用: [https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q4 **如何理解epsilon greedy 算法?**

- 1 我们的小车一开始接触到的 state 很少, 并且如果小车按照已经学到的 qtable 执行, 那么小车很有可能出错或者绕圈圈.同时我们希望小车一开始能随机的走一走, 接触到更多的 state.
- 2 基于上述原因, 我们希望小车在一开始的时候不完全按照 Q learning 的结果运行, 即以一定的概率 epsilon, 随机选择 action, 而不是根据 maxQ 来选择 action.然后随着不断的学习, 那么我会降低这个随机的概率, 使用一个衰减函数来降低 epsilon.
- 3 这个就解决了所谓的`exploration and exploitation`的问题, 在“探索”和“执行”之间寻找一个权衡.

引用: [https://blog.csdn.net/Gin07](https://blog.csdn.net/Gin07)

Q5 **解释Q-learning算法**

- Q(s, a)函数, 即质量函数, 用来表示智能体在s状态下采用a动作并在之后采取最优动作条件下
的打折的未来奖励(先不管未来的动作如何选择): 

$$
Q(s_t,a_t)=max(R_{t+1})
$$

假设有了这个Q函数, 那么我们就能够求得在当前 t 时刻当中, 做出各个决策的最大收益值, 通过对比这些收益值, 就能够得到 t 时刻某个决策是这些决策当中收益最高.根据Q函数的递推公式可以得到: 

$$
Q(s_t,a_t)=r+max_aQ(s_{t+1},a_{t+1})
$$

这就是注明的贝尔曼公式.贝尔曼公式是说, 对于某个状态来讲, 最大化未来奖励相当于 最大化即刻奖励与下一状态最大未来奖励之和.Q-learning的核心思想是: 我们能够通过贝尔曼公式迭代地近似Q-函数.

Q6 **分别写出基于状态值函数的贝尔曼方程以及基于状态-动作值函数的贝尔曼方程.**

- 基于状态值函数的贝尔曼方程: 

$$
{{V}^{\pi }}(s)={{\mathbb{E}}_{a \sim \pi (a|s)}}{{\mathbb{E}}_{s'\sim p(s'|s,a)}}[r(s,a,s')+\gamma {{V}^{\pi }}(s')]
$$

- 基于状态-动作值函数(Q函数)的贝尔曼方程: 

$$
{{Q}^{\pi }}(s,a)={{\mathbb{E}}_{s'\sim p(s'|s,a)}}\left[ r(s,a,s')+\gamma {{\mathbb{E}}_{a'\sim \pi (a'|s')}}[{{Q}^{\pi }}(s',a')] \right]
$$

Q7 **讲一下SARSA, 最好可以写出其Q(s,a)的更新公式.另外, 它是on-policy还是off-policy, 为什么?**

- SARSA可以算是Q-learning的改进, 其更新公式为: 

$$
Q(s, a) \larr Q(s, a) + \alpha [r(s,a) + \gamma Q(s', a') - Q(s, a)]
$$

其为on-policy的, SARSA必须执行两次动作得到(s,a,r,s′,a′)才可以更新一次, 而且a′是在特定策略π
的指导下执行的动作, 因此估计出来的Q(s,a)是在该策略 π之下的Q值, 样本生成用的π和估计的π
是同一个, 因此是on-policy.

Q8 **请问value-based和policy-based方法的区别是什么?**

1. 生成策略上的差异, 前者确定, 后者随机.基于价值的方法中动作-价值对的估计值最终会收敛(通常是不同的数, 可以转化为0～1 的概率), 因此通常会获得一个确定的策略, 基于策略的方法不会收敛到一个确定的值, 另外他们会趋向于生成最佳随机策略.如果最佳策略是确定的, 那么最优动作对应的值函数的值将远大于次优动作对应的值函数的值, 值函数的大小代表概率的大小.
2. 动作空间是否连续, 前者离散, 后者连续.基于价值的方法, 对于连续动作空间问题, 虽然可以将动作空间离散化处理, 但离散间距的选取不易确定.过大的离散间距会导致算法取不到最优动作, 会在最优动作附近徘徊, 过小的离散间距会使得动作的维度增大, 会和高维度动作空间一样导致维度灾难, 影响算法的速度.而基于策略的方法适用于连续的动作空间, 在连续的动作空间中, 可以不用计算每个动作的概率, 而是通过正态分布选择动作.
3. 基于价值的方法, 例如Q学习算法, 是通过求解最优价值函数而间接地求解最优策略, 基于策略的方法, 例如REINFORCE等算法直接将策略参数化, 通过策略搜索, 策略梯度或者进化方法来更新参数以最大化回报.基于价值的方法不易扩展到连续动作空间, 并且当同时采用非线性近似, 自举等策略时会有收敛问题.策略梯度具有良好的收敛性.
4. 另外, 对于价值迭代和策略迭代, 策略迭代有两个循环, 一个是在策略估计的时候, 为了求当前策略的价值函数需要迭代很多次, 另一个是外面的大循环, 即策略评估, 策略提升.价值迭代算法则是一步到位, 直接估计最优价值函数, 因此没有策略提升环节.

引用: 《Easy RL》

# 中级:

Q9 **能否简单说下动态规划, 蒙特卡洛和时序差分的异同点?**

Q10 **简述DQN?**

Q11 **DQN中的两个技巧: 目标网络(target network)和经验回放(experience replay)的具体作用是什么呢?**

Q12 **DQN(Deep Q-learning)和Q-learning有什么异同点?**

Q13 **DQN都有哪些变种?**

Q14 **简述Double DQN(双深度Q网络)原理?**

Q15 **请问Dueling DQN(竞争深度Q网络)模型有什么优势?**

# 高级:

Q16 **请问Actor - Critic有何优点?**

Q17 **简述一下DDPG算法?**

Q18 **请问DDPG是on-policy还是off-policy, 为什么?**
