# Optimisation

## Easy

Q1: **解释什么是贝叶斯优化方法?**

- 贝叶斯优化是一种黑盒优化算法, 用于求解表达式未知的函数的极值问题.算法根据一组采样点处的函数值预测出任意点处函数值的概率分布, 这通过高斯过程回归而实现.根据高斯过程回归的结果构造采集函数, 用于衡量每一个点值得探索的程度, 求解采集函数的极值从而确定下一个采样点.最后返回这组采样点的极值作为函数的极值.
- 贝叶斯算法在机器学习中被用于AutoML算法, 自动确定机器学习算法的超参数.某些NAS算法也使用了贝叶斯优化算法.

Q2: **解释机器学习中的凸优化**

- 对于机器学习来说, 如果要优化的问题被证明是凸优化问题, 则说明此问题可以被比较好的用梯度下降等方法解决
- 凸优化更快, 更简单且计算量更少

![Untitled](Optimisation%204bb09c6ac2e54f3595cf6827042b5183/Untitled.png)

引用: [https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q3: 你知道哪些超参数调整(Hyperparameters tuning)方法?

- **随机搜索**: 超参数创建了一个可能值的网格.每次迭代都会尝试从该网格中随机组合超参数, 记录性能, 最后返回提供最佳性能的超参数组合.
- **网格搜索: **将搜索空间定义为超参数值的网格, 并评估网格中每个位置的算法性能(遍历), 最后返回提供最佳性能的超参数组合.
- **贝叶斯优化**: 假设超参数与最后我们需要优化的损失函数存在一个函数关系, 通过SMBO等算法最小化函数, 从而得到最优的参数组合

Q4: **解释Adam优化器**

- Adam: Adaptive Moment Estimation 利用梯度的一阶矩估计和二阶矩估计动态调节每个参数的学习率, 
- Adam算法汇集了Momentum+ RMSProp算法

Q5: **Adam算法有什么局限性?**

- 虽然使用Adam进行训练有助于快速收敛, 但结果模型的泛化性能往往不如使用SGD进行动量训练时的泛化性能.
- 另一个问题是, 即使Adam有自适应学习率, 当使用良好的学习率计划时, 它的性能也会提高.特别是在训练的早期, 使用较低的学习率来避免发散是有益的.这是因为在一开始, 模型的权值是随机的, 因此得到的梯度不是很可靠.如果学习率太大, 可能会导致模型采取太大的步骤, 而没有确定合适的权重.当模型克服了这些初始稳定性问题后, 可以提高学习速度, 加快收敛速度.

Q6: ***Batch Size*如何影响梯度下降方法的收敛性?为什么?**

- 大Bacthsize训练倾向于收敛到接近初始参数值的最小值, 而不是探索所有参数空间.
- 大Bacthsize训练倾向于收敛到更清晰的最小值, 而小批量训练倾向于收敛到更平坦的最小值.

![Untitled](Optimisation%204bb09c6ac2e54f3595cf6827042b5183/Untitled%201.png)

如图, 两个最小值都达到相同的损失值, 但平坦的最小值对参数空间中的扰动不太敏感.他们提供的实验证据表明, 大批量训练更有可能收敛到尖锐的最小值和接近起点的最小值.小批量训练中的固有噪声有助于将参数推出尖锐的盆地.

引用: 

Q7: **什么情况下, 选择遗传算法作为优化算法?**

- 当许多处理器可以并行使用时
- 当目标函数具有高模态(许多局部最优)时

引用: [https://stats.stackexchange.com/questions/249471/when-are-genetic-algorithms-a-good-choice-for-optimization](https://stats.stackexchange.com/questions/249471/when-are-genetic-algorithms-a-good-choice-for-optimization)

Q8: **Adagrad算法是怎样调整学习率的?**

- AdaGrad是一个基于梯度的优化算法, 它的主要特点是: 它对不同的参数调整学习率, 具体而言, 对低频出现的参数进行大的更新, 对高频出现的参数进行小的更新. 因此, 他很适合于处理稀疏数据.具体计算过程如下: 
- 计算梯度:

$$
g_t = \nabla_\theta J(\theta_{t-1})
$$

- 累计平方梯度: 

$$
r_t = r_{t-1} + g_t \odot g_t
$$

- 计算梯度更新: 

$$
\Delta \theta = {\eta \over \epsilon + \sqrt{r_t}} \odot g_t
$$

- 更新学习率: 

$$
\theta_t=\theta_{t-1} - \Delta \theta
$$

Q9: **比较牛顿法和梯度下降**

- 梯度下降是一种基于一阶导数信息的优化算法, 它在每一步中向梯度相反的方向更新参数.由于梯度下降只考虑了一阶导数信息, 因此可能会受到噪声和局部极小值的干扰.牛顿法是一种基于二阶导数信息的优化算法, 它使用牛顿法求解目标函数的局部二次近似, 然后在每一步中通过求解线性方程组来更新参数.
- 区别: 
    - 梯度下降法是一阶优化算法, 牛顿法是二阶优化算法
    - 牛顿法的收敛速度相比梯度下降法常常较快
    - 牛顿法每次需要更新一个二维矩阵, 计算代价很大, 实际使用中常使用拟牛顿法
    - 牛顿法对初始值有一定要求, 在非凸优化问题中(如神经网络训练), 牛顿法很容易陷入鞍点(牛顿法步长会越来越小), 而梯度下降法则很容易逃离鞍点(因此在神经网络训练中一般使用梯度下降法, 高维空间的神经网络中存在大量鞍点)
    - 梯度下降法在靠近最优点时会震荡, 因此步长调整在梯度下降法中是必要的, 具体有adagrad, adadelta, rmsprop, adam等一系列自适应学习率的方法

Q10: **贝叶斯优化如何在超参数优化中使用?**

- 在超参数优化中, 贝叶斯优化可以使用高斯过程模型来对超参数进行建模.对于每一次迭代, 该模型会根据之前的结果预测出下一个超参数组合的性能, 并在该组合处进行新的函数评估.这样, 模型会不断地更新, 直到找到最佳超参数组合.

Q11: **剪枝方法是否适用于神经网络优化?**

- 剪枝方法可以用于神经网络优化, 以减少模型的计算负担和过拟合风险.剪枝方法通过评估每个神经元的重要性来确定哪些神经元可以被删除.这可以通过在训练过程中使用正则化技术来实现, 或者通过在训练后使用一些特定的剪枝算法来实现.

![Untitled](Optimisation%204bb09c6ac2e54f3595cf6827042b5183/Untitled%202.png)

Q12: **在随机森林模型中, 如何优化森林中树的数量?**

- 在随机森林模型中, 可以通过交叉验证的方式来确定最佳的树的数量.通过对不同数量的树进行评估, 可以找到最优的树的数量, 以最大化模型的准确性.在实践中, 通常会评估10到1000棵树, 并选择最佳数量.

Q13: ****若CNN网络很庞大, 在手机上运行效率不高, 对应模型压缩方法有了解吗?****

- 低秩近似: 卷积, 实则就是矩阵运算, 低秩近似的技术是通过一系列小规模矩阵将权重矩阵重构出来, 以此降低运算量和存储开销.两种方法包括**奇异值分解与Toeplitz矩阵重构**
- 模型剪枝与稀疏约束: 模型剪枝衡量神经元的重要程度, 移除掉一部分不重要的神经元, 再对网络进行finetuning, 如此往复, 稀疏约束的思路是在网络的优化目标中加入正则化, 使得训练网络的部分权重趋于0, 这些0值元素就是被剪枝的对象, 稀疏约束的好处是只需要进行一遍训练即可完成剪枝
- 参数量化: 相比于剪枝操作, 参数量化则是一种常用的后端压缩技术.量化就是从权重中归纳出若干个有代表性的权重, 由这些代表来表示某一类权重的具体数值.这些“代表”被存储在码本(codebook)中, 而原权重矩阵只需记录各自“代表”的索引即可, 从而极大地降低了存储开销.
- 知识蒸馏: 使用一个较大的已经训练好的网络去教导一个较小的网络, 将一个高精度且笨重的teacher网络转换为一个更加紧凑的student网络

Q14: **讲述模型蒸馏的原理与操作**

- 模型蒸馏: 训练teacher模型softmax层的超参数获得一个合适的soft target集合(“软标签”指的是大网络在每一层卷积后输出的feature map.), 然后对要训练的student模型, 使用同样的超参数值尽可能地接近teacher模型的soft target集合, 作为student模型总目标函数的一部分, 以诱导student模型的训练, 实现知识的迁移.

引用: [https://blog.csdn.net/weixin_46838716/article/details](https://blog.csdn.net/weixin_46838716/article/details)